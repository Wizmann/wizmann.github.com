<!doctype html><html lang=zh-cn dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Introduction to Ceph | Maerlyn's Rainbow</title>
<meta name=keywords content="Ceph,Distributed System,Storage System,Storage,System Design"><meta name=description content="什么是Ceph
Ceph是一个可扩展的，高性能的分布式存储系统。提供了三种不同类型的接口以适应不同的应用场景：

block-based: 块存储，可以用做VM的虚拟磁盘
object-based: 对象存储，与Amazon S3等常用对象存储兼容
file system: POSIX兼容的分布式文件系统，可以被本地系统挂载，并且能被多个客户端共享

"><meta name=author content="wizmann"><link rel=canonical href=https://wizmann.top/posts/introduction-to-ceph/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=https://wizmann.top/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://wizmann.top/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://wizmann.top/favicon-32x32.png><link rel=apple-touch-icon href=https://wizmann.top/apple-touch-icon.png><link rel=mask-icon href=https://wizmann.top/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh-cn href=https://wizmann.top/posts/introduction-to-ceph/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-2X5NE9PX0B"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-2X5NE9PX0B")</script><meta property="og:url" content="https://wizmann.top/posts/introduction-to-ceph/"><meta property="og:site_name" content="Maerlyn's Rainbow"><meta property="og:title" content="Introduction to Ceph"><meta property="og:description" content="什么是Ceph Ceph是一个可扩展的，高性能的分布式存储系统。提供了三种不同类型的接口以适应不同的应用场景：
block-based: 块存储，可以用做VM的虚拟磁盘 object-based: 对象存储，与Amazon S3等常用对象存储兼容 file system: POSIX兼容的分布式文件系统，可以被本地系统挂载，并且能被多个客户端共享 "><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-11-29T00:00:00+00:00"><meta property="article:modified_time" content="2020-11-29T00:00:00+00:00"><meta property="article:tag" content="Ceph"><meta property="article:tag" content="Distributed System"><meta property="article:tag" content="Storage System"><meta property="article:tag" content="Storage"><meta property="article:tag" content="System Design"><meta name=twitter:card content="summary"><meta name=twitter:title content="Introduction to Ceph"><meta name=twitter:description content="什么是Ceph
Ceph是一个可扩展的，高性能的分布式存储系统。提供了三种不同类型的接口以适应不同的应用场景：

block-based: 块存储，可以用做VM的虚拟磁盘
object-based: 对象存储，与Amazon S3等常用对象存储兼容
file system: POSIX兼容的分布式文件系统，可以被本地系统挂载，并且能被多个客户端共享

"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://wizmann.top/posts/"},{"@type":"ListItem","position":2,"name":"Introduction to Ceph","item":"https://wizmann.top/posts/introduction-to-ceph/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Introduction to Ceph","name":"Introduction to Ceph","description":"什么是Ceph Ceph是一个可扩展的，高性能的分布式存储系统。提供了三种不同类型的接口以适应不同的应用场景：\nblock-based: 块存储，可以用做VM的虚拟磁盘 object-based: 对象存储，与Amazon S3等常用对象存储兼容 file system: POSIX兼容的分布式文件系统，可以被本地系统挂载，并且能被多个客户端共享 ","keywords":["Ceph","Distributed System","Storage System","Storage","System Design"],"articleBody":"什么是Ceph Ceph是一个可扩展的，高性能的分布式存储系统。提供了三种不同类型的接口以适应不同的应用场景：\nblock-based: 块存储，可以用做VM的虚拟磁盘 object-based: 对象存储，与Amazon S3等常用对象存储兼容 file system: POSIX兼容的分布式文件系统，可以被本地系统挂载，并且能被多个客户端共享 Ceph的特性 由于采用了CRUSH算法，Ceph有着优异的可扩展性（宣称可以无限扩展）。并且借助可扩展性，进而实现高性能、高可靠性和高可用性。\nCeph是一个去中心化的存储系统，无需中心节点进行资源的管理与调度，全部的管理功能由存储节点自治完成。使得整个系统可以自我管理与自我恢复，减少运维成本与管理成本。\nRADOS - Ceph的存储引擎 RADOS=Reliable Autonomic Distributed Object Store。RADOS是Ceph底层的存储引擎，所有的接口都建立在RADOS的功能之上。\nRADOS中的存储结构 存储池（pool）：逻辑层，每一个pool里都包含一些放置组 放置组（placement-group, PG)：逻辑层，一份数据会在PG当中进行灾备复制。每一个PG都对应着一系列的存储节点 存储节点（OSD）：用以存储数据的物理节点。与PG之间形成多对多的关系。 一份数据在写入RADOS时，会先选中一个pool。Pool中再使用一定的hash规则，伪随机的选中某一个PG。PG会将数据写入多个OSD中。读取数据时，也是类似的规则。\nPool是用户可见的管理数据的基本单位，用户可以对Pool进行一系列的配置（权限控制、使用SSD or HDD、使用数据拷贝 or 纠删码，etc.）。而PG与OSD对于用户是不可见的。\nPG的组织 在一致性哈希中，我们使用节点来划分哈希值域。这种方法的问题是，如果产生了数据不平衡，我们需要重新进行划分值域来进行再平衡。这会造成大量的数据迁移。\nCRUSH采用了虚拟节点（也就是PG）将哈希值域划分成了固定的等长区域。这种方法在单条数据与物理节点之间加入了一个虚拟层。之后，再使用哈希取模的算法确定数据属于哪个PG。使得数据的迁移是以虚拟节点为单位，而不是对每一条数据都重新计算。Ceph官方的建议是，每1个OSD对应着100个PG。\n一般情况下，在规划的初期需要确定PG的数目，如果后期需要调整PG，有可能会导致大量的数据迁移，甚至需要服务暂时停止服务。\n监控子集群（MON）与Cluster Map RADOS集群中除了OSD存储节点之外，还有监控子集群（MON），用于存储系统的拓扑结构——Cluster Map。\n元数组管理（MDS）节点用于管理CephFS中的文件元信息，后文会有介绍。\n不同于传统的中心化管理节点，MON并不会对资源进行调配与调度，而仅仅是一个观测者，用以存储系统当前的拓扑与状态。\nMON与OSD、OSD与OSD之间会定时发送心跳包，检测OSD是否健康。如果某个节点失效，MON会更新内部存储的拓扑结构信息（ClusterMap），并且通过P2P协议广播出去，从而使得整个系统都有着（最终）一致的拓扑信息。\n主从同步与节点自治 在一个PG中，会有一个主节点（Primary）和一个或多个从节点（Secondary）。主节点负责维护从节点的状态，包括数据复制（replication）、失效检测（failure detection）和失效恢复（failure recovery）。\nCRUSH - Ceph皇冠上的明珠 CRUSH是一个可扩展的，伪随机的数据放置算法。以去中心化方法，将PG按规则映射到相应的存储设备上。并且系统的拓扑结构发生变化时，尽可能的减少数据的迁移。\nCRUSH的优势 CRUSH对于每一个数据元素使用一个伪随机算法，决定其放置的位置。所以，只要所有参与者都拥有相同的系统拓扑结构信息，那么数据的位置就是一定的。所以我们可以去掉中心节点，采用P2P的方法来进行数据的存储与检索。\n并且，由于伪随机算法只与单个PG相关，如果我们操作得当，节点数量的变化并不会引起大量的数据迁移，而是会接近理论最优值。\n数据放置（Data Placement） CRUSH的数据放置算法有很多不同的实现，这里只介绍最常用的straw算法。\nCeph中，每一个OSD节点都有一个权值w，代表着某个节点能支持多少数据的存储与检索。一般来说，权值与节点的容量成正比。\n假设一个pool里面有n个PG，在一条新的数据写入时，我们分别会计算这n个PG的length值。\n$$ length_{i} = f(w_{i}) * hash(x) $$\n$f(w_{i})$是一个只于当前OSD节点权值有关的函数。$hash(x)$代表当前PG的哈希值。所以，PG会放置在哪个OSD上，仅与其权值相关。\n假设某个OSD节点发生变化时（新加、删除、权值变化），在此受影响节点的数据会迁移到其它的OSD节点。其它OSD节点的原有数据并不会受到影响。\nCeph当中的straw算法有straw1和straw2。straw1的实现采用了有缺陷f(w)函数，会导致意外的数据迁移。straw2解决了这个问题。 详情请戳这里\n主从架构 每个PG所包含的OSD都由CRUSH算法计算得出，并根据配置选出前r个OSD进行主从配对。列表中的第1个OSD做为主节点（Primary），其它的节点为从节点（Secondary）。\n主从节点的分配与管理由PG内部进行自治，不需要额外的外部系统进行管理。\nCephFS CephFS是一个POSIX兼容的（共享）文件系统。CephFS利用文件元数据子系统（MDS）来维护目录树结构和文件和目录的元信息（owner, timestamps, inodes, etc.)等。\nMDS子系统会在内存里面维护一份Cache，对于需要持久化的信息，会使用WAL的方式写入RADOS里一个专用的Pool当中。\n动态树划分（Dynamic Tree Partitioning，DTP） CephFS的扩展性的关键之一，在于元信息子系统的扩展性。CephFS实现了动态树划分的算法，将目录树结构根据当前系统的负载，将其划分到不同的MDS节点上去。\n维护目录树结构的优势在于利用了文件系统的局部性（locality），可以方便的进行预取（prefetch）。动态的树划分，可以保证元信息可以线性增长，以保持高可扩展性。\n写在最后 本文基于Ceph的三篇论文综合而成（CephFS、RADOS、CRUSH）。其中加入了一些自己的看法，使其逻辑通顺，并不保证与论文的思路完全一致。\n这三篇论文并没有明确的依赖关系，换句话说，需要综合阅读，才能有比较明确的理解。\n建议在通读论文后，去学习一下这个视频，会对理解Ceph有很大的帮助。Youtube上面还有很多Ceph的tech talk，可以一并的了解一下。\nCeph相关的书籍以实践居多，只推荐Learning Ceph。\n参考链接 一致性 Hash 算法分析 从一致性 hash 到 ceph crush Ceph Publications ","wordCount":"2715","inLanguage":"zh-cn","datePublished":"2020-11-29T00:00:00Z","dateModified":"2020-11-29T00:00:00Z","author":{"@type":"Person","name":"wizmann"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://wizmann.top/posts/introduction-to-ceph/"},"publisher":{"@type":"Organization","name":"Maerlyn's Rainbow","logo":{"@type":"ImageObject","url":"https://wizmann.top/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://wizmann.top/ accesskey=h title="Maerlyn's Rainbow (Alt + H)">Maerlyn's Rainbow</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Introduction to Ceph</h1><div class=post-meta><span title='2020-11-29 00:00:00 +0000 UTC'>November 29, 2020</span>&nbsp;·&nbsp;wizmann</div></header><div class=post-content><h2 id=什么是ceph>什么是Ceph<a hidden class=anchor aria-hidden=true href=#什么是ceph>#</a></h2><p>Ceph是一个可扩展的，高性能的分布式存储系统。提供了三种不同类型的接口以适应不同的应用场景：</p><ul><li>block-based: 块存储，可以用做VM的虚拟磁盘</li><li>object-based: 对象存储，与Amazon S3等常用对象存储兼容</li><li>file system: POSIX兼容的分布式文件系统，可以被本地系统挂载，并且能被多个客户端共享</li></ul><p><img loading=lazy src=https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/20-11-28/2020-11-28_22-04-49.png></p><h3 id=ceph的特性>Ceph的特性<a hidden class=anchor aria-hidden=true href=#ceph的特性>#</a></h3><p>由于采用了CRUSH算法，Ceph有着优异的可扩展性（宣称可以无限扩展）。并且借助可扩展性，进而实现高性能、高可靠性和高可用性。</p><p>Ceph是一个去中心化的存储系统，无需中心节点进行资源的管理与调度，全部的管理功能由存储节点自治完成。使得整个系统可以自我管理与自我恢复，减少运维成本与管理成本。</p><h2 id=rados---ceph的存储引擎>RADOS - Ceph的存储引擎<a hidden class=anchor aria-hidden=true href=#rados---ceph的存储引擎>#</a></h2><p>RADOS=Reliable Autonomic Distributed Object Store。RADOS是Ceph底层的存储引擎，所有的接口都建立在RADOS的功能之上。</p><h3 id=rados中的存储结构>RADOS中的存储结构<a hidden class=anchor aria-hidden=true href=#rados中的存储结构>#</a></h3><p><img loading=lazy src=https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/20-11-28/2020-11-28_22-24-56.png></p><ul><li>存储池（pool）：逻辑层，每一个pool里都包含一些放置组</li><li>放置组（placement-group, PG)：逻辑层，一份数据会在PG当中进行灾备复制。每一个PG都对应着一系列的存储节点</li><li>存储节点（OSD）：用以存储数据的物理节点。与PG之间形成多对多的关系。</li></ul><p>一份数据在写入RADOS时，会先选中一个pool。Pool中再使用一定的hash规则，伪随机的选中某一个PG。PG会将数据写入多个OSD中。读取数据时，也是类似的规则。</p><p>Pool是用户可见的管理数据的基本单位，用户可以对Pool进行一系列的配置（权限控制、使用SSD or HDD、使用数据拷贝 or 纠删码，etc.）。而PG与OSD对于用户是不可见的。</p><p><img loading=lazy src=https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/20-11-28/2020-11-28_23-00-51.png></p><h4 id=pg的组织>PG的组织<a hidden class=anchor aria-hidden=true href=#pg的组织>#</a></h4><p>在一致性哈希中，我们使用节点来划分哈希值域。这种方法的问题是，如果产生了数据不平衡，我们需要重新进行划分值域来进行再平衡。这会造成大量的数据迁移。</p><p>CRUSH采用了虚拟节点（也就是PG）将哈希值域划分成了固定的等长区域。这种方法在单条数据与物理节点之间加入了一个虚拟层。之后，再使用哈希取模的算法确定数据属于哪个PG。使得数据的迁移是以虚拟节点为单位，而不是对每一条数据都重新计算。Ceph官方的建议是，每1个OSD对应着100个PG。</p><p>一般情况下，在规划的初期需要确定PG的数目，如果后期需要调整PG，有可能会导致大量的数据迁移，甚至需要服务暂时停止服务。</p><h3 id=监控子集群mon与cluster-map>监控子集群（MON）与Cluster Map<a hidden class=anchor aria-hidden=true href=#监控子集群mon与cluster-map>#</a></h3><p><img loading=lazy src=https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/20-11-28/2020-11-28_22-48-10.png></p><p>RADOS集群中除了OSD存储节点之外，还有监控子集群（MON），用于存储系统的拓扑结构——Cluster Map。</p><blockquote><p>元数组管理（MDS）节点用于管理CephFS中的文件元信息，后文会有介绍。</p></blockquote><p>不同于传统的中心化管理节点，MON并不会对资源进行调配与调度，而仅仅是一个观测者，用以存储系统当前的拓扑与状态。</p><p>MON与OSD、OSD与OSD之间会定时发送心跳包，检测OSD是否健康。如果某个节点失效，MON会更新内部存储的拓扑结构信息（ClusterMap），并且通过P2P协议广播出去，从而使得整个系统都有着（最终）一致的拓扑信息。</p><h3 id=主从同步与节点自治>主从同步与节点自治<a hidden class=anchor aria-hidden=true href=#主从同步与节点自治>#</a></h3><p>在一个PG中，会有一个主节点（Primary）和一个或多个从节点（Secondary）。主节点负责维护从节点的状态，包括数据复制（replication）、失效检测（failure detection）和失效恢复（failure recovery）。</p><h2 id=crush---ceph皇冠上的明珠>CRUSH - Ceph皇冠上的明珠<a hidden class=anchor aria-hidden=true href=#crush---ceph皇冠上的明珠>#</a></h2><p>CRUSH是一个可扩展的，伪随机的数据放置算法。以去中心化方法，将PG按规则映射到相应的存储设备上。并且系统的拓扑结构发生变化时，尽可能的减少数据的迁移。</p><h3 id=crush的优势>CRUSH的优势<a hidden class=anchor aria-hidden=true href=#crush的优势>#</a></h3><p>CRUSH对于每一个数据元素使用一个伪随机算法，决定其放置的位置。所以，只要所有参与者都拥有相同的系统拓扑结构信息，那么数据的位置就是一定的。所以我们可以去掉中心节点，采用P2P的方法来进行数据的存储与检索。</p><p>并且，由于伪随机算法只与单个PG相关，如果我们操作得当，节点数量的变化并不会引起大量的数据迁移，而是会接近理论最优值。</p><h3 id=数据放置data-placement>数据放置（Data Placement）<a hidden class=anchor aria-hidden=true href=#数据放置data-placement>#</a></h3><p>CRUSH的数据放置算法有很多不同的实现，这里只介绍最常用的straw算法。</p><p>Ceph中，每一个OSD节点都有一个权值w，代表着某个节点能支持多少数据的存储与检索。一般来说，权值与节点的容量成正比。</p><p>假设一个pool里面有n个PG，在一条新的数据写入时，我们分别会计算这n个PG的length值。</p><p>$$ length_{i} = f(w_{i}) * hash(x) $$</p><p>$f(w_{i})$是一个只于当前OSD节点权值有关的函数。$hash(x)$代表当前PG的哈希值。所以，PG会放置在哪个OSD上，仅与其权值相关。</p><p>假设某个OSD节点发生变化时（新加、删除、权值变化），在此受影响节点的数据会迁移到其它的OSD节点。其它OSD节点的原有数据并不会受到影响。</p><blockquote><p>Ceph当中的straw算法有<code>straw1</code>和<code>straw2</code>。<code>straw1</code>的实现采用了有缺陷f(w)函数，会导致意外的数据迁移。<code>straw2</code>解决了这个问题。
详情请戳<a href=https://www.spinics.net/lists/ceph-devel/msg21635.html>这里</a></p></blockquote><h4 id=主从架构>主从架构<a hidden class=anchor aria-hidden=true href=#主从架构>#</a></h4><p>每个PG所包含的OSD都由CRUSH算法计算得出，并根据配置选出前r个OSD进行主从配对。列表中的第1个OSD做为主节点（Primary），其它的节点为从节点（Secondary）。</p><p>主从节点的分配与管理由PG内部进行自治，不需要额外的外部系统进行管理。</p><h2 id=cephfs>CephFS<a hidden class=anchor aria-hidden=true href=#cephfs>#</a></h2><p>CephFS是一个POSIX兼容的（共享）文件系统。CephFS利用文件元数据子系统（MDS）来维护目录树结构和文件和目录的元信息（owner, timestamps, inodes, etc.)等。</p><p>MDS子系统会在内存里面维护一份Cache，对于需要持久化的信息，会使用WAL的方式写入RADOS里一个专用的Pool当中。</p><h3 id=动态树划分dynamic-tree-partitioningdtp>动态树划分（Dynamic Tree Partitioning，DTP）<a hidden class=anchor aria-hidden=true href=#动态树划分dynamic-tree-partitioningdtp>#</a></h3><p>CephFS的扩展性的关键之一，在于元信息子系统的扩展性。CephFS实现了动态树划分的算法，将目录树结构根据当前系统的负载，将其划分到不同的MDS节点上去。</p><p>维护目录树结构的优势在于利用了文件系统的局部性（locality），可以方便的进行预取（prefetch）。动态的树划分，可以保证元信息可以线性增长，以保持高可扩展性。</p><p><img loading=lazy src=https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/20-11-29/2020-11-29_18-53-09.png></p><h2 id=写在最后>写在最后<a hidden class=anchor aria-hidden=true href=#写在最后>#</a></h2><p>本文基于Ceph的三篇论文综合而成（CephFS、RADOS、CRUSH）。其中加入了一些自己的看法，使其逻辑通顺，并不保证与论文的思路完全一致。</p><p>这三篇论文并没有明确的依赖关系，换句话说，需要综合阅读，才能有比较明确的理解。</p><p>建议在通读论文后，去学习一下<a href="https://www.youtube.com/watch?v=PmLPbrf-x9g&amp;ab_channel=Ceph">这个视频</a>，会对理解Ceph有很大的帮助。Youtube上面还有很多Ceph的tech talk，可以一并的了解一下。</p><p>Ceph相关的书籍以实践居多，只推荐<a href=https://www.oreilly.com/library/view/learning-ceph-/9781787127913/>Learning Ceph</a>。</p><h2 id=参考链接>参考链接<a hidden class=anchor aria-hidden=true href=#参考链接>#</a></h2><ul><li><a href=https://crossoverjie.top/2018/01/08/Consistent-Hash/>一致性 Hash 算法分析</a></li><li><a href=https://zhuanlan.zhihu.com/p/60963885>从一致性 hash 到 ceph crush</a></li><li><a href=https://ceph.io/publications/>Ceph Publications</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://wizmann.top/tags/ceph/>Ceph</a></li><li><a href=https://wizmann.top/tags/distributed-system/>Distributed System</a></li><li><a href=https://wizmann.top/tags/storage-system/>Storage System</a></li><li><a href=https://wizmann.top/tags/storage/>Storage</a></li><li><a href=https://wizmann.top/tags/system-design/>System Design</a></li></ul><nav class=paginav><a class=prev href=https://wizmann.top/posts/uw-cse341-au20/><span class=title>«</span><br><span>我到底从UW-CSE341学到了什么</span>
</a><a class=next href=https://wizmann.top/posts/jisuanke-climb-the-hill/><span class=title>»</span><br><span>2020 计蒜之道 线上决赛 - C. 攀登山峰</span></a></nav></footer><div id=disqus_thread></div><script>(function(){var e=document,t=e.createElement("script");t.src="https://wizmann.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></article></main><footer class=footer><span>&copy; 2025 <a href=https://wizmann.top/>Maerlyn's Rainbow</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>