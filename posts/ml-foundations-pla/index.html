<!doctype html><html lang=zh-CN dir=ltr><head><script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]]},svg:{fontCache:"global"}}</script><script defer src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body class=notransition><div id=container><header id=main-header><div role=navigation aria-label=Main><div class=nav-left><a href=https://wizmann.top/ style=color:inherit>Maerlyn's Rainbow</a></div><div class=nav-right><div style=position:absolute;width:0;height:0><div id=nav-dropdown-menu class=hidden href=#><div class=nav-item><a aria-current=true class=ancestor href=/posts/>Posts</a></div></div></div><a id=nav-dropdown-button href=#><svg width="20" height="20" viewBox="0 0 24 24" fill="none"><path d="M4 6H20M4 12H20M4 18H20" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg></a><div id=nav-menu><div class=nav-item><a aria-current=true class=ancestor href=/posts/>Posts</a></div></div><a id=theme-switcher href=#><svg class="light-icon" viewBox="0 0 24 24" fill="none"><path d="M12 3V4m0 16v1M4 12H3M6.31412 6.31412 5.5 5.5m12.1859.81412L18.5 5.5M6.31412 17.69 5.5 18.5001M17.6859 17.69 18.5 18.5001M21 12H20m-4 0c0 2.2091-1.7909 4-4 4-2.20914.0-4-1.7909-4-4 0-2.20914 1.79086-4 4-4 2.2091.0 4 1.79086 4 4z" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg><svg class="dark-icon" viewBox="0 0 24 24" fill="none"><path d="M3.32031 11.6835c0 4.9706 4.02944 9 8.99999 9 3.7872.0 7.028-2.3392 8.3565-5.6515C19.6402 15.4486 18.5059 15.6834 17.3203 15.6834c-4.9706.0-8.99999-4.0294-8.99999-8.99998C8.32031 5.50338 8.55165 4.36259 8.96453 3.32996 5.65605 4.66028 3.32031 7.89912 3.32031 11.6835z" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg></a></div></div></header><div class="flex grow"><div id=main-pane><main id=main-content><div class=single-header><ol class=breadcrumbs itemscope itemtype=https://schema.org/BreadcrumbList><li itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a itemprop=item href=https://wizmann.top/><span itemprop=name></span>
</a><meta itemprop=position content='1'></li><span>&nbsp»&nbsp</span><li itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a itemprop=item href=https://wizmann.top/posts/><span itemprop=name>Posts</span>
</a><meta itemprop=position content='2'></li><span>&nbsp»&nbsp</span></ol><h1>機器學習基石 - PLA算法初步</h1><time class=dim datetime=2013-11-29T00:00:00+00:00>November 29, 2013</time><div class=term-container><div class=tag><a href=https://wizmann.top/tags/%E5%85%AC%E5%BC%80%E8%AF%BE/>#公开课</a></div><div class=tag><a href=https://wizmann.top/tags/%E5%88%86%E7%B1%BB%E5%99%A8/>#分类器</a></div></ol></div><section class=page-section><h2 id=什么是pla算法>什么是PLA算法</h2><p>PLA = Perceptrons Learning Alogrithm</p><p>WikiPedia上有一个大概的历史背景介绍。</p><blockquote><p>感知机（英语：Perceptron）是Frank Rosenblatt在1957年就职于Cornell航空实验室(Cornell Aeronautical Laboratory)时所发明的一种人工神经网络。它可以被视为一种最简单形式的前馈式人工神经网络，是一种二元线性分类器。</p></blockquote><h2 id=pla算法的原理>PLA算法的原理</h2><p><img src=https://github.com/Wizmann/assets/raw/master/wizmann-tk-pic/blog-perceptron-Ncell.png alt=感知机示意图></p><blockquote><p>对于每种输入值(1 - D)，我们计算一个权重。当前神经元的总激发值(a)就等于每种输入值(x)乘以权重(w)之和。</p></blockquote><p>由此我们就可以推导出公式如下。</p><p><img src=https://github.com/Wizmann/assets/raw/master/wizmann-tk-pic/blog-perceptron-formula-1.jpg alt="neuron sum"></p><p>我们可以为这个“神经元”的激发值设定一个阈值<code>threshold</code>。</p><p>如果 <code>a > threshold</code>，则判定输入为正例。
如果 <code>a &lt; threshold</code>，则判定输入为负例。
对于 <code>a == threshold</code>的情况，认为是特殊情况，不予考虑。</p><p>所以，我们的感知器分类器就可以得到以下式子。</p><p><img src=https://github.com/Wizmann/assets/raw/master/wizmann-tk-pic/blog-perceptron-formula-2.png alt=perceptron-formula-2></p><p>我们在数据向量中加入了阈值，并把式子统一成向量积的形式。</p><h2 id=pla算法的错误修正>PLA算法的错误修正</h2><p>PLA算法是_错误驱动_的算法。</p><blockquote><p>当我们训练这个算法时，只要输出值是正确的，这个算法就不会进行任何数据的调整。反之，当输出值与实际值异号，这个算法就会自动调整参数的比重。</p></blockquote><p><img src=https://github.com/Wizmann/assets/raw/master/wizmann-tk-pic/blog-perceptron-update.png alt=错误修正></p><p>我们先取一个随机向量<code>W</code>，与现有的数据<code>X[i]</code>做点乘，取得结果的符号。</p><p>如果符号符合我们的预期的话，则<code>continue</code>。
否则就要对<code>W</code>进行修正。</p><p>修正的方式是<code>W += y * X[i]</code>，每一次修正都是减少现有向量<code>W</code>与向量<code>y * X[i]</code>的夹角，从而调整答案的正确性。</p><h2 id=naive-pla-与-pocket-pla>Naive PLA 与 Pocket PLA</h2><h3 id=naive-pla>Naive PLA</h3><p>Naive PLA算法的思想很简单。一直修正向量<code>W</code>，直到向量<code>W</code>满足所有数据为止。</p><p>代码如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> numpy <span style=color:#f92672>import</span> <span style=color:#f92672>*</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>naive_pla</span>(datas):
</span></span><span style=display:flex><span>    w <span style=color:#f92672>=</span> datas[<span style=color:#ae81ff>0</span>][<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    iteration <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>        iteration <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        false_data <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> data <span style=color:#f92672>in</span> datas:
</span></span><span style=display:flex><span>            t <span style=color:#f92672>=</span> dot(w, data[<span style=color:#ae81ff>0</span>])
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> sign(data[<span style=color:#ae81ff>1</span>]) <span style=color:#f92672>!=</span> sign(t):
</span></span><span style=display:flex><span>                error <span style=color:#f92672>=</span> data[<span style=color:#ae81ff>1</span>]  
</span></span><span style=display:flex><span>                false_data <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>                w <span style=color:#f92672>+=</span> error <span style=color:#f92672>*</span> data[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        print <span style=color:#e6db74>&#39;iter</span><span style=color:#e6db74>%d</span><span style=color:#e6db74> (</span><span style=color:#e6db74>%d</span><span style=color:#e6db74> / </span><span style=color:#e6db74>%d</span><span style=color:#e6db74>)&#39;</span> <span style=color:#f92672>%</span> (iteration, false_data, len(datas))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> false_data:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> w
</span></span></code></pre></div><h3 id=pocket-pla>Pocket PLA</h3><p>Naive PLA的一大问题就是如果数据有杂音，不能完美的分类的话，算法就不会中止。</p><p>所以，对于有杂音的数据，我们只能期望找到错误最少的结果。然后这是一个<code>NP Hard</code>问题。</p><p>Pocket PLA一个贪心的近似算法。和Naive PLA算法类似。</p><p>变顺序迭代为随机迭代，如果找出错误，则修正结果。在修正过程中，记录犯错误最少的向量。</p><p>代码如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>pocket_pla</span>(datas, limit):
</span></span><span style=display:flex><span>    <span style=color:#75715e>###############</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_calc_false</span>(vec):
</span></span><span style=display:flex><span>        res <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> data <span style=color:#f92672>in</span> datas:
</span></span><span style=display:flex><span>            t <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(vec, data[<span style=color:#ae81ff>0</span>])
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> np<span style=color:#f92672>.</span>sign(data[<span style=color:#ae81ff>1</span>]) <span style=color:#f92672>!=</span> np<span style=color:#f92672>.</span>sign(t):
</span></span><span style=display:flex><span>                res <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> res
</span></span><span style=display:flex><span>    <span style=color:#75715e>###############</span>
</span></span><span style=display:flex><span>    w <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>5</span>)
</span></span><span style=display:flex><span>    least_false <span style=color:#f92672>=</span> _calc_false(w)
</span></span><span style=display:flex><span>    res <span style=color:#f92672>=</span> w
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> xrange(limit):
</span></span><span style=display:flex><span>        data <span style=color:#f92672>=</span> random<span style=color:#f92672>.</span>choice(datas)
</span></span><span style=display:flex><span>        t <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(w, data[<span style=color:#ae81ff>0</span>])
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> np<span style=color:#f92672>.</span>sign(data[<span style=color:#ae81ff>1</span>]) <span style=color:#f92672>!=</span> np<span style=color:#f92672>.</span>sign(t):
</span></span><span style=display:flex><span>            t <span style=color:#f92672>=</span> w <span style=color:#f92672>+</span> data[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>*</span> data[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>            t_false <span style=color:#f92672>=</span> _calc_false(t)
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            w <span style=color:#f92672>=</span> t
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> t_false <span style=color:#f92672>&lt;=</span> least_false:
</span></span><span style=display:flex><span>                least_false <span style=color:#f92672>=</span> t_false
</span></span><span style=display:flex><span>                res <span style=color:#f92672>=</span> t
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> res, least_false
</span></span></code></pre></div><h2 id=参考链接>参考链接</h2><p>本文主要参考了<a href=http://shaoxiongjiang.com/2013/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8-%E6%84%9F%E7%9F%A5%E5%99%A8-perceptron/>机器学习入门 - 感知器 (Perceptron)</a>和Wikipedia上面<a href=http://zh.wikipedia.org/wiki/%E6%84%9F%E7%9F%A5%E5%99%A8>感知机</a>的词条。</p><p>以及<a href=https://class.coursera.org/ntumlone-001/class>機器學習基石 (Machine Learning Foundations)</a>公开课的幻灯片。</p><h2 id=updated>Updated</h2><p>2013-12-8</p><p>修改了pocket-pla算法，提升了效率和准确性。</p><p>参考了<a href="https://class.coursera.org/ntumlone-001/forum/thread?thread_id=116#post-632">课程论坛</a>的讨论。并且感谢Li Tianyi同学指出我的问题。</p></section></main><footer id=main-footer><div class=footer><a href=#></a><div class=footer-copyright><div class=dim>© 2025</div><div></div></div></div></footer></div><aside id=side-pane class=side-sticky><div class=side-details><span>1098 </span><span>4 - 5</span></div><h3></h3><nav id=TableOfContents><ul><li><a href=#什么是pla算法>什么是PLA算法</a></li><li><a href=#pla算法的原理>PLA算法的原理</a></li><li><a href=#pla算法的错误修正>PLA算法的错误修正</a></li><li><a href=#naive-pla-与-pocket-pla>Naive PLA 与 Pocket PLA</a><ul><li><a href=#naive-pla>Naive PLA</a></li><li><a href=#pocket-pla>Pocket PLA</a></li></ul></li><li><a href=#参考链接>参考链接</a></li><li><a href=#updated>Updated</a></li></ul></nav><h3></h3><ul><li><a href=/posts/cse351-lab1/>CSE351 - Lab 1: Manipulating Bits Using C</a></li><li><a href=/posts/washington-university-cse351/>CSE 351 - Hardware/Software Interface</a></li></ul></aside></div></div></body></html>