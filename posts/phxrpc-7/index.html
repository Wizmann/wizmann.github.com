<!doctype html><html lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=Content-Security-Policy content="script-src 'self'; style-src 'self' 'unsafe-inline'; object-src 'none'; base-uri 'none'"><link rel=stylesheet href=/css/style.feedcd92d05c2cf19ee7487656b358ee4805f831b7b39711851199d0a6f8934cf9ba379d02425d485c5b65ae299bdffda3770739545d595074076bf3ead284ab.css media=screen integrity="sha512-/u3NktBcLPGe50h2VrNY7kgF+DG3s5cRhRGZ0Kb4k0z5ujedAkJdSFxbZa4pm9/9o3cHOVRdWVB0B2vz6tKEqw==" crossorigin=anonymous><title>RPC - phxrpc代码阅读(7)</title>
<meta name=description content="前言 看了这么久代码，终于我们要接近phxrpc的核心部分了。
但是出人意料的是，rpc部分并没有过多的概念和magic trick。而且因为ucontext已经被封装好了，所以在rpc里的操作，可以完全按照同步的写法来搞，开发者们不需要切换同步异步的思维模式，就可以在底层的封装之上，做自己想做的事了。
"><link rel=canonical href=https://wizmann.top/posts/phxrpc-7/><link rel=alternate hreflang=zh-CN href=https://wizmann.top/posts/phxrpc-7/><link rel=alternate hreflang=x-default href=https://wizmann.top/posts/phxrpc-7/><meta property="og:title" content="RPC - phxrpc代码阅读(7)"><meta property="og:description" content="前言
看了这么久代码，终于我们要接近phxrpc的核心部分了。
但是出人意料的是，rpc部分并没有过多的概念和magic trick。而且因为ucontext已经被封装好了，所以在rpc里的操作，可以完全按照同步的写法来搞，开发者们不需要切换同步异步的思维模式，就可以在底层的封装之上，做自己想做的事了。"><meta property="og:type" content="article"><meta property="og:url" content="https://wizmann.top/posts/phxrpc-7/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2016-10-22T23:03:36+00:00"><meta property="article:modified_time" content="2016-10-22T23:03:36+00:00"><meta property="og:site_name" content="Maerlyn's Rainbow"><meta name=twitter:card content="summary"><meta name=twitter:title content="RPC - phxrpc代码阅读(7)"><meta name=twitter:description content="前言
看了这么久代码，终于我们要接近phxrpc的核心部分了。
但是出人意料的是，rpc部分并没有过多的概念和magic trick。而且因为ucontext已经被封装好了，所以在rpc里的操作，可以完全按照同步的写法来搞，开发者们不需要切换同步异步的思维模式，就可以在底层的封装之上，做自己想做的事了。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"RPC - phxrpc代码阅读(7)","datePublished":"2016-10-22T23:03:36+00:00","dateModified":"2016-10-22T23:03:36+00:00","mainEntityOfPage":"https://wizmann.top/","publisher":{"@type":"Organization","name":"Maerlyn's Rainbow"},"wordcount":2512,"description":"前言 看了这么久代码，终于我们要接近phxrpc的核心部分了。\n但是出人意料的是，rpc部分并没有过多的概念和magic trick。而且因为ucontext已经被封装好了，所以在rpc里的操作，可以完全按照同步的写法来搞，开发者们不需要切换同步异步的思维模式，就可以在底层的封装之上，做自己想做的事了。\n","keywords":null}</script></head><body class="posts single d-flex flex-column min-vh-100"><header class=main-header><nav class="navbar navbar-expand-lg"><div class=container><a class=navbar-brand href=/>Maerlyn's Rainbow
</a><button class=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="menu-main navbar-nav me-auto mb-2 mb-lg-0"></ul></div></div></nav></header><div id=content><div class="container py-3"><h2 id=前言>前言</h2><p>看了这么久代码，终于我们要接近phxrpc的核心部分了。</p><p>但是出人意料的是，rpc部分并没有过多的概念和magic trick。而且因为ucontext已经被封装好了，所以在rpc里的操作，可以完全按照同步的写法来搞，开发者们不需要切换同步异步的思维模式，就可以在底层的封装之上，做自己想做的事了。</p><h2 id=线程安全的队列---threadqueue>线程安全(?)的队列 - ThreadQueue</h2><p>我不知道开发者为啥要起<code>ThdQueue</code>这样令人迷惑的名字，这种诡异的命名风格贯穿了整个代码。咋一看这个类是maintain一堆线程的，类似于线程池，但其实这个类就是一个<code>BlockingQueue</code>的实现。</p><p>之后，这个队列有三种操作，<code>push</code>、<code>pluck</code>和<code>break_out</code>。push操作不用多说，pluck对应的我们所理解的pop操作，即从队列中弹出元素（pluck这个词貌似是从grpc里面来的，那我就不吐槽了，毕竟Google爸爸）。</p><p>更令人疑惑的是<code>break_out</code>这个操作。从代码来看，像是清空队列，并且在dtor中也显式的调用了这个函数。</p><p>但是有以下的几个问题。</p><p>一，<code>break_out_</code>是一个bool变量，且在不同线程间共享，问题在于这个变量可能被cache住，直接访问可能会造成非预期的结果，可能需要<code>volitaile</code>，或者在<code>pluck</code>函数里加一个mem barrier。</p><p>二来，在析构函数中调用<code>break_out_</code>，有可能的一种情况是有其它线程还在<code>pluck</code>函数中，而<code>ThdQueue</code>对象已经被析构了，我们就需要承担这种不安全行为的后果（此处有广告：大铁棍子医院捅主任，张姐去了都说好）。</p><p>当然，如果这个函数只在结束进程时使用，其实写的糙一点也无所谓，因为毕竟线上服务是没有“退出”这种状态的。当我们要清空队列时，已经不需要对外提供服务，之后直接<code>kill -9</code>就好，不会触发多线程的坑。不过，这里我觉得应该还是要加小心。</p><h2 id=uthreadcaller>UThreadCaller</h2><p>这个破类让我看了一小时，分析它的keepalive是怎么实现的。结果发现这个类被没有被调用。</p><p>GG。</p><p><img src=https://github.com/Wizmann/assets/raw/master/wizmann-pic/16-10-22/54117736.jpg alt></p><h2 id=一个超级文件---hshaserver>一个超级文件 - HshaServer</h2><blockquote><p>不知道为啥开发者要把这么多文件写一块，拆开不好吗？</p></blockquote><h3 id=dataflow>DataFlow</h3><p>DataFlow包含了Request和Response两个Queue，还附加了入队的时间戳和一个args参数指针。</p><h3 id=hshaserverstat>HshaServerStat</h3><p>一个统计类。会在后台新建一个线程，约每一秒打印一次统计日志。</p><p>这个类里有一个技巧，在<code>CallFunc()</code>函数中，每一秒循环一次并没有使用sleep家族的函数，也没有使用select的超时。而是使用了<code>condtional variable</code>。</p><p><code>std::condition_variable::wait_for</code>函数，实质是就是带超时的等待。而这里，在一般状态下，是没有线程会notify的，所以wait_for函数会睡满1s。但是在退出时，会显式的notify统计线程，破坏等待状态，使统计线程退出。</p><p><code>wait_for</code>函数的具体用法，可以参考<a href=http://en.cppreference.com/w/cpp/thread/condition_variable/wait_for target=_blank rel=noopener>文档</a>。</p><p>下面的<code>HshaServerQos</code>也是一样的思路，Qos即“Quality of service”。</p><h3 id=worker和workerpool>Worker和WorkerPool</h3><p>这两个类其实是一个和一堆的关系，不过由于这里的诡异的写法，导致一个依赖一堆，一堆调用一个。</p><p>WorkerPool是一个全局的线程池，里面有线程（废话），输入输出队列，Disipatcher和调度器。所以Worker要反过来依赖WorkerPool里面的数据。造成了很大的耦合性。</p><p>Worker从输入队列中获取信息，并且使用<code>dispatcher</code>进行CPU密集的处理（我觉得<code>dispatcher</code>这个名字起的也有问题）。之后将结果放入输出队列，由后面的<code>HshaServerIO::ActiveSocketFunc</code>驱动协程库进行之后的IO操作。</p><h3 id=完成调度器---hshaserverio>完成调度器 - HshaServerIO</h3><p>这个类的主要作用就是补全调度器缺少的函数，并提供了一个IO的工作函数<code>HshaServerIO::IOFunc</code>。</p><p>调度器的工作流程前面已经说过了，我们现在就从更具体化的实现上来阅读一下。</p><p><code>HshaServerIO :: AddAcceptedFd</code>，这个函数由外部调用，传入已经accept的fd，之后<code>HshaServerIO::HandlerAcceptedFd</code>将这个fd，和IO工作函数<code>IOFunc</code>一起放入调度器中进行调度。</p><p>工作函数<code>IOFunc</code>只负责将请求放入队列，而并不负责从输出队列中取出响应。这个事情由<code>HshaServerIO::ActiveSocketFunc</code>负责。</p><p>换句话说，在调度器的工作循环中，<code>epoll_wait</code>中等待的只有在进行IO的两种fd，一是读还没读完的，二是写还没写完的。</p><p>进行完CPU操作的fd，由<code>active_socket_func_</code>函数重新激活，向客户端写回响应。所以这个函数应该叫<code>activate_socket_with_resp_func_</code>更合适一些。（至少第一个单词得是个动词好不。）</p><p>后面的keepalive的处理也是非常浅显的，这里就不多说了。</p><h3 id=多线程io---hshaserverunit和hshaserver>多线程IO - HshaServerUnit和HshaServer</h3><p>前面我们说了不少协程的事，但这并不代表我们不使用多线程带来的红利。或者至少在性能不符合预期的时候，用多线程来tuning一下。</p><p>HashServerUnit包装了一组线程，其中包括一个IO线程和若干CPU线程。我们在HshaServer中，还可以配置多个Unit，使得我们有多个IO线程，充分榨干CPU和IO的每一滴汗水。</p><p>由于手里也没有测试数据，也就不能更详细的来说配置服务参数的策略。但是无责任猜测，IO线程应该不超过3个。CPU线程数目应该略多于CPU核数。</p><h3 id=一个独立的acceptor>一个独立的Acceptor</h3><p><code>HshaServerAcceptor</code>类相对比较独立，它是用来接受访问请求。是主线程的工作循环。</p><p>这里比较奇怪的是，<code>LoopAccept</code>函数设置了CPU亲和性。使得控制线程只在CPU0上运行。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span>    cpu_set_t mask;
</span></span><span style=display:flex><span>    CPU_ZERO(<span style=color:#f92672>&amp;</span>mask);
</span></span><span style=display:flex><span>    CPU_SET(<span style=color:#ae81ff>0</span>, <span style=color:#f92672>&amp;</span>mask);
</span></span><span style=display:flex><span>    pid_t thread_id <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>int</span> ret <span style=color:#f92672>=</span> sched_setaffinity(thread_id, <span style=color:#66d9ef>sizeof</span>(mask), <span style=color:#f92672>&amp;</span>mask);
</span></span></code></pre></div><p>具体原因有待探讨，可能是和中断亲和性有关。</p><h2 id=写在后面>写在后面</h2><p>总算囫囵吞枣的把这RPC读完了，其实这里还是有好多疑问的。但是由于phxrpc的文档实在是。。。基本算是没有吧。所以可能还要去Github上提一波Issue。</p><p>在学习过程中，真的感觉自己懂的还是太少。简直药丸。</p><p>还需要更加努力才好。</p></div></div><footer class="py-3 mt-auto bg-light"><div class="container py-1 my-1"><div class="d-flex flex-wrap justify-content-between align-items-center"><p class="col-md mb-0 text-muted"></p><ul class="nav col-md-auto justify-content-end"></ul></div></div></footer><script src=/js/main.min.e8d88c82c0438b527f1aca4115652ba1e2877bf805b75593b23ac0e3fe2b3fe95a467d1feef275355132ba061da6404b0d24d55afabc209b294b1db043be014d.js integrity="sha512-6NiMgsBDi1J/GspBFWUroeKHe/gFt1WTsjrA4/4rP+laRn0f7vJ1NVEyugYdpkBLDSTVWvq8IJspSx2wQ74BTQ==" crossorigin=anonymous defer></script></body></html>