<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Azure on Maerlyn's Rainbow</title><link>https://wizmann.top/tags/azure/</link><description>Recent content in Azure on Maerlyn's Rainbow</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Mon, 18 Dec 2017 00:49:00 +0000</lastBuildDate><atom:link href="https://wizmann.top/tags/azure/index.xml" rel="self" type="application/rss+xml"/><item><title>Bw-Tree：在新硬件平台上的新B-Tree</title><link>https://wizmann.top/posts/bw-tree/</link><pubDate>Mon, 18 Dec 2017 00:49:00 +0000</pubDate><guid>https://wizmann.top/posts/bw-tree/</guid><description>&lt;h2 id="ars-与-bw-tree">ARS 与 Bw-Tree &lt;a href="#ars-%e4%b8%8e-bw-tree" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>nosql数据库从本质上说，都属于ARS（Atomic Record Stores，“原子记录存储”）。&lt;/p>
&lt;p>最常见的“原子记录存储”一种实现就是朴素的Hash表：通过一个特定的key，来读写一条独立的数据记录。&lt;/p>
&lt;p>一些基于key-value（键-值）模型的nosql的内部实现正是使用了Hash表，例如Redis&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>、memcache、Riak等。&lt;/p>
&lt;p>而有一些nosql数据库为了支持高效的key-sequential（键-序列）查找，采用了tree-based&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>数据结构进行数据存储，所以B-Tree成为了一些数据库（both nosql and sql）的首选。&lt;/p>
&lt;p>本文介绍了一种基于新型硬件的高性能ARS实现，其核心技术有：&lt;/p>
&lt;ol>
&lt;li>页式内存管理&lt;/li>
&lt;li>Bw-Tree，一种基于B-Tree的树存储结构&lt;/li>
&lt;li>基于日志的存储管理&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-8/19953353.jpg" alt="">&lt;/p>
&lt;h2 id="在现代硬件上的bw-tree">在现代硬件上的Bw-Tree &lt;a href="#%e5%9c%a8%e7%8e%b0%e4%bb%a3%e7%a1%ac%e4%bb%b6%e4%b8%8a%e7%9a%84bw-tree" class="anchor">🔗&lt;/a>&lt;/h2>&lt;h3 id="现代多核cpu">现代多核CPU &lt;a href="#%e7%8e%b0%e4%bb%a3%e5%a4%9a%e6%a0%b8cpu" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>多核CPU带来了高并发，但是同时也引入了锁竞争和缓存失效问题。&lt;/p>
&lt;p>为了讲解锁竞争，这里我们举一个经典的例子：多线程计数器。&lt;/p>
&lt;p>我们都知道，如果不使用锁来保护计数器变量的话，那么最终的结果几乎不可能是正确的。多个线程中，只能有唯一一个线程可以持有锁，其它的线程都需要等待。这样就影响到了程序的并发度。&lt;/p>
&lt;blockquote>
&lt;p>这里一些高级玩家会提到CAS（compare-and-swap）命令。CAS翻译成中文就是“比较并交换”，用在多线程编程中实现数据交换操作。详情请见：&lt;a href="https://en.wikipedia.org/wiki/Compare-and-swap">比较并交换&lt;/a>&lt;/p>&lt;/blockquote>
&lt;p>缓存失效产生于两种场景。一是上文提到的锁争用，即每次锁争用都会让当前线程陷入内核态，产生上下文切换，导致CPU缓存的失效。二是缓存一致性协议导致的缓存行失效，即多个CPU在共享一个共同的主存资源时，为了保证缓存中的数据一致性，在主存资源被修改后，相应的缓存行也会失效。&lt;/p>
&lt;p>Bw-Tree为了解决以上的问题，使用了以下两个措施。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>无锁数据结构 &lt;br>
避免了系统阻塞在锁上，更避免了由于锁争用导致的缓存失效以及上下文切换。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>增量更新 &lt;br>
使用增量更新而不是原地更新，避免修改共享的主存资源，规避缓存一致性产生的缓存失效问题。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="现代存储设备">现代存储设备 &lt;a href="#%e7%8e%b0%e4%bb%a3%e5%ad%98%e5%82%a8%e8%ae%be%e5%a4%87" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>数据库系统的一个重要瓶颈就是IO性能。SSD（固态硬盘）提供了比传统机械 硬盘更好的读写性能，虽然表面上缓解了问题，但是IO仍然可能是整个系统的瓶颈。&lt;/p>
&lt;p>所以，Bw-Tree使用大内存做为读缓存，同时采用append-only结构保存写信息。因为不论是SSD还是机械硬盘，顺序读写的性能都非常出色。&lt;/p>
&lt;h2 id="bw-tree的页式存储结构">Bw-Tree的页式存储结构 &lt;a href="#bw-tree%e7%9a%84%e9%a1%b5%e5%bc%8f%e5%ad%98%e5%82%a8%e7%bb%93%e6%9e%84" class="anchor">🔗&lt;/a>&lt;/h2>&lt;h3 id="映射表mapping-table">映射表（Mapping Table） &lt;a href="#%e6%98%a0%e5%b0%84%e8%a1%a8mapping-table" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>上文中我们提到了缓存一致性（cache coherance），简单来说，就是当我们修改多个CPU共享的内存资源时，会导致缓存的失效，进而影响指令的执行效率。&lt;/p>
&lt;p>Bw-Tree使用页式（paginated）存储管理与映射表解决了这个问题。Bw-Tree以“页”做为存储的单元，每一页都有一个PID。页的物理位置可以存储在主存上，也可以存储在磁盘上。&lt;/p>
&lt;p>映射表将PID所代表的映射到页所在的具体物理位置。所以当页的物理位置发生改变时，仍然保持PID一致，不需要对Bw-Tree的结构进行任何修改。&lt;/p>
&lt;h3 id="增量修改delta-updating">增量修改（Delta Updating） &lt;a href="#%e5%a2%9e%e9%87%8f%e4%bf%ae%e6%94%b9delta-updating" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-10/18893113.jpg" alt="">&lt;/p>
&lt;p>假设我们现在有数据&lt;code>D&lt;/code>，当有修改请求&lt;code>delta&lt;/code>到来时，我们可以将数据&lt;code>D&lt;/code>&lt;strong>原地&lt;/strong>修改为&lt;code>D'&lt;/code>。也可以使用增量更新的方法将其保存为&lt;code>delta + D&lt;/code>的形式。&lt;/p>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-18/98762144.jpg" alt="">&lt;/p>
&lt;p>增量更新看起来比原地更新耗费了更多的内存，但其优势也非常明显。一是修改不需要对页加锁，可以使用CAS无锁方法安全的更新共享资源。二是不需要修改页内存，不会导致现有cache失效。&lt;/p>
&lt;p>当增量过多时，Bw-Tree会自动合并增量和页内存，这个过程同样是无锁的。&lt;/p>
&lt;h3 id="弹性虚拟页elastic-virtual-pages">弹性虚拟页（Elastic Virtual Pages） &lt;a href="#%e5%bc%b9%e6%80%a7%e8%99%9a%e6%8b%9f%e9%a1%b5elastic-virtual-pages" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-10/91449803.jpg" alt="image">&lt;/p>
&lt;blockquote>
&lt;p>B+-Tree，图片来源：&lt;a href="https://www.cs.usfca.edu/~galles/visualization/BPlusTree.html">B+ Trees Visualization&lt;/a>&lt;/p>&lt;/blockquote>
&lt;p>B+-Tree的特性有：&lt;/p>
&lt;ol>
&lt;li>所有记录都在叶子节点（节点也被称为页）上，并且是顺序存放的&lt;/li>
&lt;li>每个非叶子节点包含关键字（references）与孩子指针&lt;/li>
&lt;li>叶子节点有一个指向左右兄弟的指针，方便顺序遍历数据&lt;/li>
&lt;/ol>
&lt;p>Bw-Tree在B+-Tree的基础上添加了：&lt;/p>
&lt;ol>
&lt;li>low/high key，用来标明本节点以及子节点的数据范围&lt;/li>
&lt;li>每个节点（包括叶子节点和中间节点）都有一个side pointer指向其右面的兄弟节点&lt;/li>
&lt;/ol>
&lt;h3 id="范围查询">范围查询 &lt;a href="#%e8%8c%83%e5%9b%b4%e6%9f%a5%e8%af%a2" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>由于Bw-Tree的数据都分布在叶子节点，并且叶子节点之间可以通过兄弟指针进行遍历。&lt;/p>
&lt;p>我们使用游标（cursor）来标记当前检索的位置。并且使用缓存当前页上符合条件的记录。这样可以加速逐条遍历操作（&amp;ldquo;next-record&amp;rdquo; functionality）。&lt;/p>
&lt;p>逐条遍历操作是原子的，但是逐条遍历某一段数据则不是原子的。如果读写同时发生在同一页，则我们会根据事务的同步级别来决定是否需要重建检索缓存。&lt;/p>
&lt;h3 id="垃圾回收">垃圾回收 &lt;a href="#%e5%9e%83%e5%9c%be%e5%9b%9e%e6%94%b6" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>无锁数据结构导致的一个结果，是我们无法显式的让某个页失效。这意味着，一些线程会持有过时的数据，所以在删除这些过时的数据时要非常小心。&lt;/p>
&lt;p>所以这里引入了“代”（epoch）的概念，每一个线程执行每一个操作都在某一个代中，并且这一代中的所有数据以及过时数据都不会被删除。当某一代中所有线程操作都被执行完毕后，这一代就会结束，其中的过时数据就会被安全的回收。&lt;/p>
&lt;h2 id="bw-tree的结构改变">Bw-Tree的结构改变 &lt;a href="#bw-tree%e7%9a%84%e7%bb%93%e6%9e%84%e6%94%b9%e5%8f%98" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>Bw-Tree的结构会在某些操作下进行改变，以维护其性质。结构改变的操作仍然是无锁的。&lt;/p>
&lt;p>为了方便理解，我们假设结构改变的写操作不是并发的。&lt;/p>
&lt;h3 id="节点分裂">节点分裂 &lt;a href="#%e8%8a%82%e7%82%b9%e5%88%86%e8%a3%82" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>当某线程发现Bw-Tree中某个节点的大小超出了系统设定的阈值后，在执行完其预定的任务后，就会触发节点的分裂操作。&lt;/p>
&lt;p>节点分裂分为两步执行，每一步都是原子的：&lt;/p>
&lt;ol>
&lt;li>子节点分裂&lt;/li>
&lt;li>子节点更新后，向上更新父节点&lt;/li>
&lt;/ol>
&lt;h4 id="子节点分裂">子节点分裂 &lt;a href="#%e5%ad%90%e8%8a%82%e7%82%b9%e5%88%86%e8%a3%82" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-13/53138696.jpg" alt="">&lt;/p>
&lt;p>上图是Bw-Tree中的一部分，现在我们想分裂&lt;code>PQ&lt;/code>节点为&lt;code>P&lt;/code>和&lt;code>Q&lt;/code>两个子节点。即大于某个值&lt;code>KP&lt;/code>的数据全部迁移到新节点&lt;code>Q&lt;/code>。&lt;/p>
&lt;p>节点分裂的操作如下：&lt;/p>
&lt;ol>
&lt;li>先拷贝Q中的数据到一个新的数据页&lt;code>Q'&lt;/code>，这一步并不会影响到其它的线程。&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-13/52526691.jpg" alt="">&lt;/p>
&lt;ol start="2">
&lt;li>在P节点，使用CAS操作，添加分裂的信息Δ，标明该节点已经分裂，其中包含着&lt;code>KP&lt;/code>的信息，所以当查询的Key大于&lt;code>KP&lt;/code>时，会跳转到新节点&lt;code>Q'&lt;/code>进行查询。&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-13/7152787.jpg" alt="">&lt;/p>
&lt;h4 id="父节点更新">父节点更新 &lt;a href="#%e7%88%b6%e8%8a%82%e7%82%b9%e6%9b%b4%e6%96%b0" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>同样，父节点的更新仍然是采用增量修改的模式。&lt;/p>
&lt;p>我们在父节点使用CAS操作添加一条修改增量（index term delta record）。当查询请求落在&lt;code>Q&lt;/code>节点的范围内时，会直接跳转到&lt;code>Q'&lt;/code>节点。&lt;/p>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-13/29669418.jpg" alt="">&lt;/p>
&lt;h4 id="更新合并">更新合并 &lt;a href="#%e6%9b%b4%e6%96%b0%e5%90%88%e5%b9%b6" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>使用增量更新可以减少数据更新、节点分裂时的时间开销，从而避免失败的分裂操作（failed splits）。&lt;/p>
&lt;p>但是最终，我们仍会将产生的更新增量合并成一条完整的记录，以避免过多的空间浪费和指针跳转。&lt;/p>
&lt;h3 id="子节点合并">子节点合并 &lt;a href="#%e5%ad%90%e8%8a%82%e7%82%b9%e5%90%88%e5%b9%b6" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>在Bw-Tree数据减少时，我们为了避免过多的指针跳转，需要进行节点合并。节点合并比节点分裂复杂一点，所以需要一系列的原子操作来进行。&lt;/p>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-14/37353459.jpg" alt="">&lt;/p>
&lt;ol>
&lt;li>删除节点&lt;/li>
&lt;/ol>
&lt;p>删除节点时使用标记删除，仍然是CAS操作，添加一个删除增量（remove node delta）到&lt;code>Q&lt;/code>。之后所有的读写请求全部转发到其左兄弟&lt;code>P&lt;/code>。&lt;/p>
&lt;blockquote>
&lt;p>这里有个猜测，在Q中原有的数据仍然是要到Q节点进行访问的。论文里没有说，通过常识推断一下应该如此。&lt;/p>&lt;/blockquote>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-14/79837579.jpg" alt="">&lt;/p>
&lt;ol start="2">
&lt;li>合并子节点&lt;/li>
&lt;/ol>
&lt;p>在&lt;code>P&lt;/code>节点之前用CAS操作添加合并增量（node merge delta）。合并增量将&lt;code>P&lt;/code>节点和&lt;code>Q&lt;/code>节点从逻辑上合为一个，并且负责转发读写请求。&lt;/p>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-14/81902381.jpg" alt="">&lt;/p>
&lt;h4 id="父节点更新-1">父节点更新 &lt;a href="#%e7%88%b6%e8%8a%82%e7%82%b9%e6%9b%b4%e6%96%b0-1" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>这个就非常简单，就是CAS操作在父节点标记删除&lt;code>Q&lt;/code>节点的Key即可。&lt;/p>
&lt;h3 id="序列化结构修改">序列化结构修改 &lt;a href="#%e5%ba%8f%e5%88%97%e5%8c%96%e7%bb%93%e6%9e%84%e4%bf%ae%e6%94%b9" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>上面的Bw-Tree结构修改的实现都基于一个假设：数据库的同步机制会处理好数据更新冲突问题。&lt;/p>
&lt;p>同步机制包括数据库系统的锁管理器（lock manager）、事务管理组件（transactional component）；或者是采用弃疗方案，允许随机交叉写。&lt;/p>
&lt;p>但是在Bw-Tree内部，我们需要序列化树的结构修改（俗称排队）。当一个SMO操作遇到了未完成的SMO操作时，当前线程会等待之前的SMO操作完成后再继续当前操作。&lt;/p>
&lt;p>这样的方案可能会形成一个“SMO栈”，但是这种场景比较少见，实现起来也比较简单。&lt;/p>
&lt;h2 id="缓存管理">缓存管理 &lt;a href="#%e7%bc%93%e5%ad%98%e7%ae%a1%e7%90%86" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>缓存层负责内存与SSD磁盘之间的读、写回以及换页操作。缓存层为Bw-Tree层提供了逻辑页的抽象。&lt;/p>
&lt;p>Bw-Tree使用PID访问逻辑页，当逻辑页位于磁盘中时，缓存层先把页读取到内存中，再使用CAS操作将PID指针从磁盘偏移量指向内存偏移量。&lt;/p>
&lt;h3 id="预写日志协议与日志序列号lsn">预写日志协议与日志序列号（LSN） &lt;a href="#%e9%a2%84%e5%86%99%e6%97%a5%e5%bf%97%e5%8d%8f%e8%ae%ae%e4%b8%8e%e6%97%a5%e5%bf%97%e5%ba%8f%e5%88%97%e5%8f%b7lsn" class="anchor">🔗&lt;/a>&lt;/h3>&lt;ul>
&lt;li>日志序列号（LSN）&lt;/li>
&lt;/ul>
&lt;p>插入或修改Bw-Tree的增量都会被打上日志序列号。日志序列号由事务子系统产生。&lt;/p>
&lt;p>在页的刷写增量（flush delta）中，记录着最后被刷入（flush）磁盘的日志序列号。&lt;/p>
&lt;ul>
&lt;li>事务日志协调&lt;/li>
&lt;/ul>
&lt;p>事务子系统维护着一个“最大写入日志序列号”（End of Stable Log, ESL）。事务子系统通过ESL来限制数据管理子系统的持久化进度，也就是说，数据管理子系统只能持久化事务号小于ESL的事务。这保证了预写日志协议的因果性。&lt;/p>
&lt;p>数据管理子系统通过向事务子系统反馈最后刷写的事务序号以提供反馈。当事务子系统想要向前移动“事务重做指针”（Redo-Scan-Start-Point, RSSP）时，会向事务子系统发送请求。当事务序号大于RSSP，且小于ESL的所以事务全部刷写到磁盘之后，数据管理子系统会向事务子系统发送ACK信号。最后事务子系统向前移动RSSP，已经落盘的事务信息抛弃。&lt;/p>
&lt;ul>
&lt;li>Bw-Tree的结构修改&lt;/li>
&lt;/ul>
&lt;p>我们将Bw-Tree的结构修改（SMO）封装成为一个系统事务。由于Bw-Tree的结构修改是无锁的，所以会有多个线程操作同一页的情况。&lt;/p>
&lt;p>将SMO视做事务，允许我们并发的执行SMO操作，当且仅当某一个SMO操作“胜出”时，才将事务提交。&lt;/p>
&lt;h3 id="将页写回日志式存储lss">将页写回日志式存储（LSS） &lt;a href="#%e5%b0%86%e9%a1%b5%e5%86%99%e5%9b%9e%e6%97%a5%e5%bf%97%e5%bc%8f%e5%ad%98%e5%82%a8lss" class="anchor">🔗&lt;/a>&lt;/h3>&lt;ul>
&lt;li>页封装（Page Marshalling）&lt;/li>
&lt;/ul>
&lt;p>我们如果要将页写回LSS，首先要先逻辑页转化成线性顺序存储的结构，便于我们将其存储到磁盘。&lt;/p>
&lt;p>然后我们要将页的状态设定为“锁定”（captured），以防止后续的修改破坏预写日志协议。&lt;/p>
&lt;ul>
&lt;li>增量刷写（Incremental Flushing）&lt;/li>
&lt;/ul>
&lt;p>当刷写页面时，缓存管理器只会刷写ESL之前的数据记录。这样的好处有：&lt;/p>
&lt;ol>
&lt;li>减少磁盘的使用&lt;/li>
&lt;li>避免产生过多的磁盘垃圾&lt;/li>
&lt;li>避免SSD的写入放大开销&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>刷写操作&lt;/li>
&lt;/ul>
&lt;p>使用双Buffer进行磁盘刷写，可以避免线程等待IO操作。&lt;/p>
&lt;p>在刷写操作完成之后，我们会更新mapping table，将页指针改变成磁盘的偏移量。并将内存中的页换出。&lt;/p>
&lt;p>缓存管理器会监视系统内存的使用情况，如果内存不够用，就将一部分页从自动内存中换出。&lt;/p>
&lt;h2 id="性能">性能 &lt;a href="#%e6%80%a7%e8%83%bd" class="anchor">🔗&lt;/a>&lt;/h2>&lt;h3 id="我们的对手">我们的对手 &lt;a href="#%e6%88%91%e4%bb%ac%e7%9a%84%e5%af%b9%e6%89%8b" class="anchor">🔗&lt;/a>&lt;/h3>&lt;ul>
&lt;li>BerkeleyDB&lt;/li>
&lt;/ul>
&lt;p>性能良好的独立存储引擎。我们使用C语言实现版本，内置存储引擎为B-Tree。&lt;/p>
&lt;p>运行模式为性能较好的“无事务模式”，支持一写多读。&lt;/p>
&lt;p>在整个实验中，我们将BerkeleyDB的缓存开到足够大，全部使用主存进行存储与查询操作。&lt;/p>
&lt;ul>
&lt;li>SkipList&lt;/li>
&lt;/ul>
&lt;p>无锁的跳表实现。&lt;/p>
&lt;h3 id="测试数据集">测试数据集 &lt;a href="#%e6%b5%8b%e8%af%95%e6%95%b0%e6%8d%ae%e9%9b%86" class="anchor">🔗&lt;/a>&lt;/h3>&lt;ul>
&lt;li>XBox Live &lt;br>
网游数据。Key的平均大小是94Bytes，Value的平均大小是1.2K。读写比大概是7.5: 1&lt;/li>
&lt;li>商业数据库的备份流 &lt;br>
总共有27M数据块，去重后有12M。读写比为2.2：1，Key是20Bytes的SHA1哈希值，Value是44Bytes的元信息。&lt;/li>
&lt;li>综合KV数据 &lt;br>
8Bytes整数Key，8Bytes整数Value。读写比从5：1。&lt;/li>
&lt;/ul>
&lt;h3 id="比赛项目">比赛项目 &lt;a href="#%e6%af%94%e8%b5%9b%e9%a1%b9%e7%9b%ae" class="anchor">🔗&lt;/a>&lt;/h3>&lt;h4 id="增量链长度阈值对性能的影响">增量链长度阈值对性能的影响 &lt;a href="#%e5%a2%9e%e9%87%8f%e9%93%be%e9%95%bf%e5%ba%a6%e9%98%88%e5%80%bc%e5%af%b9%e6%80%a7%e8%83%bd%e7%9a%84%e5%bd%b1%e5%93%8d" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-17/40340311.jpg" alt="">&lt;/p>
&lt;p>增量链长度阈值影响触发页合并（consolidation）的频率。从图中我们做出如下推测：&lt;/p>
&lt;ol>
&lt;li>增量链长度较短时，读写性能好，但页合并开销大&lt;/li>
&lt;li>增量链长度转长时，读写开销大，页合并开销小&lt;/li>
&lt;/ol>
&lt;p>系统的整体性能就在这两个因素下面取一个平衡。&lt;/p>
&lt;h4 id="无锁环境下的更新失败">无锁环境下的更新失败 &lt;a href="#%e6%97%a0%e9%94%81%e7%8e%af%e5%a2%83%e4%b8%8b%e7%9a%84%e6%9b%b4%e6%96%b0%e5%a4%b1%e8%b4%a5" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-17/63409936.jpg" alt="">&lt;/p>
&lt;p>数据中我们可以看出，Bw-Tree写入失败（Failed Updates）是非常少的。但是“综合”数据集的“页拆分失败”和“页合并失败”明显高于其它数据集。&lt;/p>
&lt;p>这是因为“综合”数据集中的数据都比较小，更新起来比较快，所以会导致比较多的线程竞争。&lt;/p>
&lt;h4 id="与传统b-tree的比较">与传统B-Tree的比较 &lt;a href="#%e4%b8%8e%e4%bc%a0%e7%bb%9fb-tree%e7%9a%84%e6%af%94%e8%be%83" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-17/63409936.jpg" alt="">&lt;/p>
&lt;p>Bw-Tree明显胜出，其原因有：&lt;/p>
&lt;ol>
&lt;li>Bw-Tree是无锁的，所以并发程度更高&lt;/li>
&lt;li>Bw-Tree对CPU cache更加友好&lt;/li>
&lt;/ol>
&lt;h4 id="bw-tree与无锁跳表比较">Bw-Tree与无锁跳表比较 &lt;a href="#bw-tree%e4%b8%8e%e6%97%a0%e9%94%81%e8%b7%b3%e8%a1%a8%e6%af%94%e8%be%83" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-17/63409936.jpg" alt="">&lt;/p>
&lt;p>Bw-Tree仍然明显胜出，其原因归于Bw-Tree更友好使用了CPU cache。&lt;/p>
&lt;h4 id="缓存性能">缓存性能 &lt;a href="#%e7%bc%93%e5%ad%98%e6%80%a7%e8%83%bd" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-17/63354469.jpg" alt="">&lt;/p>
&lt;p>Bw-Tree的优势有：&lt;/p>
&lt;ol>
&lt;li>与跳表相比不需要过多的指针跳转&lt;/li>
&lt;li>内存使用更加高效&lt;/li>
&lt;/ol>
&lt;h2 id="后续201026">后续（201026） &lt;a href="#%e5%90%8e%e7%bb%ad201026" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>到了新组之后，发现了一个使用BW-Tree的存储引擎。不过已经即将废弃了（挥手&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>特指Redis的一些主要功能&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>tree-based数据结构的一个例子就是图书馆。图书馆用A-Z代表大类，然后使用数字代表小类，最后加上一个数字确定这本书的书号。 &lt;br>
图书馆藏书的顺序是按照书号的顺序进行排序的。所以我们在查找一本书的时候，可以使用特定的书号，利用其中的层次信息查找某一本书，也可以根据某一个书号范围进行查找一系列连续的书。 &lt;br>
如果还弄不明白的话，可以使用&lt;a href="http://www.ztflh.com/">中国图书馆分类法网站&lt;/a>加深一下理解。&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Windows Azure Storage Made Simple</title><link>https://wizmann.top/posts/azure-storage-made-simple/</link><pubDate>Wed, 11 Oct 2017 21:12:15 +0000</pubDate><guid>https://wizmann.top/posts/azure-storage-made-simple/</guid><description>&lt;h2 id="加机器就是一把梭">加机器就是一把梭 &lt;a href="#%e5%8a%a0%e6%9c%ba%e5%99%a8%e5%b0%b1%e6%98%af%e4%b8%80%e6%8a%8a%e6%a2%ad" class="anchor">🔗&lt;/a>&lt;/h2>&lt;blockquote>
&lt;p>没有什么问题是加一千台机器解决不了的，如果有，就再加一千台。 &lt;br>
—— 《21天精通分布式系统》&lt;/p>&lt;/blockquote>
&lt;p>分布式系统在设计之初，是为了解决单机系统的可用性和可扩展性问题的。&lt;/p>
&lt;p>举个例子，单机系统就是雇一个小弟替你干活，但是这个小弟不太靠谱，偶尔泡个病号不上班，偶尔工作太多一个人干不过来。&lt;/p>
&lt;p>分布式系统就是雇一群小弟帮你干活，偶尔有一两个小弟泡病号，我们会有富裕的人力顶上，工作太多我们可以继续拉新的小弟入伙，美滋滋。&lt;/p>
&lt;p>这个比喻很好的描述了单机系统和分布式系统之间的关系。所以一种可能的分布式系统就是这个样子的：&lt;/p>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-9-25/28278606.jpg" alt="">&lt;/p>
&lt;p>我们将相同功能的服务器组成一个整体，通过一个load balancer对外提供服务。&lt;/p>
&lt;p>这样的系统初步解决了我们的问题，在面对可扩展性和可用性的问题时，我们会：&lt;/p>
&lt;ul>
&lt;li>容量不足就加机器&lt;/li>
&lt;li>单机挂掉了就把流量调度到其它的节点上&lt;/li>
&lt;/ul>
&lt;p>不过单纯的加机器并不能完全满足我们的需要。对于CPU密集型的服务，增加副本数可以有效的均摊计算压力，但是对于存储密集型的服务，我们需要增加分库逻辑才能有效的增加系统的计算能力。&lt;/p>
&lt;p>例如我们有100G的数据，但是数据库的容量最多只支持50G。这样无论怎么样增加副本都不能解决问题。如果我们将100G的数据均分，存储在两个50G的分库上，我们就可以支持单机系统容纳不了的数据了。&lt;/p>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-9-22/14141171.jpg" alt="">&lt;/p>
&lt;p>我们还可以把多个这样的分布式子系统组合起来，就可以组成一个小有规模的分布式系统了。现在我们在使用的一些服务，仍在使用这种模型。&lt;/p>
&lt;p>上面分布式模型虽然有效，但是引入了一个严重的问题：因为节点之间是隔离的，并且只能通过消息传递进行通信与协调，所以基本无法完全保证副本之间保持一致的内部状态。&lt;/p>
&lt;p>这就是所谓的一致性问题。&lt;/p>
&lt;h2 id="补充一点理论知识">补充一点理论知识 &lt;a href="#%e8%a1%a5%e5%85%85%e4%b8%80%e7%82%b9%e7%90%86%e8%ae%ba%e7%9f%a5%e8%af%86" class="anchor">🔗&lt;/a>&lt;/h2>&lt;h3 id="cap理论">CAP理论 &lt;a href="#cap%e7%90%86%e8%ae%ba" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>CAP理论指的是：&lt;/p>
&lt;blockquote>
&lt;p>一个分布式系统不可能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三个基本要求，最多只能同时满足其中的两项。&lt;/p>&lt;/blockquote>
&lt;p>CAP理论由Eric Brewer在2000年首次提出，并且2年后被Seth Gilbert和Nancy Lynch从理论上证明这个理论。&lt;/p>
&lt;ul>
&lt;li>一致性是指数据的多个副本之间保持一致的特性；&lt;/li>
&lt;li>可用性是指系统的服务必须一直处于可用的状态;&lt;/li>
&lt;li>而分区容错性是指系统在遭遇任何网络分区故障的时候仍然能够保证对外提供满足一致性和可用性的服务，除非是整个网络环境都发生了故障;&lt;/li>
&lt;/ul>
&lt;p>抛开一致性问题不谈，“可用性”和“分区容错性”从某种意义上讲是一个分布式系统的“标配”：单机和网络连接并不可靠，从一堆不可靠的硬件上层建立一个不可靠的系统并没有任何意义。&lt;/p>
&lt;p>所以一致性常常被牺牲，来保证分布式系统的可用性和分区容错性。&lt;/p>
&lt;h3 id="paxos算法">Paxos算法 &lt;a href="#paxos%e7%ae%97%e6%b3%95" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>Paxos是Leslie Lamport在1990年提出的一种基于消息传递且具有高度容错性的一致性算法。&lt;/p>
&lt;p>Paxos解决的问题是如何在一个可能发生上述异常的分布式系统中，快速且正确地在集群内部对某个数据的值达成一致，并且保证不论发生任何异常，都不会破坏整个系统的一致性。&lt;/p>
&lt;p>Paxos的描述和证明有一些复杂，这里不做展开。简单来说，就是系统中的各个节点使用类似于选举投票的算法来达成一致，选举过程中会有短暂的数据不可用中间状态。&lt;/p>
&lt;h2 id="我最讨厌我想做的事情做不成">“我最讨厌我想做的事情做不成” &lt;a href="#%e6%88%91%e6%9c%80%e8%ae%a8%e5%8e%8c%e6%88%91%e6%83%b3%e5%81%9a%e7%9a%84%e4%ba%8b%e6%83%85%e5%81%9a%e4%b8%8d%e6%88%90" class="anchor">🔗&lt;/a>&lt;/h2>&lt;blockquote>
&lt;p>在分布式环境中，一致性是指数据在多个副本之间是否能够保持一致的特性。在一致性的需求下，当一个系统在数据一致的状态下执行更新操作后，应该保证系统的数据仍然处于一致的状态。 &lt;br>
—— 《从Paxos到ZooKeeper》&lt;/p>&lt;/blockquote>
&lt;p>无数的理论和实践证明，解决分布式系统的一致性问题是非常困难的。从最直观上来说，我们找不到两片一样的树叶，也找不到两台状态完全一致的分布式节点。&lt;/p>
&lt;p>但是前人的智慧是无穷的，我们仍然有以下几种方案来保证数据的一致性：&lt;/p>
&lt;h3 id="2pc-and-3pc">2PC and 3PC &lt;a href="#2pc-and-3pc" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>2PC（Two-phase Commit，两阶段提交）是用来保证分布式系统一致性的一个协议。其流程可以由这个例子来表示：&lt;/p>
&lt;blockquote>
&lt;p>PM（协调者）: 我要加个需求！（阶段1，提交事务请求） &lt;br>
Dev1（参与者）：你加吧 &lt;br>
Dev2（参与者）：你加吧 &lt;br>
PM（协调者）：我真加了！（阶段2，执行事务提交） &lt;br>
Dev1（参与者）：加完了 &lt;br>
Dev2（参与者）：加完了&lt;/p>&lt;/blockquote>
&lt;blockquote>
&lt;p>注：PM和Dev是程序员老梗而已。各个职业只是分工不同，本质上都是为人民服务。&lt;/p>&lt;/blockquote>
&lt;p>整个流程看起来很美好，但是我们会遇到如下问题：&lt;/p>
&lt;ol>
&lt;li>同步阻塞 &lt;br>
在提交过程中，系统需要等待所有参与者的响应，所以系统整体处于一种阻塞状态，影响效率。&lt;/li>
&lt;li>单点问题 &lt;br>
协调者是系统的单点，如果当掉，整个系统就处于不可用状态。&lt;/li>
&lt;li>数据不一致 &lt;br>
一个数据一致性协议会导致数据不一致，确实是非常讽刺的事情。在执行事务提交阶段，如果协调者与参与者的通信中断，可能会导致数据的不一致：
&lt;ul>
&lt;li>当协调者发往部分参与者的Commit消息丢失，没有收到Commit的参与者会因为超时而认为这次请求失败而进行回滚&lt;/li>
&lt;li>当参与者发往协调者的ACK消息丢失，协调者会认为这次请求失败，但是参与者确实已经Commit了数据&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>3PC（Three-phace Commit，三阶段提交）是2PC协议的改进版，将整个请求分为三部分：&lt;/p>
&lt;ul>
&lt;li>事务询问&lt;/li>
&lt;li>预提交&lt;/li>
&lt;li>执行提交&lt;/li>
&lt;/ul>
&lt;p>这样的改进减少了同步阻塞的粒度，但是仍然没有解决单点问题和数据不一致的问题。所以2PC和3PC都不是非常理想的数据一致性协议。&lt;/p>
&lt;h3 id="paxos">Paxos &lt;a href="#paxos" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>上文中我们提到Paxos是一种被数学证明过的，具有高度容错性的一致性算法。但是其流程复杂，通信次数较多，并且在提案未被选中之前不能保证数据的可用性。&lt;/p>
&lt;p>所以Paxos的应用场景多为命名服务、配置管理、分布式锁等轻量级但是而又非常核心的服务，从而扬长避短，更好的为人民服务，&lt;del>更好的反三俗&lt;/del>。。&lt;/p>
&lt;h3 id="quorum-like-algorithm-amazon-dynamo">Quorum-like Algorithm (Amazon Dynamo) &lt;a href="#quorum-like-algorithm-amazon-dynamo" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>对于分布式系统的一致性，最直观的解决方案就是在修改节点状态时，将修改广播到所有节点上。这样从理论上讲，就可以保持节点的一致性了。&lt;/p>
&lt;p>但是在现实工程当中，广播修改是最不可靠的方案，因为通信可能失败，节点可能失效，我们的修改很有可能不会同步到所有的节点上（假设每个节点的可靠性为99.9%，那么10个节点同时可靠概率只有99%，100个节点同时可靠的概率为90%）。&lt;/p>
&lt;p>不过，让所有节点都保持完全一致虽然很完美，但是其付出的代价也许是现有 的系统难以承受的。Amazon Dynamo使用了一种类似选举的算法来保证数据的一致性。&lt;/p>
&lt;p>设我们有N个节点，我们再指定两个数R和W，使得 R + W &amp;gt; N。在写入数据的时候，我们同时向N个节点写入，当有W个（及以上）节点写入成功，就向客户端返回成功；在读取的时候，我们向R个节点请求数据，此时一定会读到刚刚写入新数据的节点，然后我们将最新的数据返回给用户。&lt;/p>
&lt;p>不过这种设计并不完善，当出现网络错误或者大规模节点失效的时候，数据一致性就很难保证了。&lt;/p>
&lt;p>我称这种一致性模型为 —— “尽力一致性”。&lt;/p>
&lt;h3 id="event-stream-for-eventual-consistency-apache-kafka">Event Stream for Eventual Consistency (Apache Kafka) &lt;a href="#event-stream-for-eventual-consistency-apache-kafka" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>从Dynamo的经验中我们可以看出，直接将状态的修改广播到不同的节点是不可靠的。那么我们有没有办法解决这个问题呢？&lt;/p>
&lt;p>Apache Kafka使用了类似“预写式日志”的方法来保证一致性。当状态修改请求到来时，先写入一条公共的数据流，这条数据流往往是持久化到磁盘的。然后不同的节点再访问数据流，执行同样的状态修改请求，保证节点状态的一致性。&lt;/p>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-9-27/17974300.jpg" alt="">&lt;/p>
&lt;p>这种设计的缺陷是虽然最终所有节点都会依次执行相同的命令，但是节点执行命令的速度有快有慢，所以在瞬时会有不一致的情况。&lt;/p>
&lt;p>这种一致性模型被称为 —— “最终一致性”。&lt;/p>
&lt;h2 id="主角登场windows-azure-storage">主角登场：Windows Azure Storage &lt;a href="#%e4%b8%bb%e8%a7%92%e7%99%bb%e5%9c%bawindows-azure-storage" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>Windows Azure Storage（以下简称WAS）采用了多种一致性算法，在提供较高性能服务的同时，来努力保证存储系统的CAP性质。&lt;/p>
&lt;p>其中WAS使用了一些创新性的算法，不过我们还是先从它的整体架构谈起。&lt;/p>
&lt;ul>
&lt;li>Windows Azure Storage的架构&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-9-19/98830090.jpg" alt="">&lt;/p>
&lt;p>WAS从架构上分为三层，分别是前端（FE Layer），分区层（Partition Layer）和数据流层（Stream Layer）。这三层从功能上各有分工，但是又分别解耦，可以部署在不同的机器上。&lt;/p>
&lt;p>下面我们自底向上的讨论一下各层之间的功能与联系。&lt;/p>
&lt;h2 id="was---数据流层">WAS - 数据流层 &lt;a href="#was---%e6%95%b0%e6%8d%ae%e6%b5%81%e5%b1%82" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>数据流包含着各种数据操作，操作包含插入、修改、删除等基本操作，“流”则可以看做是一条无限长的队列。&lt;/p>
&lt;h3 id="为什么要使用流结构">为什么要使用流结构？ &lt;a href="#%e4%b8%ba%e4%bb%80%e4%b9%88%e8%a6%81%e4%bd%bf%e7%94%a8%e6%b5%81%e7%bb%93%e6%9e%84" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>为什么要用流结构呢，原因非常简单，因为流中的操作都是有序的，否则会造成不确定的结果。&lt;/p>
&lt;p>例如我们现在有三个操作：&lt;/p>
&lt;p>操作1：插入值x=1 &lt;br>
操作2：修改该值为x=2 &lt;br>
操作3：修改该值为x=5&lt;/p>
&lt;p>如果操作是按照顺序来做的话，那么我们获得的值为x=5。但是如果我们将操作2和操作3调换之后，那么我们获得的值为x=2；如果我们将操作1放置在操作2或操作3之后，我们就在修改一个不存在的值，这势必会造成错误。&lt;/p>
&lt;p>所以，数据流的有序性保证了操作的“回放一致性”，也就是说，相同的数据流应用在不同的机器上，获得的最终状态是一致的。&lt;/p>
&lt;p>同时，由于HDD的顺序读写性能优于随机读写性能。使用流数据结构可以将写入请求顺序化，追加写入磁盘，在廉价的硬件上获得最好的性能。&lt;/p>
&lt;h5 id="科普时间hdd与ssd">科普时间：HDD与SSD &lt;a href="#%e7%a7%91%e6%99%ae%e6%97%b6%e9%97%b4hdd%e4%b8%8essd" class="anchor">🔗&lt;/a>&lt;/h5>&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-10-10/63227271.jpg" alt="image">&lt;/p>
&lt;p>HDD指的是Hard Disk Drive，又称Spinning Disk，是计算机使用的一种以磁性碟片存储数据的一种非易失性存储设备（人话就是：断电后不丢数据）。数据存储在同心圆磁道上，工作时碟片会旋转（和CD机一样），磁头会移动到相应的磁道上进行数据的读写（CD机用的是光头）。所以，顺序读写时，HDD的性能会比随机读写要好，因为随机读写带来的大量磁盘寻址会让磁头不断移动，从而拖慢性能。&lt;/p>
&lt;p>与此同时，由于碟片内弧的旋转&lt;strong>线速度&lt;/strong>要小于外弧，所以外弧的读写性能会优于内弧。&lt;/p>
&lt;p>SSD指的是Solid State Disk，即固态硬盘，也是一种常见的非易失性存储设备。其使用闪存芯片来存储数据。由于其并不像HDD一样需要物理上移动磁头来进行寻址，在顺序写入与HDD或略强于HDD磁盘的同时，随机写入性能要远远强于HDD（这就是为什么我们装电脑都喜欢把系统装到SSD盘上）。&lt;/p>
&lt;p>但是SSD在成本和质量上弱于HDD磁盘。1TB的SSD的价格约为400刀，而1TB的HHD价格仅为80~100刀（2017年10月&lt;a href="https://pcpartpicker.com/trends/internal-hard-drive/">数据&lt;/a>）；SSD有写入次数的限制，当闪存芯片损坏时，会影响SSD的性能。相比而言，HDD的技术的性能更稳定，技术更为成熟，质量会更好一些。&lt;/p>
&lt;h3 id="数据流的一致性算法">数据流的一致性算法 &lt;a href="#%e6%95%b0%e6%8d%ae%e6%b5%81%e7%9a%84%e4%b8%80%e8%87%b4%e6%80%a7%e7%ae%97%e6%b3%95" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>因为要保证可用性，所以数据流必须要是多机的，而多机系统就要面临一致性问题。WAS提出了一种“链式提交”方法来保证了分布式数据流的一致性。&lt;/p>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-10-7/32688212.jpg" alt="">&lt;/p>
&lt;p>与之前提到的分布式一致性的多机并行的算法不同，WAS在数据流层采用了一种“链式”的提交算法：&lt;/p>
&lt;ol>
&lt;li>当副本1接受到数据后，会首先将数据持久化到磁盘，之后再将相同的请求转发给副本2；&lt;/li>
&lt;li>副本2获得数据后，持久化到磁盘，再请请求转发给副本3；&lt;/li>
&lt;li>副本3将数据成功持久化后，返回ACK；&lt;/li>
&lt;li>副本2接收到副本3的ACK消息后，同样返回ACK；&lt;/li>
&lt;li>副本1接收到副本2的ACK消息后，向请求方返回ACK。&lt;/li>
&lt;/ol>
&lt;p>这样做的好处是，当数据请求返回ACK后，一定有三个（或以上，根据系统的参数而定）副本同时写入并持久化这条请求。此时三个副本一定是“强一致”的。&lt;/p>
&lt;p>不过，如果链式提交当中有任何一个环节出现了问题，比如机器失效、网络超时等，那么我们就要面临数据不一致的情况。WAS为了解决这个问题，引入了被称为“封存”的机制。&lt;/p>
&lt;h3 id="封存sealing机制">封存（sealing）机制 &lt;a href="#%e5%b0%81%e5%ad%98sealing%e6%9c%ba%e5%88%b6" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>上文说到，数据流是一个无限长的队列，只能在尾部追加写入。所以我们如果想要存储这条数据流，就势必要把这条数据流分为多个节，并把它们分别存储在不同的机器上。否则数据流的大小就要受限于单机的存储容量，这并不是我们想要看到的。&lt;/p>
&lt;p>又由于数据流的尾部追加写入特性，所以数据流被分为多个节之后，只有最后一个节是可读写的，其它的所有节都是只读的。&lt;/p>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-10-7/46032879.jpg" alt="">&lt;/p>
&lt;p>我们将把可读可追加的数据流转化为只读的数据流的操作称为“封存”。&lt;/p>
&lt;p>封存的作用之一，就是在数据流的一节过大的时候，停止写入该节，使得数据节的大小可控。&lt;/p>
&lt;p>第二个作用，是上面我们提到的，在“链式提交”失败的时候，我们会将所有的数据流副本封存，封存的时候取所有副本的数据的交集。这样就能排除因为写入失败而不一致的数据。&lt;/p>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-9-12/93804080.jpg" alt="">&lt;/p>
&lt;blockquote>
&lt;p>此时有同学会问，如果我们已经成功的写入了三个副本，但是由于网络原因并没能向请求方返回ACK。请求方会认为这段数据写入失败。但是我们在封存的时候，仍然会将这部分数据封存在数据节中。这种情况怎么处理呢？ &lt;br>
这个问题后面会有解答，请耐心阅读哦~&lt;/p>&lt;/blockquote>
&lt;h3 id="数据流层的架构">数据流层的架构 &lt;a href="#%e6%95%b0%e6%8d%ae%e6%b5%81%e5%b1%82%e7%9a%84%e6%9e%b6%e6%9e%84" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-10-8/29641653.jpg" alt="">&lt;/p>
&lt;p>在前面的文章里，没有提到“数据流管理器”（Stream Manager，简称SM）这个角色。这个角色在整个数据流层扮演了一个很重要的角色。&lt;/p>
&lt;p>SM是由一个小型集群使用paxos选举出来的master节点，其它节点备用。SM只用来管理整个数据流的状态，并不负责具体的读写操作。&lt;/p>
&lt;p>SM的主要功能有：&lt;/p>
&lt;ol>
&lt;li>统计节点状态&lt;/li>
&lt;li>监控节点健康状况&lt;/li>
&lt;li>负载均衡&lt;/li>
&lt;li>封存、创建新的数据节&lt;/li>
&lt;li>备份与垃圾回收&lt;/li>
&lt;/ol>
&lt;p>SM管理着一批存储着数据节的节点（Extent Node，简称EN），这些节点存储着数据流中的节。这些节点相互通信，以完成提交、备份等功能。&lt;/p>
&lt;p>EN节点中采用了预写入日志。每一次写入操作，EN会并发的&lt;/p>
&lt;ol>
&lt;li>追加写入日志文件，并将新数据写入内存缓存&lt;/li>
&lt;li>写入持久化存储&lt;/li>
&lt;/ol>
&lt;p>当二者有一个完成，就返回成功标记。这样做的好处是，由于HDD追加写入的性能非常好并且稳定，所以步骤1的响应时间是稳定的并且可以估计的。保证了系统整体的响应时间不出现大的颠簸。&lt;/p>
&lt;h2 id="was---分区层">WAS - 分区层 &lt;a href="#was---%e5%88%86%e5%8c%ba%e5%b1%82" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>分区层为我们提供了一个被称为“对象表”（Object Table）的抽象。对象表提供了插入、更新、删除功能。&lt;/p>
&lt;p>为了提供容量支持，一个对象表会被划分为多个分区（这就是“分区层”名字的来源）。分区之间相互独立，不会互相影响。&lt;/p>
&lt;p>对象表构建在数据流层之上。但是从直观上来讲，表结构和流结构是完全不同的，那么分区层是如何将表存储在数据流层的呢？&lt;/p>
&lt;h3 id="lsm-treelog-structed-merge-tree">LSM Tree（Log-Structed Merge-Tree） &lt;a href="#lsm-treelog-structed-merge-tree" class="anchor">🔗&lt;/a>&lt;/h3>&lt;blockquote>
&lt;p>段子： &lt;br>
一个学生背着满书包的书进入图书馆，叮叮叮，图书馆的报警器响了，学生赶紧把书从书包倒出来，准备一本一本的验证，看是哪本书有问题。 &lt;br>
一旁扫地的阿姨看不下去了，过来把书分成了两挪，先检查第一挪，叮叮叮，报警器响了，说明这一挪有问题，又把这一挪分成两挪，先检查其中一挪，要是哪一挪响了，就把这一挪继续分成两挪，继续检查，不到三回合，大妈就把有问题的哪本书找出来了。 &lt;br>
大妈用鄙视的眼神看着那学生，仿佛在说连O（n）跟O（log n）都分不清楚。&lt;/p>&lt;/blockquote>
&lt;p>我们上文中提到，HDD磁盘可以在较低成本下提供很好的追加写入性能，但是对于随机读写的性能不佳。&lt;/p>
&lt;p>但是对于表结构来说，必然涉及到大量的随机读写。因为输入数据就像打乱过的扑克牌，很难保证规律性；而我们又要把特定的输入数据放到特定的行上，所以数据的写入位置必然是随机的。所以，直接在磁盘中存储表结构是非常低效的。&lt;/p>
&lt;p>那么LSM Tree是怎么解决问题的呢？&lt;/p>
&lt;p>首先我们把已有的数据根据key进行排序，存储在磁盘中，标记第0代。所有写入磁盘的数据都是有序的、只读的。&lt;/p>
&lt;p>当有新的数据写入时，我们首先将其缓存在内存中。当数据达到一个阈值时，我们将内存中的数据进行排序，并写入磁盘。这一部分数据被标记为第1代。&lt;/p>
&lt;p>之后的数据依次类推，被标记为第2代，第3代&amp;hellip;第n代。代数越高，表示数据越新。并且代数高的数据在逻辑上可以覆盖代数低的数据。&lt;/p>
&lt;p>在查询时，我们首先检索内存中的缓存数据，如果没有找到，就依次从第n代的数据查询到第0代。如果仍然没有找到，就返回未找到。由于所有的查询都使用二分查找，效率较高。同时我们还可以使用[布隆过滤器][6]，快速判定一个key是否在LSM Tree中。&lt;/p>
&lt;blockquote>
&lt;p>细心的同学会发现，按照上面描述的算法，LSM Tree的代数会无限膨胀下去，最终导致非常差的的检索效果。在论文中没有明确提到LSM Tree的调教参数，但是我猜测其代数不会太多。并且在冗余数据过多时，会进行垃圾回收，保证检索的效率。 &lt;br>
同时这也提醒我们在使用WAS的时候，尽量不要向某一个或某一些连续的分区连续写入大量数据，否则会导致分区层的性能退化。&lt;/p>&lt;/blockquote>
&lt;h3 id="分区层使用的数据流">分区层使用的数据流 &lt;a href="#%e5%88%86%e5%8c%ba%e5%b1%82%e4%bd%bf%e7%94%a8%e7%9a%84%e6%95%b0%e6%8d%ae%e6%b5%81" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>分区层使用数据流来保存LSM Tree，使用顺序写入代替随机写入以获得性能提升，这条流被称为“行数据流”（Row Data Stream）。&lt;/p>
&lt;p>与此同时，分区层还使用了“二进制数据流”（Blob Data Stream）来保存二进制数据。以及“提交日志流”（Commit Log Stream）来持久化预写入日志。&lt;/p>
&lt;p>分区层向数据流层写入时，只有当数据流层返回ACK后，才认为写入成功。所以，在数据流层写入成功，但未能返回ACK时，虽然数据流层保留了数据，但是分区层并不能访问这部分数据，我们就在分区层保证了数据的正确性。&lt;/p>
&lt;h3 id="分区层的一致性保证">分区层的一致性保证 &lt;a href="#%e5%88%86%e5%8c%ba%e5%b1%82%e7%9a%84%e4%b8%80%e8%87%b4%e6%80%a7%e4%bf%9d%e8%af%81" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>分区层的持久化信息依赖于数据流层，所以如果分区层使用在线多副本，就会遇到节点状态不一致的问题。也就是说，虽然不同的节点加载了相同的数据，但是加载速度有快有慢，很难保证节点之间以一个同样的状态对外服务。同时，在线多副本的写入会引入竞态，如果再加锁，就会大大的影响系统的效率。&lt;/p>
&lt;p>那么WAS是怎么解决这个问题的呢？&lt;/p>
&lt;p>答：不能保证一致性就不要一致性。&lt;/p>
&lt;p>这样的不要一致性并不是放任节点间的状态不同步，而是每一个分区只使用一个在线服务节点。对于唯一一个节点来说，一致性就是不存在的。（是不是非常暴力）&lt;/p>
&lt;p>所以，在同一时间只有一台机器对外提供服务（读+写）。其它的副本是在线备份，在对外提供服务的机器在计划内下线或意外当掉时，就会启用另一台副本对外提供服务。当然这也会造成一致性问题，所以这里牺牲了一点可用性，也就是在旧节点下线后，新节点必须加载完所有的数据才可以对外提供服务。中间的一段时间这个分区是不可用的。&lt;/p>
&lt;h3 id="分区层的架构">分区层的架构 &lt;a href="#%e5%88%86%e5%8c%ba%e5%b1%82%e7%9a%84%e6%9e%b6%e6%9e%84" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-10-10/51400577.jpg" alt="">&lt;/p>
&lt;p>图中的PS指的就是上文提到的分区服务，每个PS维护着对象表的一个分区，其持久化信息都储存在数据流层上。&lt;/p>
&lt;p>PM（Partition Manager）也是使用paxos算法选举出来的master节点，用来管理分区服务的负载均衡、灾备等操作。&lt;/p>
&lt;p>Lock Service用来对分区节点进行选举，选出的唯一master节点对外提供服务。&lt;/p>
&lt;p>Partion Map Table用来存储与检索分区所对应的节点。&lt;/p>
&lt;h2 id="was---前端层">WAS - 前端层 &lt;a href="#was---%e5%89%8d%e7%ab%af%e5%b1%82" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>前端层只是薄薄的一层REST API封装，并没有什么其它特殊功能。这里一笔带过。&lt;/p>
&lt;h2 id="战胜cap理论你他娘的真是个天才">战胜CAP理论？你他娘的真是个天才！ &lt;a href="#%e6%88%98%e8%83%9ccap%e7%90%86%e8%ae%ba%e4%bd%a0%e4%bb%96%e5%a8%98%e7%9a%84%e7%9c%9f%e6%98%af%e4%b8%aa%e5%a4%a9%e6%89%8d" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-9-20/10049124.jpg" alt="">&lt;/p>
&lt;p>WAS宣称自己战胜了CAP理论，提供了高可用性和强一致性。WAS能做到这点主要是因为数据流层设计的好。&lt;/p>
&lt;p>数据流层使用了只追加写的数据模型，加上链式提交提供的高一致性，所以能应对节点失效（可用性）和网络失效（分区容错性）的故障。&lt;/p>
&lt;p>数据流层之上的分区层利用了其特性，很容易的实现了高可用性和分区容错性。同时，又由于其使用单点向外提供服务，所以一致性也有了保障。&lt;/p>
&lt;p>所以WAS宣称自己战胜了CAP理论，也不是没有道理。但是个人感觉WAS在分区层的单点设定，是以牺牲可用性（A）换取一致性（C）的一种妥协，只是由于机器故障并非多发，触发问题的机率比较小。&lt;/p>
&lt;h2 id="写在后面">写在后面 &lt;a href="#%e5%86%99%e5%9c%a8%e5%90%8e%e9%9d%a2" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>本文是《Windows Azure Storage: A Highly Available Cloud Storage Service with Strong Consistency》一文的读后感，删节了一部分不容易理解的细节。想要更详尽的理解WAS的设计思想，请阅读论文原文以及官方文档。一切内容和理解上的冲突，以原文和官方文档为准。&lt;/p>
&lt;p>撰写本文时，我使用Atreus2tap迷之小键盘，在学习前人经验教训的同时，重新学习了打字。&lt;/p>
&lt;p>小键盘的好处是便携，以及避免手指移动带来的劳损。缺点是需要精心设计并练习大量的组合键，同时成套的键帽真是不好买。&lt;/p>
&lt;p>上一张图。&lt;/p>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-10-10/71011744.jpg" alt="">&lt;/p>
&lt;blockquote>
&lt;p>听说用这种奇怪键盘的人都注孤了。&lt;/p>&lt;/blockquote>
&lt;h2 id="参考链接">参考链接 &lt;a href="#%e5%8f%82%e8%80%83%e9%93%be%e6%8e%a5" class="anchor">🔗&lt;/a>&lt;/h2>&lt;ul>
&lt;li>&lt;a href="http://wetest.qq.com/lab/view/105.html">分布式系统设计的求生之路&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://waylau.com/talk-about-distributed-system/">用大白话聊聊分布式系统&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://book.douban.com/subject/26292004/">《从Paxos到Zookeeper》&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf">Dynamo: Amazon’s Highly Available Key-value Store&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>