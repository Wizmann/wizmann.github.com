<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ceph on Maerlyn's Rainbow</title><link>https://wizmann.top/tags/ceph/</link><description>Recent content in Ceph on Maerlyn's Rainbow</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Sun, 29 Nov 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://wizmann.top/tags/ceph/index.xml" rel="self" type="application/rss+xml"/><item><title>Introduction to Ceph</title><link>https://wizmann.top/posts/introduction-to-ceph/</link><pubDate>Sun, 29 Nov 2020 00:00:00 +0000</pubDate><guid>https://wizmann.top/posts/introduction-to-ceph/</guid><description>&lt;h2 id="什么是ceph">什么是Ceph &lt;a href="#%e4%bb%80%e4%b9%88%e6%98%afceph" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>Ceph是一个可扩展的，高性能的分布式存储系统。提供了三种不同类型的接口以适应不同的应用场景：&lt;/p>
&lt;ul>
&lt;li>block-based: 块存储，可以用做VM的虚拟磁盘&lt;/li>
&lt;li>object-based: 对象存储，与Amazon S3等常用对象存储兼容&lt;/li>
&lt;li>file system: POSIX兼容的分布式文件系统，可以被本地系统挂载，并且能被多个客户端共享&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/20-11-28/2020-11-28_22-04-49.png" alt="">&lt;/p>
&lt;h3 id="ceph的特性">Ceph的特性 &lt;a href="#ceph%e7%9a%84%e7%89%b9%e6%80%a7" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>由于采用了CRUSH算法，Ceph有着优异的可扩展性（宣称可以无限扩展）。并且借助可扩展性，进而实现高性能、高可靠性和高可用性。&lt;/p>
&lt;p>Ceph是一个去中心化的存储系统，无需中心节点进行资源的管理与调度，全部的管理功能由存储节点自治完成。使得整个系统可以自我管理与自我恢复，减少运维成本与管理成本。&lt;/p>
&lt;h2 id="rados---ceph的存储引擎">RADOS - Ceph的存储引擎 &lt;a href="#rados---ceph%e7%9a%84%e5%ad%98%e5%82%a8%e5%bc%95%e6%93%8e" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>RADOS=Reliable Autonomic Distributed Object Store。RADOS是Ceph底层的存储引擎，所有的接口都建立在RADOS的功能之上。&lt;/p>
&lt;h3 id="rados中的存储结构">RADOS中的存储结构 &lt;a href="#rados%e4%b8%ad%e7%9a%84%e5%ad%98%e5%82%a8%e7%bb%93%e6%9e%84" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>&lt;img src="https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/20-11-28/2020-11-28_22-24-56.png" alt="">&lt;/p>
&lt;ul>
&lt;li>存储池（pool）：逻辑层，每一个pool里都包含一些放置组&lt;/li>
&lt;li>放置组（placement-group, PG)：逻辑层，一份数据会在PG当中进行灾备复制。每一个PG都对应着一系列的存储节点&lt;/li>
&lt;li>存储节点（OSD）：用以存储数据的物理节点。与PG之间形成多对多的关系。&lt;/li>
&lt;/ul>
&lt;p>一份数据在写入RADOS时，会先选中一个pool。Pool中再使用一定的hash规则，伪随机的选中某一个PG。PG会将数据写入多个OSD中。读取数据时，也是类似的规则。&lt;/p>
&lt;p>Pool是用户可见的管理数据的基本单位，用户可以对Pool进行一系列的配置（权限控制、使用SSD or HDD、使用数据拷贝 or 纠删码，etc.）。而PG与OSD对于用户是不可见的。&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/20-11-28/2020-11-28_23-00-51.png" alt="">&lt;/p>
&lt;h4 id="pg的组织">PG的组织 &lt;a href="#pg%e7%9a%84%e7%bb%84%e7%bb%87" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>在一致性哈希中，我们使用节点来划分哈希值域。这种方法的问题是，如果产生了数据不平衡，我们需要重新进行划分值域来进行再平衡。这会造成大量的数据迁移。&lt;/p>
&lt;p>CRUSH采用了虚拟节点（也就是PG）将哈希值域划分成了固定的等长区域。这种方法在单条数据与物理节点之间加入了一个虚拟层。之后，再使用哈希取模的算法确定数据属于哪个PG。使得数据的迁移是以虚拟节点为单位，而不是对每一条数据都重新计算。Ceph官方的建议是，每1个OSD对应着100个PG。&lt;/p>
&lt;p>一般情况下，在规划的初期需要确定PG的数目，如果后期需要调整PG，有可能会导致大量的数据迁移，甚至需要服务暂时停止服务。&lt;/p>
&lt;h3 id="监控子集群mon与cluster-map">监控子集群（MON）与Cluster Map &lt;a href="#%e7%9b%91%e6%8e%a7%e5%ad%90%e9%9b%86%e7%be%a4mon%e4%b8%8ecluster-map" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>&lt;img src="https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/20-11-28/2020-11-28_22-48-10.png" alt="">&lt;/p>
&lt;p>RADOS集群中除了OSD存储节点之外，还有监控子集群（MON），用于存储系统的拓扑结构——Cluster Map。&lt;/p>
&lt;blockquote>
&lt;p>元数组管理（MDS）节点用于管理CephFS中的文件元信息，后文会有介绍。&lt;/p>&lt;/blockquote>
&lt;p>不同于传统的中心化管理节点，MON并不会对资源进行调配与调度，而仅仅是一个观测者，用以存储系统当前的拓扑与状态。&lt;/p>
&lt;p>MON与OSD、OSD与OSD之间会定时发送心跳包，检测OSD是否健康。如果某个节点失效，MON会更新内部存储的拓扑结构信息（ClusterMap），并且通过P2P协议广播出去，从而使得整个系统都有着（最终）一致的拓扑信息。&lt;/p>
&lt;h3 id="主从同步与节点自治">主从同步与节点自治 &lt;a href="#%e4%b8%bb%e4%bb%8e%e5%90%8c%e6%ad%a5%e4%b8%8e%e8%8a%82%e7%82%b9%e8%87%aa%e6%b2%bb" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>在一个PG中，会有一个主节点（Primary）和一个或多个从节点（Secondary）。主节点负责维护从节点的状态，包括数据复制（replication）、失效检测（failure detection）和失效恢复（failure recovery）。&lt;/p>
&lt;h2 id="crush---ceph皇冠上的明珠">CRUSH - Ceph皇冠上的明珠 &lt;a href="#crush---ceph%e7%9a%87%e5%86%a0%e4%b8%8a%e7%9a%84%e6%98%8e%e7%8f%a0" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>CRUSH是一个可扩展的，伪随机的数据放置算法。以去中心化方法，将PG按规则映射到相应的存储设备上。并且系统的拓扑结构发生变化时，尽可能的减少数据的迁移。&lt;/p>
&lt;h3 id="crush的优势">CRUSH的优势 &lt;a href="#crush%e7%9a%84%e4%bc%98%e5%8a%bf" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>CRUSH对于每一个数据元素使用一个伪随机算法，决定其放置的位置。所以，只要所有参与者都拥有相同的系统拓扑结构信息，那么数据的位置就是一定的。所以我们可以去掉中心节点，采用P2P的方法来进行数据的存储与检索。&lt;/p>
&lt;p>并且，由于伪随机算法只与单个PG相关，如果我们操作得当，节点数量的变化并不会引起大量的数据迁移，而是会接近理论最优值。&lt;/p>
&lt;h3 id="数据放置data-placement">数据放置（Data Placement） &lt;a href="#%e6%95%b0%e6%8d%ae%e6%94%be%e7%bd%aedata-placement" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>CRUSH的数据放置算法有很多不同的实现，这里只介绍最常用的straw算法。&lt;/p>
&lt;p>Ceph中，每一个OSD节点都有一个权值w，代表着某个节点能支持多少数据的存储与检索。一般来说，权值与节点的容量成正比。&lt;/p>
&lt;p>假设一个pool里面有n个PG，在一条新的数据写入时，我们分别会计算这n个PG的length值。&lt;/p>
&lt;p>$$ length_{i} = f(w_{i}) * hash(x) $$&lt;/p>
&lt;p>$f(w_{i})$是一个只于当前OSD节点权值有关的函数。$hash(x)$代表当前PG的哈希值。所以，PG会放置在哪个OSD上，仅与其权值相关。&lt;/p>
&lt;p>假设某个OSD节点发生变化时（新加、删除、权值变化），在此受影响节点的数据会迁移到其它的OSD节点。其它OSD节点的原有数据并不会受到影响。&lt;/p>
&lt;blockquote>
&lt;p>Ceph当中的straw算法有&lt;code>straw1&lt;/code>和&lt;code>straw2&lt;/code>。&lt;code>straw1&lt;/code>的实现采用了有缺陷f(w)函数，会导致意外的数据迁移。&lt;code>straw2&lt;/code>解决了这个问题。
详情请戳&lt;a href="https://www.spinics.net/lists/ceph-devel/msg21635.html">这里&lt;/a>&lt;/p>&lt;/blockquote>
&lt;h4 id="主从架构">主从架构 &lt;a href="#%e4%b8%bb%e4%bb%8e%e6%9e%b6%e6%9e%84" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>每个PG所包含的OSD都由CRUSH算法计算得出，并根据配置选出前r个OSD进行主从配对。列表中的第1个OSD做为主节点（Primary），其它的节点为从节点（Secondary）。&lt;/p>
&lt;p>主从节点的分配与管理由PG内部进行自治，不需要额外的外部系统进行管理。&lt;/p>
&lt;h2 id="cephfs">CephFS &lt;a href="#cephfs" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>CephFS是一个POSIX兼容的（共享）文件系统。CephFS利用文件元数据子系统（MDS）来维护目录树结构和文件和目录的元信息（owner, timestamps, inodes, etc.)等。&lt;/p>
&lt;p>MDS子系统会在内存里面维护一份Cache，对于需要持久化的信息，会使用WAL的方式写入RADOS里一个专用的Pool当中。&lt;/p>
&lt;h3 id="动态树划分dynamic-tree-partitioningdtp">动态树划分（Dynamic Tree Partitioning，DTP） &lt;a href="#%e5%8a%a8%e6%80%81%e6%a0%91%e5%88%92%e5%88%86dynamic-tree-partitioningdtp" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>CephFS的扩展性的关键之一，在于元信息子系统的扩展性。CephFS实现了动态树划分的算法，将目录树结构根据当前系统的负载，将其划分到不同的MDS节点上去。&lt;/p>
&lt;p>维护目录树结构的优势在于利用了文件系统的局部性（locality），可以方便的进行预取（prefetch）。动态的树划分，可以保证元信息可以线性增长，以保持高可扩展性。&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/20-11-29/2020-11-29_18-53-09.png" alt="">&lt;/p>
&lt;h2 id="写在最后">写在最后 &lt;a href="#%e5%86%99%e5%9c%a8%e6%9c%80%e5%90%8e" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>本文基于Ceph的三篇论文综合而成（CephFS、RADOS、CRUSH）。其中加入了一些自己的看法，使其逻辑通顺，并不保证与论文的思路完全一致。&lt;/p>
&lt;p>这三篇论文并没有明确的依赖关系，换句话说，需要综合阅读，才能有比较明确的理解。&lt;/p>
&lt;p>建议在通读论文后，去学习一下&lt;a href="https://www.youtube.com/watch?v=PmLPbrf-x9g&amp;amp;ab_channel=Ceph">这个视频&lt;/a>，会对理解Ceph有很大的帮助。Youtube上面还有很多Ceph的tech talk，可以一并的了解一下。&lt;/p>
&lt;p>Ceph相关的书籍以实践居多，只推荐&lt;a href="https://www.oreilly.com/library/view/learning-ceph-/9781787127913/">Learning Ceph&lt;/a>。&lt;/p>
&lt;h2 id="参考链接">参考链接 &lt;a href="#%e5%8f%82%e8%80%83%e9%93%be%e6%8e%a5" class="anchor">🔗&lt;/a>&lt;/h2>&lt;ul>
&lt;li>&lt;a href="https://crossoverjie.top/2018/01/08/Consistent-Hash/">一致性 Hash 算法分析&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://zhuanlan.zhihu.com/p/60963885">从一致性 hash 到 ceph crush&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://ceph.io/publications/">Ceph Publications&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>