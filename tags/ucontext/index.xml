<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ucontext on Maerlyn's Rainbow</title><link>https://wizmann.top/tags/ucontext/</link><description>Recent content in Ucontext on Maerlyn's Rainbow</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Mon, 17 Oct 2016 01:28:40 +0000</lastBuildDate><atom:link href="https://wizmann.top/tags/ucontext/index.xml" rel="self" type="application/rss+xml"/><item><title>使用epoll驱动ucontext - phxrpc代码阅读(5)</title><link>https://wizmann.top/posts/phxrpc-5/</link><pubDate>Mon, 17 Oct 2016 01:28:40 +0000</pubDate><guid>https://wizmann.top/posts/phxrpc-5/</guid><description>&lt;h2 id="用pipe叫醒你--epollnotifier">用pipe叫醒你 — EpollNotifier &lt;a href="#%e7%94%a8pipe%e5%8f%ab%e9%86%92%e4%bd%a0--epollnotifier" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>&lt;code>class EpollNotifier&lt;/code>类型封装了一个使用pipe传递信号的Notifier类。&lt;/p>
&lt;p>&lt;code>Run()&lt;/code>函数（其实我觉得叫Register或Activate会更好）首先声明了两个单向的pipe：&lt;code>pipe_fds_&lt;/code>，从&lt;a href="http://man7.org/linux/man-pages/man2fpipe.2.html">文档&lt;/a>中我们可以知道&lt;code>pipe_fds_[0]&lt;/code>是读管道，而&lt;code>pipe_fds_[1]&lt;/code>是写管道。这里有一丁点反直觉，就是pipe拿了两个fd，但是仍旧是单工的。&lt;/p>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/16-10-16/2335602.jpg" alt="">&lt;/p>
&lt;p>然后将读fd设为&lt;code>O_NONBLOCK&lt;/code>以供epoll调度，最后将&lt;code>Func()&lt;/code>函数传入&lt;code>scheduler_&lt;/code>中。&lt;/p>
&lt;blockquote>
&lt;p>这里跑个题，想起了当年我大一的时候上过的通信导论的选修课。那会我还没有沉迷代码，还是一个积极乐观好好学习的新时代大学生。自从开始写了代码，人就越来越废物了，连女朋友都找不到了。 &lt;br>
年轻人们啊，有饭辙干点啥都行，千万别写码啊。&lt;/p>&lt;/blockquote>
&lt;p>&lt;code>Func()&lt;/code>函数做的事情很简单，就是从管道里尝试poll一段数据，拿到数据后直接扔掉。因为管道里传来的数据并没有实际意义，这样设计的主要意义在于唤醒epoll。&lt;/p>
&lt;p>我们可以从&lt;code>Notify()&lt;/code>函数中看出，传入管道的是一个字符&amp;quot;a&amp;quot;。&lt;/p>
&lt;h2 id="调度器类--uthreadepollscheduler">调度器类 — UThreadEpollScheduler &lt;a href="#%e8%b0%83%e5%ba%a6%e5%99%a8%e7%b1%bb--uthreadepollscheduler" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>调试器类在初始化时，声明了协程栈的大小以及调试器所调度的最大任务数。不过这个最大任务数是一个“软线”，因为在最新的Linux内核中，epoll使用动态内存管理fd，&lt;code>epoll_create&lt;/code>中的&lt;code>size&lt;/code>参数已经失去了作用。而后面&lt;code>epoll_wait&lt;/code>中的&lt;code>max_event&lt;/code>参数只是每次返回的最多event数，也就是如果我们向调度器中加入了超过限制的fd，也不会有什么恶劣的后果。（参考&lt;a href="http://man7.org/linux/man-pages/man2/epoll_create.2.html">epoll文档&lt;/a>和&lt;a href="http://man7.org/linux/man-pages/man2/epoll_wait.2.html">epoll_wait文档&lt;/a>）&lt;/p>
&lt;h3 id="让人搞不懂的instance函数">让人搞不懂的Instance函数 &lt;a href="#%e8%ae%a9%e4%ba%ba%e6%90%9e%e4%b8%8d%e6%87%82%e7%9a%84instance%e5%87%bd%e6%95%b0" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>这个函数看起来像一个Singleton的实现，但是明明&lt;code>UThreadEpollScheduler&lt;/code>类的构造函数是public的。也就是这个函数像是一个单例，但它又不是一个单例。&lt;/p>
&lt;p>在其它的代码中，也没有调用个函数的地方，我觉得这个函数是开发者忘记删了。&lt;/p>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/16-10-16/85921046.jpg" alt="">&lt;/p>
&lt;h3 id="远古智慧--createsocket">远古智慧 — CreateSocket &lt;a href="#%e8%bf%9c%e5%8f%a4%e6%99%ba%e6%85%a7--createsocket" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>这个函数其实没啥可说的，算是对&lt;code>UThreadSocket_t&lt;/code>构造的封装，但是这里面有一个小技巧，就是&lt;code>calloc&lt;/code>的使用。&lt;/p>
&lt;p>&lt;code>calloc&lt;/code>的作用是向内核申请一段栈空间（和&lt;code>malloc&lt;/code>行为一致），然后将这一段内存清0。&lt;/p>
&lt;p>个人感觉这样做的目的是防止指针没有初始化带来的一系列诡异的问题。把指针清零，可以让问题在第一时间出现，方便出错时的调试。但是我觉得还是用测试覆盖这种问题已经好，因为空指针在特定的情况下，仍可能是导致诡异行为的源头。&lt;/p>
&lt;h3 id="跑--run">跑 — Run &lt;a href="#%e8%b7%91--run" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>&lt;code>Run()&lt;/code>函数是调度器的核心函数（当然啦），简单来说就是一个循环获取event，用适当的协程处理event。&lt;/p>
&lt;p>函数的一开始，先调用&lt;code>ConsumeTodoList()&lt;/code>函数，将列表中的协程全部激活，并hang在epoll上。&lt;/p>
&lt;p>之后进入一个“死循环”，通过&lt;code>epoll_wait&lt;/code>将有数据可读的fd取出，并调用相应的协程进行处理。这里我们看到，&lt;code>epoll_wait&lt;/code>的超时时间是写死的4ms，并没有使用&lt;code>next_timeout&lt;/code>给出的下次超时时间。这是因为这里支持了“active socket”，即服务器对活动连接操作，例如发送响应甚至新建一个socket。&lt;/p>
&lt;p>后面的&lt;code>handler_accepted_fd_func_&lt;/code>和&lt;code>active socket&lt;/code>是类似的，不过这个函数是用来处理已经建立好的连接，为其分配相应的协程。&lt;/p>
&lt;p>所以，由此可见，这个循环即是事件驱动的，又是轮询的。然而这两种模型，居然能写在一个函数里，真是令人印象深刻。&lt;/p>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/16-10-16/4157866.jpg" alt="">&lt;/p>
&lt;p>下面的&lt;code>DealwithTimeout&lt;/code>函数处理了一下超时的协程，并且更新了&lt;code>next_timeout&lt;/code>变量。然而这个变量因为众所周知的原因，并没有什么卵用。&lt;/p>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/16-10-16/4157866.jpg" alt="">&lt;/p>
&lt;h2 id="poll来poll去--一堆epoll函数的封装">Poll来Poll去 — 一堆epoll函数的封装 &lt;a href="#poll%e6%9d%a5poll%e5%8e%bb--%e4%b8%80%e5%a0%86epoll%e5%87%bd%e6%95%b0%e7%9a%84%e5%b0%81%e8%a3%85" class="anchor">🔗&lt;/a>&lt;/h2>&lt;h3 id="uthreadpoll1">UThreadPoll(1) &lt;a href="#uthreadpoll1" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>&lt;code>UThreadPoll&lt;/code>函数有两个版本，一个是poll单个socket，另外一个是poll一堆socket。我们先从单个socket的看起。&lt;/p>
&lt;p>第一步是注册一个超时时间，第二步将这个socket放到epoll的监听列表上。之后调用yield，把控制权交还给主控制流。&lt;/p>
&lt;p>当epoll收到相应的事件时，主控制流会将控制权交还给协程，协程将socket从epoll监听列表中移除，之后进行后面的操作。&lt;/p>
&lt;p>整体的工作流可以参考下图。&lt;/p>
&lt;p>&lt;img src="http://7lrx26.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-10-16%2021.33.48.png" alt="image">&lt;/p>
&lt;h3 id="uthreadpoll2---边缘触发和水平触发">UThreadPoll(2) - 边缘触发和水平触发 &lt;a href="#uthreadpoll2---%e8%be%b9%e7%bc%98%e8%a7%a6%e5%8f%91%e5%92%8c%e6%b0%b4%e5%b9%b3%e8%a7%a6%e5%8f%91" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>epoll有两种触发模式，边缘触发(edge-trigger, ET)和水平触发(level-trigger, LT)。&lt;/p>
&lt;p>简单解释一下epoll的这两种触发模式。ET意味着只要有fd可读或可写，&lt;code>epoll_wait&lt;/code>就返回这个fd，而LT意味着当且仅当fd由“不可读变为可读”或由“不可写变为可写”时，&lt;code>epoll_wait&lt;/code>才会返回。（这有可能出现所谓的“粘包”现象，详见&lt;a href="http://man7.org/linux/man-pages/man7/epoll.7.html">这里&lt;/a>）&lt;/p>
&lt;blockquote>
&lt;p>第一次听到“粘包”这个词，我一直以为这是啥好吃的。。。&lt;/p>&lt;/blockquote>
&lt;p>这意味着，当我们使用LT时，我们必须清理干净fd中的数据，即只要可读，就一直读；只要可写，就一直写。否则就会出现问题。&lt;/p>
&lt;p>在这里，我们使用的是比较常用的ET模式。并且我们利用了ET的特性实现了“监听多个fd，返回最早响应的那一个”。&lt;/p>
&lt;p>首先，我们新建了一个epoll fd（简称内部epoll），将列表中的所有socket放到里面监听。之后将这个socket fd放到&lt;code>list[0]-&amp;gt;epollfd&lt;/code>所对应的epoll（简称外部epoll，通常是主工作循环的那个epoll）监听列表中。&lt;/p>
&lt;p>当列表中的socket有返回时，内部的epoll会返回一个&lt;code>EPOLLIN&lt;/code>事件，外部的epoll接收到这个事件后，进行协程切换，回到当前函数中。&lt;/p>
&lt;p>下一步我们&lt;code>epoll_wait&lt;/code>内部的epoll fd，因为我们确定此时一定有可操作的fd，所以我们将&lt;code>epoll_wait&lt;/code>的timeout参数设为0。之后我们将返回的fd的&lt;code>waited_events&lt;/code>参数填好，最后返回操作成功的fd的数目。&lt;/p>
&lt;p>这个函数比较绕，不过有一个好消息 —— 这个函数也没有被其它地方调用过。不过这种&lt;code>cascaded epoll&lt;/code>的技巧确实是让人耳目一新。&lt;/p>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/16-10-16/4157866.jpg" alt="">&lt;/p>
&lt;h3 id="延时执行---uthreadwait">延时执行 - UThreadWait &lt;a href="#%e5%bb%b6%e6%97%b6%e6%89%a7%e8%a1%8c---uthreadwait" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>剩下的几个函数基本都是无脑封装，顺着看一遍代码基本就知道是啥意思了。不过&lt;code>UThreadWait&lt;/code>这个函数比较有意思，可以用来复习一下&lt;code>uthread + epoll&lt;/code>的工作流程。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-cpp" data-lang="cpp">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">void&lt;/span> &lt;span style="color:#a6e22e">UThreadWait&lt;/span>(UThreadSocket_t &lt;span style="color:#f92672">&amp;amp;&lt;/span> socket, &lt;span style="color:#66d9ef">int&lt;/span> timeout_ms) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> socket.uthread_id &lt;span style="color:#f92672">=&lt;/span> socket.scheduler&lt;span style="color:#f92672">-&amp;gt;&lt;/span>GetCurrUThread();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> socket.scheduler&lt;span style="color:#f92672">-&amp;gt;&lt;/span>AddTimer(&lt;span style="color:#f92672">&amp;amp;&lt;/span>socket, timeout_ms);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> socket.scheduler&lt;span style="color:#f92672">-&amp;gt;&lt;/span>YieldTask();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> socket.scheduler&lt;span style="color:#f92672">-&amp;gt;&lt;/span>RemoveTimer(socket.timer_id);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>首先，获取当前uthread的ID，当我们调用&lt;code>Resume()&lt;/code>函数时，让代码知道我们要返回到哪个协程上面。&lt;/p>
&lt;p>然后我们向调度器中添加一个定时器。之后&lt;code>Yield()&lt;/code>，离开当前协程。&lt;/p>
&lt;p>主工作循环运行到超时时间后，会在这个协程打上一个“超时”标签，然后&lt;code>Resume()&lt;/code>切换回这个协程上来。&lt;/p>
&lt;p>剩下的工作，就交由协程内部绑定的函数来进行处理。&lt;/p>
&lt;h2 id="写在最后">写在最后 &lt;a href="#%e5%86%99%e5%9c%a8%e6%9c%80%e5%90%8e" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>在网络编程方面，真心是一个初学者。很多用词可能不恰当，也有一些是自己生造的。大家阅读的时候，尽量以代码和更专业的术语为准。&lt;/p>
&lt;p>然后因为在行文中，可能追求了过多的逗比感，对原代码调侃了几句。并没有什么恶意，如果哪里说的不对，欢迎拍砖。&lt;/p>
&lt;p>&lt;img src="http://7lrx26.com1.z0.glb.clouddn.com/IMG_20161017_012107.jpg" alt="image">&lt;/p>
&lt;p>佛祖保佑，永无Bug。&lt;/p></description></item><item><title>ucontext - phxrpc代码阅读(4)</title><link>https://wizmann.top/posts/phxrpc-4/</link><pubDate>Thu, 13 Oct 2016 23:45:24 +0000</pubDate><guid>https://wizmann.top/posts/phxrpc-4/</guid><description>&lt;h2 id="写在前面">写在前面 &lt;a href="#%e5%86%99%e5%9c%a8%e5%89%8d%e9%9d%a2" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>国庆假期过半，phxrpc的代码阅读大概要小小告一段落啦。因为这两天还要读工作相关的代码，以及最后几天还有一次短途旅行。&lt;/p>
&lt;p>所以非阻塞TCP流可能要留到下一篇了，这一篇只涉及非阻塞TCP流使用到的ucontext协程库，及其使用的一些框架代码。&lt;/p>
&lt;blockquote>
&lt;p>161013更新：这点破东西写到今天才写完，GG。&lt;/p>&lt;/blockquote>
&lt;h2 id="什么是ucontext">什么是ucontext &lt;a href="#%e4%bb%80%e4%b9%88%e6%98%afucontext" class="anchor">🔗&lt;/a>&lt;/h2>&lt;blockquote>
&lt;p>&amp;ldquo;Subroutines are special cases of &amp;hellip; coroutines.&amp;rdquo; –Donald Knuth.&lt;/p>&lt;/blockquote>
&lt;p>首先我们来看一下，什么是线程。线程是进程内一条执行流的状态，包含了硬件状态（硬件计数器，寄存器，条件码等）和堆栈中的数据。&lt;/p>
&lt;p>线程通常只有一个入口和一个出口。当线程返回时，线程的生命周期也结束了。所以，通常线程的执行由内核调度。&lt;/p>
&lt;p>协程的定义与线程类似，也是硬件状态+堆栈的状态组合。但是与线程不同的是，协程可以有多个出口。可以通过yield来暂停自己，调用其它协程。再次启动时，会从上次挂起的地方继续运行。&lt;/p>
&lt;h2 id="phxrpc中的ucontext">phxrpc中的ucontext &lt;a href="#phxrpc%e4%b8%ad%e7%9a%84ucontext" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>phxrpc提供了system和boost两种ucontext的实现，所以提供了一个&lt;code>uthread_context_base&lt;/code>的基类。其实在这里我是有一点怀疑虚函数的性能的，不过好在协程的切换以及网络IO操作还是比较耗性能的，所以虚函数多出来的几次内存寻址也并非不能接受。&lt;/p>
&lt;p>在这篇文章中，我们只看&lt;code>uthread_context_system&lt;/code>这个使用系统ucontext库的实现。&lt;/p>
&lt;h3 id="协程上下文uthreadcontext">协程上下文：&lt;code>UThreadContext&lt;/code> &lt;a href="#%e5%8d%8f%e7%a8%8b%e4%b8%8a%e4%b8%8b%e6%96%87uthreadcontext" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>这个类是协程上下文的虚基类，所以代码很少。并且也没有什么好解释的。&lt;/p>
&lt;p>&lt;code>void Make(UThreadFunc_t func, void * args)&lt;/code>函数是&lt;code>makecontext()&lt;/code>的封装。&lt;/p>
&lt;p>&lt;code>bool Resume()&lt;/code>和&lt;code>bool Yield()&lt;/code>是&lt;code>swapcontext&lt;/code>的封装。&lt;/p>
&lt;p>个人感觉这个类拆分成一个工厂类（传入一个Create仿函数）和一个上下文基类会更清楚一点。&lt;/p>
&lt;h3 id="使用系统ucontext库的协程上下文uthreadcontextsystem">使用系统ucontext库的协程上下文：&lt;code>UThreadContextSystem&lt;/code> &lt;a href="#%e4%bd%bf%e7%94%a8%e7%b3%bb%e7%bb%9fucontext%e5%ba%93%e7%9a%84%e5%8d%8f%e7%a8%8b%e4%b8%8a%e4%b8%8b%e6%96%87uthreadcontextsystem" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>在phxrpc的文档中，说明使用系统原生的ucontext库的性能要差于boost版本的。但是从数据上来看微乎其微，所以我们先从这个版本看起，力求举一反三。&lt;/p>
&lt;p>&lt;code>UThreadContextSystem&lt;/code>在构造函数中传入了协程栈大小，协程要执行的函数（及参数），协程执行后的回调，以及调试用的&lt;code>need_stack_protect&lt;/code> flag。&lt;/p>
&lt;p>每一个上下文对象都维护了两个context，&lt;code>main_context&lt;/code>用来表示主程序执行流的上下文，而&lt;code>context_&lt;/code>则用来表示协程的上下文。&lt;/p>
&lt;p>&lt;code>main_context&lt;/code>是&lt;code>static thread_local&lt;/code>修饰的，也就意味着这个静态变量在每一个线程中有且只有一个。执行在同一个线程上的不同协程，都会切换/被切换到这个上下文上。&lt;/p>
&lt;p>在&lt;code>Resume()&lt;/code>函数中，我们激活协程上下文，并将主程序执行流的上下文保存在&lt;code>main_context&lt;/code>上。&lt;/p>
&lt;p>在&lt;code>Yield()&lt;/code>函数中，我们将主程序执行流的上下文激活，将协程上下文保存回&lt;code>context_&lt;/code>中。&lt;/p>
&lt;p>这里的&lt;code>UThreadFuncWrapper()&lt;/code>值得我们特别关注。这个函数包装了我们的工作函数&lt;code>uc-&amp;gt;func_&lt;/code>，并且将&lt;code>this&lt;/code>指针传进去。&lt;/p>
&lt;p>传入指针时，这里使用了一个技巧。首先我们将指针强转为&lt;code>uintptr_t&lt;/code>，这个是编译器内置的一个&lt;code>typeof&lt;/code>，意在将指针类型无损失的转为整型。之后，将一个&lt;code>uintptr_t&lt;/code>拆为两个&lt;code>uint32_t&lt;/code>。最后，在wrapper函数中，将这两个&lt;code>uint32_t&lt;/code>拼回成一个指针类型。&lt;/p>
&lt;p>初看这段代码，我们就有这样的疑问：“这特么不是有病么？” 但是，折腾自然有折腾的道理。&lt;/p>
&lt;blockquote>
&lt;p>When this context is later activated (using setcontext(3) or swapcontext()) the function func is called, and passed the series of integer (int) arguments that follow argc; the caller must specify the number of these arguments in argc.&lt;/p>&lt;/blockquote>
&lt;p>从官方的文档中我们可以看到，用在&lt;code>setcontext&lt;/code>中的函数，只支持int类型的参数，并且需要我们显式声明参数的数目。这里一定要小心，因为变长参数列表并不能有很强的编译期检查支持，搞出UB或core dump来就非常难查。&lt;/p>
&lt;h3 id="ucontext中使用的栈内存uthreadstackmemory">ucontext中使用的栈内存：&lt;code>UThreadStackMemory&lt;/code> &lt;a href="#ucontext%e4%b8%ad%e4%bd%bf%e7%94%a8%e7%9a%84%e6%a0%88%e5%86%85%e5%ad%98uthreadstackmemory" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>ucontext协程是在同一个线程执行多个上下文，所以就要配备多个栈空间。这里的栈大小我们是可以手动管理的，所以我们可以根据程序的实际情况来调整栈大小，以节省内存使用。&lt;/p>
&lt;p>内存的申请并不是使用&lt;code>malloc&lt;/code>或者&lt;code>new&lt;/code>这种比较高层次的内存操作函数，而是使用的&lt;code>mmap&lt;/code>。这样的好处是我们可以使用参数控制申请出的内存的权限。&lt;/p>
&lt;p>栈内存有两种模式，保护和非保护。保护模式用于调试，会在正常栈内存的两端，各申请一个页大小的保护内存。正常栈内存的权限是读写：&lt;code>PROT_READ | PROT_WRITE&lt;/code>，而保护内存的权限是禁止访问：&lt;code>PROT_NONE&lt;/code>，也就是说，任何试图访问这块内存的请求，都会触发段错误。&lt;/p>
&lt;p>在非保护的运行模式下，栈内存还会使用&lt;code>MAP_ANONYMOUS | MAP_PRIVATE&lt;/code>还进行保护。&lt;code>MAP_ANONYMOUS&lt;/code>表明这段内存是匿名的，即不占用fd，也无需进行写回操作，使mmap的行为类似于malloc。&lt;code>MAP_PRIVATE&lt;/code>意为这段内存不会被其它进程访问，可以使用私有的写时复制映射。（虽然没找到相关资料，但是感觉这两个配置牺牲了可调试性来获取更好的性能）&lt;/p>
&lt;h3 id="ucontext的运行时uthreadruntime">ucontext的运行时：&lt;code>UThreadRuntime&lt;/code> &lt;a href="#ucontext%e7%9a%84%e8%bf%90%e8%a1%8c%e6%97%b6uthreadruntime" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>这个类其实很简单，但是由于代码的命名过于意识流，所以很容易把人绕晕。&lt;/p>
&lt;p>一个&lt;code>UThreadRuntime&lt;/code>代表着一个线程中运行着的N个ucontext上下文。上下文信息保存在&lt;code>std::vector&amp;lt;ContextSlot&amp;gt; context_list_&lt;/code>中。&lt;/p>
&lt;p>slot是可以复用的，&lt;code>first_done_item_&lt;/code>记录着已执行完的context的下标，然后slot中的&lt;code>next_done_item&lt;/code>记录着下一个执行完的context的下标。简而言之，这就是类似一个“脏池”的设计。不过这个命名啊，一点都不赛艇。&lt;/p>
&lt;p>剩下的代码基本就是&lt;code>UThreadContext&lt;/code>的无脑封装了。需要哪个协程开始工作就&lt;code>Resume&lt;/code>哪个协程，需要暂停就调用&lt;code>Yield&lt;/code>。结束后，调用回调函数，把运行完的协程往脏池一扔，完活。&lt;/p>
&lt;h2 id="写在最后">写在最后 &lt;a href="#%e5%86%99%e5%9c%a8%e6%9c%80%e5%90%8e" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>上面我们分析了phxrpc对ucontext协程库的封装，下一篇，我们就来正式看一看ucontext是如何与IO多路复用的技术连接在一起的。&lt;/p>
&lt;p>最后上几张皂片：&lt;/p>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/16-10-13/31415098.jpg" alt="">&lt;/p>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/16-10-13/1208386.jpg" alt="">&lt;/p>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/16-10-13/97556242.jpg" alt="">&lt;/p>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/16-10-13/44196833.jpg" alt="">&lt;/p>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/16-10-13/20763058.jpg" alt="">&lt;/p></description></item></channel></rss>