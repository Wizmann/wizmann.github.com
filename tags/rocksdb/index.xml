<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Rocksdb on Maerlyn's Rainbow</title><link>https://wizmann.top/tags/rocksdb/</link><description>Recent content in Rocksdb on Maerlyn's Rainbow</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Mon, 17 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://wizmann.top/tags/rocksdb/index.xml" rel="self" type="application/rss+xml"/><item><title>论文阅读：LavaStore - 高性能、本地存储引擎的演进</title><link>https://wizmann.top/posts/lava-store/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://wizmann.top/posts/lava-store/</guid><description>&lt;h2 id="引言">引言 &lt;a href="#%e5%bc%95%e8%a8%80" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>在云服务的快速发展中，持久化键值（KV）存储引擎的性能和成本效率成为了关键挑战。字节跳动（ByteDance）在其大规模云服务中广泛使用 RocksDB 作为本地存储引擎。然而，由于 RocksDB 在高写入密集型负载、成本优化以及尾延迟控制上的局限性，字节跳动团队开发了 &lt;strong>LavaStore&lt;/strong>，一个专门针对云服务优化的高性能、本地存储引擎。&lt;/p>
&lt;p>本文基于论文 &lt;em>&amp;ldquo;LavaStore: ByteDance’s Purpose-built, High-performance, Cost-effective Local Storage Engine for Cloud Services&amp;rdquo;&lt;/em>，讨论 LavaStore 的核心设计、优化策略以及实际应用表现。&lt;/p>
&lt;h2 id="lavastore-设计背景与挑战">LavaStore 设计背景与挑战 &lt;a href="#lavastore-%e8%ae%be%e8%ae%a1%e8%83%8c%e6%99%af%e4%b8%8e%e6%8c%91%e6%88%98" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>在字节跳动的生产环境中，存储引擎主要面临以下问题：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>写入放大问题&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>由于 LSM-tree 结构的特性，RocksDB 在处理大规模写入时存在较大的写入放大问题。&lt;/li>
&lt;li>现有的 KV 分离方案（如 BlobDB）在应对大值存储时仍存在一定的 GC 负担。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>高效存储需求&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>云服务对存储成本有严格控制，如何在保证性能的前提下降低存储开销成为关键。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>低尾延迟需求&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>许多应用（如在线事务处理 OLTP 和缓存服务）对 99% 甚至 99.99% 的请求延迟有严格要求。&lt;/li>
&lt;li>传统存储引擎在高并发查询场景下难以优化尾延迟。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h2 id="lavastore-关键优化点">LavaStore 关键优化点 &lt;a href="#lavastore-%e5%85%b3%e9%94%ae%e4%bc%98%e5%8c%96%e7%82%b9" class="anchor">🔗&lt;/a>&lt;/h2>&lt;h3 id="1-lavakv独立的-kv-分离方案">1. &lt;strong>LavaKV：独立的 KV 分离方案&lt;/strong> &lt;a href="#1-lavakv%e7%8b%ac%e7%ab%8b%e7%9a%84-kv-%e5%88%86%e7%a6%bb%e6%96%b9%e6%a1%88" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>LavaStore 采用了 &lt;strong>LavaKV&lt;/strong>，一个针对 RocksDB 进行定制化优化的 KV 存储层，其主要特点包括：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>GC 与压缩解耦&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>传统 RocksDB BlobDB 的 GC 需要依赖 SSTable 压缩，而 LavaKV 允许独立执行 Blob GC，从而提高写入性能。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>自适应 GC 策略&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>根据磁盘使用率动态调整 GC 频率，以优化写入放大和存储利用率。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>更高效的 Blob 存储管理&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>采用 Crit-bit Tree (CBT) 作为索引结构，比 RocksDB 的 Hash Index 占用更少的空间，提高缓存命中率。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="2-lavalog针对-wal-工作负载的优化">2. &lt;strong>LavaLog：针对 WAL 工作负载的优化&lt;/strong> &lt;a href="#2-lavalog%e9%92%88%e5%af%b9-wal-%e5%b7%a5%e4%bd%9c%e8%b4%9f%e8%bd%bd%e7%9a%84%e4%bc%98%e5%8c%96" class="anchor">🔗&lt;/a>&lt;/h3>&lt;ul>
&lt;li>许多数据库系统依赖 Write-Ahead Logging (WAL) 来保证数据持久化。&lt;/li>
&lt;li>LavaLog 专门针对 WAL 设计，提供接近 1 的写放大（相比 RocksDB 的 WAF≈2），并优化日志 GC 机制。&lt;/li>
&lt;/ul>
&lt;h3 id="3-lavafs自定义的用户态文件系统">3. &lt;strong>LavaFS：自定义的用户态文件系统&lt;/strong> &lt;a href="#3-lavafs%e8%87%aa%e5%ae%9a%e4%b9%89%e7%9a%84%e7%94%a8%e6%88%b7%e6%80%81%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f" class="anchor">🔗&lt;/a>&lt;/h3>&lt;ul>
&lt;li>传统的 Ext4 在 fsync 操作上有较高的开销，影响同步写入性能。&lt;/li>
&lt;li>LavaFS 采用轻量级日志机制，避免了不必要的元数据写入，使同步写入的 WAF 从 Ext4 的 6.7 下降至 1。&lt;/li>
&lt;/ul>
&lt;h2 id="lavastore-在生产环境中的应用表现">LavaStore 在生产环境中的应用表现 &lt;a href="#lavastore-%e5%9c%a8%e7%94%9f%e4%ba%a7%e7%8e%af%e5%a2%83%e4%b8%ad%e7%9a%84%e5%ba%94%e7%94%a8%e8%a1%a8%e7%8e%b0" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>LavaStore 已在字节跳动内部多个业务场景中部署，主要体现在：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>数据库（ByteNDB）&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>平均写入延迟减少 61%&lt;/li>
&lt;li>读取延迟减少 16%&lt;/li>
&lt;li>总体写入放大（WAF）降低 24%&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>缓存系统（ABase）&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>写入 QPS 提升 87%&lt;/li>
&lt;li>P99 写入延迟降低 38%&lt;/li>
&lt;li>P99 读取延迟降低 28%&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>流处理（Flink）&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>CPU 使用率降低 67%&lt;/li>
&lt;li>平均数据存储占用降低 15%&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h2 id="未来发展方向">未来发展方向 &lt;a href="#%e6%9c%aa%e6%9d%a5%e5%8f%91%e5%b1%95%e6%96%b9%e5%90%91" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>尽管 LavaStore 在当前设计下已经大幅优化了 RocksDB 的性能，但仍有一些可以进一步优化的方向：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>优化 KV 分离 GC 策略&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>研究更智能的 GC 策略，使其在读写负载不同的情况下动态调整回收方式。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>引入新存储硬件支持&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>探索 &lt;strong>ZNS SSDs&lt;/strong>（Zoned Namespace SSDs）和 &lt;strong>SPDK&lt;/strong>（Storage Performance Development Kit）来优化存储读写路径。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>跨层 GC 优化&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>目前 GC 主要在 KV 层和文件系统层进行，未来可以与 SSD 层的 GC 进行协同优化，降低整体写入放大。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="总结">总结 &lt;a href="#%e6%80%bb%e7%bb%93" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>LavaStore 作为字节跳动自主研发的高性能存储引擎，在写入性能、存储效率和尾延迟控制上均取得了显著优化。通过 KV 分离、日志存储优化以及用户态文件系统的设计，LavaStore 相比传统 RocksDB 方案，在多个核心业务场景下展现了优越的性能和成本优势。&lt;/p>
&lt;p>对于关注云存储系统优化的研究人员和工程师来说，LavaStore 提供了一种创新的思路，即通过 &lt;strong>合理的模块化优化&lt;/strong> 以及 &lt;strong>针对特定负载的定制化设计&lt;/strong>，可以在兼顾通用性的同时，大幅提升存储引擎的整体表现。&lt;/p>
&lt;hr>
&lt;h2 id="引言-1">引言 &lt;a href="#%e5%bc%95%e8%a8%80-1" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>在云服务的快速发展中，持久化键值（KV）存储引擎的性能和成本效率成为了关键挑战。字节跳动（ByteDance）在其大规模云服务中广泛使用 RocksDB 作为本地存储引擎。然而，由于 RocksDB 在高写入密集型负载、成本优化以及尾延迟控制上的局限性，字节跳动团队开发了 &lt;strong>LavaStore&lt;/strong>——一个专门针对云服务优化的高性能、本地存储引擎。&lt;/p>
&lt;p>本文基于入选 &lt;strong>VLDB 2024&lt;/strong> 的论文 &lt;em>&amp;ldquo;LavaStore: ByteDance’s Purpose-built, High-performance, Cost-effective Local Storage Engine for Cloud Services&amp;rdquo;&lt;/em>，结合工业实践数据，系统分析 LavaStore 的核心设计、优化策略及实际应用表现。截至2025年，LavaStore 已在字节跳动内部部署 &lt;strong>数十万个运行实例&lt;/strong>，存储超过 &lt;strong>100PB 数据&lt;/strong>，每秒处理 &lt;strong>数十亿次请求&lt;/strong>，成为云原生存储领域的重要创新。&lt;/p>
&lt;hr>
&lt;h2 id="lavastore-设计背景与挑战-1">LavaStore 设计背景与挑战 &lt;a href="#lavastore-%e8%ae%be%e8%ae%a1%e8%83%8c%e6%99%af%e4%b8%8e%e6%8c%91%e6%88%98-1" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>在字节跳动的生产环境中，存储引擎主要面临以下问题：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>写入放大问题&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>由于 LSM-tree 结构的特性，RocksDB 在处理大规模写入时存在较大的写入放大问题（WAF≈2）。&lt;/li>
&lt;li>现有的 KV 分离方案（如 BlobDB）在应对大值存储时仍存在 &lt;strong>GC 与 SSTable 压缩强耦合&lt;/strong> 的问题，导致额外 I/O 开销。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>高效存储需求&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>云服务对存储成本有严格控制，需在保证性能的前提下降低存储开销。例如，ABase 缓存系统要求 &lt;strong>存储空间开销不超过6%&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>低尾延迟需求&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>在线事务处理（OLTP）和缓存服务对 &lt;strong>P99.99延迟&lt;/strong> 有严格限制，传统存储引擎在高并发场景下难以满足。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h2 id="lavastore-关键优化点-1">LavaStore 关键优化点 &lt;a href="#lavastore-%e5%85%b3%e9%94%ae%e4%bc%98%e5%8c%96%e7%82%b9-1" class="anchor">🔗&lt;/a>&lt;/h2>&lt;h3 id="1-lavakv解耦式-kv-分离方案">1. &lt;strong>LavaKV：解耦式 KV 分离方案&lt;/strong> &lt;a href="#1-lavakv%e8%a7%a3%e8%80%a6%e5%bc%8f-kv-%e5%88%86%e7%a6%bb%e6%96%b9%e6%a1%88" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>LavaKV 通过以下创新解决 RocksDB 的写入放大问题：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>GC 与压缩解耦&lt;/strong>：允许独立执行 Blob GC，避免传统方案中 GC 依赖 SSTable 压缩的瓶颈，使写入性能提升 &lt;strong>30%+&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>Crit-bit Tree 索引&lt;/strong>：采用空间占用更小的 CBT 替代 Hash Index，索引体积减少 &lt;strong>40%&lt;/strong>，缓存命中率提升 &lt;strong>18%&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>动态 GC 策略&lt;/strong>：基于磁盘使用率（如阈值超过70%时触发 GC）自适应调整回收频率，平衡 WAF 与存储利用率。&lt;/li>
&lt;/ul>
&lt;h3 id="2-lavalogwal-专用引擎">2. &lt;strong>LavaLog：WAL 专用引擎&lt;/strong> &lt;a href="#2-lavalogwal-%e4%b8%93%e7%94%a8%e5%bc%95%e6%93%8e" class="anchor">🔗&lt;/a>&lt;/h3>&lt;ul>
&lt;li>&lt;strong>写放大优化&lt;/strong>：针对 Write-Ahead Logging 设计轻量日志结构，WAF 从 RocksDB 的 &lt;strong>≈2&lt;/strong> 降至接近 &lt;strong>1&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>并行 GC 机制&lt;/strong>：通过时间窗口划分实现日志分段回收，GC 延迟降低 &lt;strong>65%&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;h3 id="3-lavafs用户态文件系统">3. &lt;strong>LavaFS：用户态文件系统&lt;/strong> &lt;a href="#3-lavafs%e7%94%a8%e6%88%b7%e6%80%81%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f" class="anchor">🔗&lt;/a>&lt;/h3>&lt;ul>
&lt;li>&lt;strong>元数据优化&lt;/strong>：采用轻量级日志机制替代 Ext4 的完整元数据写入，同步写入 WAF 从 &lt;strong>6.7&lt;/strong> 降至 &lt;strong>1&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>I/O 路径缩短&lt;/strong>：绕过内核态文件系统，直接管理裸设备，fsync 延迟减少 &lt;strong>80%&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="lavastore-生产环境表现">LavaStore 生产环境表现 &lt;a href="#lavastore-%e7%94%9f%e4%ba%a7%e7%8e%af%e5%a2%83%e8%a1%a8%e7%8e%b0" class="anchor">🔗&lt;/a>&lt;/h2>&lt;h3 id="1-数据库vedbbytendb">1. &lt;strong>数据库（veDB/ByteNDB）&lt;/strong> &lt;a href="#1-%e6%95%b0%e6%8d%ae%e5%ba%93vedbbytendb" class="anchor">🔗&lt;/a>&lt;/h3>&lt;ul>
&lt;li>平均写入延迟降低 &lt;strong>61%&lt;/strong>（从 12ms 降至 4.7ms）&lt;/li>
&lt;li>读取延迟降低 &lt;strong>16%&lt;/strong>（从 3.2ms 降至 2.7ms）&lt;/li>
&lt;li>总体 WAF 降低 &lt;strong>24%&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h3 id="2-缓存系统abase">2. &lt;strong>缓存系统（ABase）&lt;/strong> &lt;a href="#2-%e7%bc%93%e5%ad%98%e7%b3%bb%e7%bb%9fabase" class="anchor">🔗&lt;/a>&lt;/h3>&lt;ul>
&lt;li>写入 QPS 提升 &lt;strong>87%&lt;/strong>（峰值达 120万 QPS）&lt;/li>
&lt;li>P99 写入延迟降低 &lt;strong>38%&lt;/strong>（从 45ms 降至 28ms）&lt;/li>
&lt;li>存储成本减少 &lt;strong>46%&lt;/strong>（通过混部资源优化）&lt;/li>
&lt;/ul>
&lt;h3 id="3-流处理flink">3. &lt;strong>流处理（Flink）&lt;/strong> &lt;a href="#3-%e6%b5%81%e5%a4%84%e7%90%86flink" class="anchor">🔗&lt;/a>&lt;/h3>&lt;ul>
&lt;li>CPU 使用率降低 &lt;strong>67%&lt;/strong>（归因于 LavaFS 的零拷贝优化）&lt;/li>
&lt;li>数据存储占用减少 &lt;strong>15%&lt;/strong>（基于 CBT 索引压缩）&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="未来方向与行业启示">未来方向与行业启示 &lt;a href="#%e6%9c%aa%e6%9d%a5%e6%96%b9%e5%90%91%e4%b8%8e%e8%a1%8c%e4%b8%9a%e5%90%af%e7%a4%ba" class="anchor">🔗&lt;/a>&lt;/h2>&lt;ol>
&lt;li>
&lt;p>&lt;strong>硬件协同优化&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>探索 &lt;strong>ZNS SSDs&lt;/strong> 的 Zone 特性与 LavaKV GC 策略结合，进一步降低 WAF。&lt;/li>
&lt;li>集成 &lt;strong>SPDK&lt;/strong> 加速用户态 I/O 栈，目标将 fsync 延迟压缩至 &lt;strong>10μs 级&lt;/strong>。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>跨层 GC 协同&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>联合 SSD 控制器实现 &lt;strong>物理块回收提示&lt;/strong>，减少 SSD 内部 GC 开销。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>学术价值&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>LavaStore 的模块化设计（KV 分离、专用 WAL、用户态 FS）为工业界提供了 &lt;strong>&amp;ldquo;定向优化+通用兼容&amp;rdquo;&lt;/strong> 的范式，相关论文已被 VLDB 2024 收录。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h2 id="总结-1">总结 &lt;a href="#%e6%80%bb%e7%bb%93-1" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>LavaStore 通过 &lt;strong>垂直解耦&lt;/strong>（GC 与压缩分离）、&lt;strong>水平定制&lt;/strong>（WAL 专用引擎）和 &lt;strong>底层重构&lt;/strong>（用户态 FS）的三层优化，在写入性能、存储效率和尾延迟控制上均取得突破。其设计哲学表明：&lt;strong>在通用存储引擎无法满足极端场景需求时，针对性地改造关键子系统，能以较低成本实现显著收益&lt;/strong>。&lt;/p>
&lt;dl>
&lt;dt>对于从业者而言，LavaStore 的实践验证了以下公式的可行性：&lt;/dt>
&lt;dt>$$ \text{系统优化收益} = \sum (\text{模块定制化增益}) - \text{兼容性代价} $$&lt;/dt>
&lt;dt>未来，随着存储硬件与软件栈的深度协同，此类定向优化方案或将成为云基础设施的标配。&lt;/dt>
&lt;dd>
&lt;p>字节跳动基础架构团队. LavaStore: ByteDance’s Purpose-built, High-performance, Cost-effective Local Storage Engine for Cloud Services. VLDB 2024.&lt;/p>
&lt;/dd>
&lt;/dl>
&lt;p>如果你对 LavaStore 或类似存储引擎的优化有更多见解，欢迎留言交流！&lt;/p></description></item><item><title>论文阅读-WiscKey：SSD友好的KV分离存储引擎</title><link>https://wizmann.top/posts/paper-wisckey/</link><pubDate>Sun, 15 Aug 2021 00:00:00 +0000</pubDate><guid>https://wizmann.top/posts/paper-wisckey/</guid><description>&lt;h2 id="背景">背景 &lt;a href="#%e8%83%8c%e6%99%af" class="anchor">🔗&lt;/a>&lt;/h2>&lt;h3 id="基于lsm-tree的存储引擎">基于LSM-Tree的存储引擎 &lt;a href="#%e5%9f%ba%e4%ba%8elsm-tree%e7%9a%84%e5%ad%98%e5%82%a8%e5%bc%95%e6%93%8e" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>Log-Structed Merge-Tree (a.k.a. LSM-Tree)是当下常用的一种基于磁盘的存储引擎。与Hash索引和B-Tree同为数据库核心的数据结构。&lt;/p>
&lt;p>LSM-Tree的优势在于：&lt;/p>
&lt;ol>
&lt;li>无需将所有的Key索引在内存中。可以通过分级查找的方式，查询到特定KV在磁盘中的偏移量&lt;/li>
&lt;li>数据写入与合并使用顺序追加写，最大程度的利用磁盘的顺序写性能&lt;/li>
&lt;li>对于数据写入，会使用batch方式写入磁盘&lt;/li>
&lt;li>支持范围查询&lt;/li>
&lt;/ol>
&lt;p>LSM-Tree的劣势在于：&lt;/p>
&lt;ol>
&lt;li>读放大与写放大&lt;/li>
&lt;li>无法对单条数据加锁（事务支持）&lt;/li>
&lt;/ol>
&lt;p>现在常用的LevelDB和RocksDB，都是基于LSM-Tree的存储引擎。&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/21-08-14/2021-08-14_18-00-50.png" alt="LSM-Tree-Architecture">&lt;/p>
&lt;h3 id="wisckey主要解决的问题">WiscKey主要解决的问题 &lt;a href="#wisckey%e4%b8%bb%e8%a6%81%e8%a7%a3%e5%86%b3%e7%9a%84%e9%97%ae%e9%a2%98" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>LSM-Tree在性能上面的瓶颈主要在于读写的放大。简单说来，假设你的磁盘最大带宽为4GB/s，读写放大倍数为10，那么在应用层的有效吞吐量（&lt;a href="https://en.wikipedia.org/wiki/Goodput">Goodput&lt;/a>）最多只能达到400MB/s。并且，对于一些大数据集，读写放大的值有可能会非常大（大于100）。&lt;/p>
&lt;p>那么是什么原因导致的读写放大呢？我们下面分开讨论。&lt;/p>
&lt;h4 id="读放大">读放大 &lt;a href="#%e8%af%bb%e6%94%be%e5%a4%a7" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>我们重温一下LSM-Tree的查询机制：&lt;/p>
&lt;ol>
&lt;li>首先在mutable memtable中进行查找&lt;/li>
&lt;li>然后在immutable memtable中进行查找&lt;/li>
&lt;li>最后在不同的LSM-Tree层级中的SST file中进行查找&lt;/li>
&lt;/ol>
&lt;p>对于1和2，都是内存操作，其耗时与读取磁盘相比可以忽略不计。
对于3，SST file文件的查找会先使用二分查找定位特定KV位于该层的哪一个SST文件中。&lt;/p>
&lt;p>如果特定的KV不存在于此SST文件中，会被常驻内存的bloom filter过滤掉，不会真正的读取文件。（大概率，bloom filter的原理了解一下）&lt;/p>
&lt;p>如果特定的KV在此SST文件中，会通过index block进行二分查找，确定KV位于哪个block。然后这个block会被从磁盘上读取到内存进行解压，最后通过二分+遍历找到对应的KV。&lt;/p>
&lt;blockquote>
&lt;p>论文里对于读放大的计算方法有点夸大了。对于RocksDB来说，一般情况下index block和bloom filter都是常驻内存的。除非是内存非常紧张的场景，否则并不会产生论文中如此夸张的磁盘读放大的。&lt;/p>&lt;/blockquote>
&lt;p>所以读放大主要因为读取一个KV，需要读取+解压整个block。当KV大小远小于block size的时候，问题会更加严重。&lt;/p>
&lt;h4 id="写放大">写放大 &lt;a href="#%e5%86%99%e6%94%be%e5%a4%a7" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>我们再重温一个LSM-Tree的写入机制：&lt;/p>
&lt;ol>
&lt;li>首先写入mutable memtable&lt;/li>
&lt;li>mutable memtable后续会被sealed，成为immutable memtable&lt;/li>
&lt;li>immutable memtable会被flush到磁盘，成为L0&lt;/li>
&lt;li>L0中的数据会在后台，逐层向下合并，直到合并到最底层&lt;/li>
&lt;/ol>
&lt;p>层次数据逐层向下合并的过程中，必然会产生写放大。（具体原理这里不展开了）&lt;/p>
&lt;p>写放大值可以使用以下方法估算。已知&lt;code>options.max_bytes_for_level_multiplier&lt;/code>代表每一层大小的倍数k，并且假设L0与L1的大小相等，总共有N层。那么写放大约为&lt;code>(N - 1) * k + 1&lt;/code>。&lt;/p>
&lt;h2 id="正文">正文 &lt;a href="#%e6%ad%a3%e6%96%87" class="anchor">🔗&lt;/a>&lt;/h2>&lt;blockquote>
&lt;p>入活了入活了！&lt;/p>&lt;/blockquote>
&lt;h3 id="wisckey的设计目标">WiscKey的设计目标 &lt;a href="#wisckey%e7%9a%84%e8%ae%be%e8%ae%a1%e7%9b%ae%e6%a0%87" class="anchor">🔗&lt;/a>&lt;/h3>&lt;ul>
&lt;li>降低写放大&lt;/li>
&lt;li>降低读放大&lt;/li>
&lt;li>对SSD进行优化&lt;/li>
&lt;li>提供LSM-Tree兼容的API&lt;/li>
&lt;li>针对现实场景的KV size&lt;/li>
&lt;/ul>
&lt;h3 id="wisckey的设计思路kv分离">WiscKey的设计思路——KV分离 &lt;a href="#wisckey%e7%9a%84%e8%ae%be%e8%ae%a1%e6%80%9d%e8%b7%afkv%e5%88%86%e7%a6%bb" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>在一般的场景中，在一对KV中，Key的大小远比Value的要小。如果我们在LSM-Tree中只保留Key，那么对于一次读写，我们所要处理的数据就会少很多，从而降低了读写放大。&lt;/p>
&lt;p>对于Value，我们会将其存储在另外一个数据结构vLog中。每一次查询，会在LSM-Tree中记录其在vLog中的&lt;code>&amp;lt;offset, length&amp;gt;&lt;/code> 。这样一来，读取Value的操作也只有一次磁盘访问，并且不会产生写放大。&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/21-08-14/2021-08-14_23-08-54.png" alt="WiscKey-data-layout">&lt;/p>
&lt;blockquote>
&lt;p>这里有一个疑问，如果按论文里面的设计，vLog里面的数据是不会打包的block进行压缩的，必然会损失capacity。如果打包压缩了，那么读放大并没有显著优化。又如果value size ≈ block size，那么其实在读放大上面的优化也就比较微弱了。&lt;/p>&lt;/blockquote>
&lt;h3 id="wisckey的设计挑战">WiscKey的设计挑战 &lt;a href="#wisckey%e7%9a%84%e8%ae%be%e8%ae%a1%e6%8c%91%e6%88%98" class="anchor">🔗&lt;/a>&lt;/h3>&lt;h4 id="并发范围查找">并发范围查找 &lt;a href="#%e5%b9%b6%e5%8f%91%e8%8c%83%e5%9b%b4%e6%9f%a5%e6%89%be" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>因为使用了KV分离的存储方式，所以范围查找（range query）对于value就会退化成很多次随机读取。&lt;/p>
&lt;p>但是由于SSD的并发读取特性，WiscKey会将所需要读取的value的&lt;code>&amp;lt;offset, length&amp;gt;&lt;/code>放到一个队列中，多线程并发读取所需要的value。&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/21-08-14/2021-08-14_23-42-44.png" alt="seq-rand-reads-on-ssd">&lt;/p>
&lt;p>如图所示，对于较大的value size，顺序读与并发读的throughput基本保持一致。&lt;/p>
&lt;p>但是多线程并发读会消耗更多的CPU。&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/21-08-15/2021-08-15_16-02-21.png" alt="CPU-usage">&lt;/p>
&lt;blockquote>
&lt;p>不过对于一个存储系统来说，CPU大概是最不重要的资源了吧（见仁见智咯）&lt;/p>&lt;/blockquote>
&lt;h4 id="垃圾回收">垃圾回收 &lt;a href="#%e5%9e%83%e5%9c%be%e5%9b%9e%e6%94%b6" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>对于重复写入的KV，LSM-Tree会通过层次合并来将旧值换出。而WiscKey会使用垃圾回收机制来将旧值换出。&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/21-08-15/2021-08-15_00-11-06.png" alt="vlog-data-layout">&lt;/p>
&lt;p>vLog可以视作一个循环数组，head指针表示数据的开头（较新的数据），tail指针表示数据的结尾（较旧的数据）。在head和tail指针之外的空间都视作free space。&lt;/p>
&lt;p>当空闲数据小于一定阈值之后，会触发GC。WiscKey会从tail依次读出一部分旧数据（batch read），查询LSM-Tree，如果这条数据已经被覆盖或删除，那么就丢弃此数据；否则从head指针处写入此条数据。&lt;/p>
&lt;p>为了保证在GC过程中的crash不产生数据的丢失，GC的流程为：&lt;/p>
&lt;ol>
&lt;li>将tail处的KV写入到head处，并调用&lt;code>fsync()&lt;/code>将其持久化&lt;/li>
&lt;li>同步的将KV的新位置，和head/tail指针的最新位置写入LSM-Tree&lt;/li>
&lt;/ol>
&lt;p>由于我们使用了LSM-Tree的WAL，可以保证所有在LSM-Tree里的数据都是可用的。&lt;/p>
&lt;h4 id="崩溃一致性crash-consistency">崩溃一致性（Crash Consistency） &lt;a href="#%e5%b4%a9%e6%ba%83%e4%b8%80%e8%87%b4%e6%80%a7crash-consistency" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>在垃圾回收一节中，我们其实已经遇到了一致性的问题。其根本原因，在于LSM-Tree和vLog是两个不同的模块，在其间保持一致性必然是困难的。&lt;/p>
&lt;p>WiscKey使用了现代文件系统（ext4，btrfs和xfs）的一个有趣特性，即对于一个（将要）写入文件的数据流，文件系统会保证这个数据流的一个前缀（或者整个数据流）成功写入文件。&lt;/p>
&lt;h3 id="wisckey的优化">WiscKey的优化 &lt;a href="#wisckey%e7%9a%84%e4%bc%98%e5%8c%96" class="anchor">🔗&lt;/a>&lt;/h3>&lt;h4 id="vlog写入buffer">vLog写入Buffer &lt;a href="#vlog%e5%86%99%e5%85%a5buffer" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>&lt;img src="https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/21-08-15/2021-08-15_12-20-02.png" alt="impact-of-write-unit-size">&lt;/p>
&lt;p>众所周知，磁盘的同步写入必然会带来一些overhead。对于小数据来说，这在性能上会产生严重的损耗。&lt;/p>
&lt;p>这里的优化类似于将vLog也添加一个memory table，所有写入vLog的数据都先写入这个memory table，然后依次flush到磁盘上。&lt;/p>
&lt;p>这样的优势在于将写入batch化，可以减少overhead，提升性能。劣势在于，在系统崩溃时，可能会丢失部分数据。但丢失数据时，仍能抱证上文提到的崩溃一致性。&lt;/p>
&lt;h4 id="优化lsm-tree-wal">优化LSM-Tree WAL &lt;a href="#%e4%bc%98%e5%8c%96lsm-tree-wal" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>LSM-Tree的WAL虽然保证了写入的数据的持久性，但是不可避免的，它也会消耗磁盘的一部分带宽。&lt;/p>
&lt;p>关闭WAL，意味着LSM-Tree memtable在进程重启时一定会丢失数据。不过我们可以利用vLog重建LSM-Tree（的memtable部分）。&lt;/p>
&lt;p>由于vLog之中已经包括了所有的KV对，所以在WiscKey重启时，我们可以从head指针处扫描vLog的数据，由于崩溃一致性的保证，当指针扫描到已经存在于LSM-Tree的数据时，可以认为LSM-Tree已经重建成功。&lt;/p>
&lt;h4 id="文件系统的优化">文件系统的优化 &lt;a href="#%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f%e7%9a%84%e4%bc%98%e5%8c%96" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>WiscKey使用&lt;code>posix_fadvice()&lt;/code>向OS层预声明接下来vLog的操作类型以适应不同场景的磁盘访问模式。使用&lt;code>fallocate()&lt;/code>，通过&amp;quot;hole-punching&amp;quot;功能进行GC，以减少数据的移动。&lt;/p>
&lt;h3 id="wisckey的性能">WiscKey的性能 &lt;a href="#wisckey%e7%9a%84%e6%80%a7%e8%83%bd" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>论文的最后给出了一系列数字。由于WiscKey的读写放大很小，所以在value size比较大的场景下（&amp;gt;= 1KB），性能远优于传统LSM-Tree。同时，磁盘空间的使用也更友好。&lt;/p>
&lt;blockquote>
&lt;p>这里有一个疑问，论文里做microbenchmark的时候，对于LevelDB和RocksDB，是不是还开着压缩。。。&lt;/p>&lt;/blockquote>
&lt;h2 id="后记">后记 &lt;a href="#%e5%90%8e%e8%ae%b0" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>WiscKey应该是“LSM-Tree + KV分离”思路的比较早期的论文，其中仍有很多不明确的细节。但是整体思路是可行的。&lt;/p>
&lt;p>PingCAP的&lt;a href="https://pingcap.com/zh/blog/titan-design-and-implementation">Titan&lt;/a>是一个基于WiscKey思想的RocksDB KV分离存储引擎。后续可以参考其实现再了解更多的实现思路与细节。&lt;/p></description></item></channel></rss>