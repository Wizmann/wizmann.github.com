<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Paxos on Maerlyn's Rainbow</title><link>https://wizmann.top/tags/paxos/</link><description>Recent content in Paxos on Maerlyn's Rainbow</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Wed, 05 Dec 2018 00:00:00 +0000</lastBuildDate><atom:link href="https://wizmann.top/tags/paxos/index.xml" rel="self" type="application/rss+xml"/><item><title>白话一致性协议 - Paxos、Raft和PacificA[1]</title><link>https://wizmann.top/posts/paxos-raft-pecifaca1/</link><pubDate>Wed, 05 Dec 2018 00:00:00 +0000</pubDate><guid>https://wizmann.top/posts/paxos-raft-pecifaca1/</guid><description>&lt;h2 id="书接上文---multi-paxos">书接上文 - Multi Paxos &lt;a href="#%e4%b9%a6%e6%8e%a5%e4%b8%8a%e6%96%87---multi-paxos" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>在上一篇文章中，我们提到了Basic Paxos和Multi Paxos的异同。在&lt;a href="https://lamport.azurewebsites.net/pubs/paxos-simple.pdf">Paxos Made Simple&lt;/a>论文中，作者提到了Multi Paxos的一种实现。这个实现允许我们对一个连续的数据流（也可以称为复制日志，replicated log）达成共识，从而实现节点状态的一致性复制。&lt;/p>
&lt;h3 id="确定性状态机">确定性状态机 &lt;a href="#%e7%a1%ae%e5%ae%9a%e6%80%a7%e7%8a%b6%e6%80%81%e6%9c%ba" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>我们可以将系统中的每一个节点抽象为一个有着确定性状态机，即给定多个状态一致的状态机，在执行同一个命令之后，其状态仍保持一致。（可以想一想编译原理里面的DFA）&lt;/p>
&lt;h3 id="leader---系统中唯一的proposer">Leader - 系统中唯一的proposer &lt;a href="#leader---%e7%b3%bb%e7%bb%9f%e4%b8%ad%e5%94%af%e4%b8%80%e7%9a%84proposer" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>如果系统中存在有多个proposer，那么就很可能会出现多个提案相互干扰的情况。虽然根据证明，最终这些提案都会收敛到一致，但是性能会非常低下。所以我们可以在系统中通过选举，选出一个leader做为主proposer（distinguishied proposer），所有的提案都由leader提出。&lt;/p>
&lt;p>这样一来，在绝大多数情况下都不会出现提案相互干扰的情况。只有在leader切换的瞬间，可能会出现相同编号的不同提案，但是我们的算法可以很好的处理这种情况。&lt;/p>
&lt;h3 id="分布式系统中的tcp">分布式系统中的“TCP” &lt;a href="#%e5%88%86%e5%b8%83%e5%bc%8f%e7%b3%bb%e7%bb%9f%e4%b8%ad%e7%9a%84tcp" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>类似于TCP协议中序列号，Multi Paxos中的每一个命令都有一个递增的编号。即我们前一个执行的命令是100号，那么下一个执行的命令一定是101号。每一个命令都是一个Paxos实例，Leader向所有节点发布这个提案，在提案达成一致之后（多数节点返回ACK），就可以认为这个命令已经达成了一致。&lt;/p>
&lt;p>和TCP一样，如果我们顺序的发布并表决提案，效率会非常低下（TCP停等模型）。所以，Multi Paxos采用类似滑动窗口的方案，每次对N个提案进行表决，以增加表决的带宽。&lt;/p>
&lt;p>和TCP不同的是，如果某些序号的TCP包在传输中丢失，最坏的情况是我们会RST这条链接，其它的工作都交给应用层逻辑来解决。&lt;/p>
&lt;p>但是对于Multi Paxos来说，如果某些提案没有被表决，那么就会在日志中留下空洞（gap）。这会直接影响系统的一致性。如果恰巧这个时候发生了Leader失效，那么新选举出来的Leader节点就要处理日志中的空洞。&lt;/p>
&lt;p>解决空洞的原理也很简单，就是Leader向所有成员询问，对于这个提案，是否已经有达成共识的值。如果有的话，就使用这个值。如果没有，就用一个no-op（无操作）命令来填补这个空位。但是，对于实际工程中来说，我们还需要解决未达成共识时值的冲突等情况。&lt;/p>
&lt;h2 id="为什么我们还需要raft">为什么我们还需要Raft？ &lt;a href="#%e4%b8%ba%e4%bb%80%e4%b9%88%e6%88%91%e4%bb%ac%e8%bf%98%e9%9c%80%e8%a6%81raft" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>Multi-Paxos在现实的工程当中更多的是一种符号。因为理论与实践上的隔阂是如此之大，如果想在工程意义上实现一个可用的Multi-Paxos算法，必然会在原算法的基础上进行一系列的魔改，这些魔改虽然均声称自己实现了Multi-Paxos算法，但是这些算法大多不能被证明是正确的。&lt;/p>
&lt;p>Raft的目标是，即让算法满足工程化需要，又能保证其正确性。&lt;/p>
&lt;blockquote>
&lt;p>Raft论文当中说Paxos算法难以理解，我并不这么觉得。因为Paxos论文里面把困难的部分都一笔带过了。只剩下简单的那部分了。&lt;/p>&lt;/blockquote>
&lt;h2 id="raft-vs-paxos">Raft vs. Paxos &lt;a href="#raft-vs-paxos" class="anchor">🔗&lt;/a>&lt;/h2>&lt;h3 id="leader选举">Leader选举 &lt;a href="#leader%e9%80%89%e4%b8%be" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>在Paxos论文中，Leader选举被视作一种特殊的“提案选举”。只需要Proposer和Acceptor进行一轮或多轮（取决于运气）投票，就可以确定Leader。&lt;/p>
&lt;p>但在实际工程中，我们需要考虑以下的问题：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>如何判断Leader是否存活&lt;/p>
&lt;/li>
&lt;li>
&lt;p>是否每一个节点都有资格担当Leader&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h4 id="leader的任期">Leader的任期 &lt;a href="#leader%e7%9a%84%e4%bb%bb%e6%9c%9f" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>Leader在被选举出来之后，都会被赋予一个任期编号（term）。在任期里，Leader会向所有成员发送心跳包以延续自己的任期。&lt;/p>
&lt;p>如果Leader失效无法发送心跳包的话，成员就会产生一个“选举超时”，此时就会重新触发一轮选举。&lt;/p>
&lt;p>选举的流程和Basic-Paxos算法类似，proposer向所有成员发送“我要当老大”的提案，成员们会酌情回复。如果得到了多数成员的肯定，这个proposer就是下一个任期的Leader了。&lt;/p>
&lt;p>从本质上说，每一个任期的Leader选举，都是一个独立的Basic-Paxos实例。任期号相当于Paxos里面的提案编号。&lt;/p>
&lt;h4 id="leader资格的认定">Leader资格的认定 &lt;a href="#leader%e8%b5%84%e6%a0%bc%e7%9a%84%e8%ae%a4%e5%ae%9a" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>Raft采用强一致性的模型，对于已经ACK的用户请求，要尽力保证其状态不丢失。&lt;/p>
&lt;blockquote>
&lt;p>如果你问我，现在来一核弹把机房炸了，数据都丢了，那你怎么能保证强一致性。 &lt;br>
其实这个问题非常简单。很明显，我们保证了丢数据的强一致性。&lt;/p>&lt;/blockquote>
&lt;p>所以我们要选出一个Leader，使其能够包含所有已经ACK的提案。当一个proposer向其它节点发送提案时，就会收到其它节点的响应。因为一个已经ACK的提案必然被多数节点所认可，所以如果一个proposer没有包含所有被ACK的提案时，它的提案就会被其它包含更多状态的节点驳回。最后被选出来的Leader，一定是包含所有被ACK的状态的节点。&lt;/p>
&lt;h3 id="日志复制">日志复制 &lt;a href="#%e6%97%a5%e5%bf%97%e5%a4%8d%e5%88%b6" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>当Leader被选出后，Leader就会开始处理用户的请求。用户的请求可以看做一系列的命令，在接收到提案后，提案首先被分发到所有节点，节点的状态机顺序执行这些命令。在多数节点返回ACK后，这个命令就被视为“已提交”（commited）。&lt;/p>
&lt;p>上文中已经提到了一致性算法中的日志非常类似于网络协议中的TCP。即如果两个命令的ID一样，那么其内容必定也一样；如果两个节点都有认可了编号为p的命令，那么所有编号小于p的命令也必定保持一致。（被称为Log Matching Property）&lt;/p>
&lt;p>Raft为了简化算法的工程实现，把节点的状态抽象为严格append only的日志。即我们可以将日志指针向后或向前移动，来“回滚”或“更新”状态。但是绝对不允许在日志中间添加或删除日志条目。所以，在Leader发生变化时，如果leader和其他follower之间的日志不同，那么follower需要回滚日志以保持和leader日志的一致性。&lt;/p>
&lt;h3 id="安全性">安全性 &lt;a href="#%e5%ae%89%e5%85%a8%e6%80%a7" class="anchor">🔗&lt;/a>&lt;/h3>&lt;h4 id="不归我管的事我不拿主意">不归我管的事我不拿主意 &lt;a href="#%e4%b8%8d%e5%bd%92%e6%88%91%e7%ae%a1%e7%9a%84%e4%ba%8b%e6%88%91%e4%b8%8d%e6%8b%bf%e4%b8%bb%e6%84%8f" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>前文我们说到，一个节点要成为Leader，一定要拥有所有已经被ACK的状态，否则就会被其它节点驳回。&lt;/p>
&lt;p>但是现实都会出现一些小小的意外。在系统的运行过程中，如果有一些提案只被少数节点认可，与此同时发起提案的Leader意外退出。那么在不同节点上的日志会产生“分叉”，那么我们如何解决日志当中的冲突呢？&lt;/p>
&lt;p>很明显，因为这些以少数节点认可的提案并没有被确认。所以我们无论是接受提案还是驳回提案，都不影响我们强一致性的要求。所以关键是处理冲突，使其不产生影响系统一致性的后效。&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/18-12-02/Snipaste_2018-12-02_17-34-59.png" alt="">&lt;/p>
&lt;p>假设在Term1，最后一个提案是&amp;quot;Value:3&amp;quot;，这个提案并没有得到多数节点的认可，Leader就挂掉了。选出来了一个新Leader，Term数加1。在Term2，被提出的第一个提案是&amp;quot;Value:1&amp;quot;，这个提案也没有得到多数节点的认可，Leader也挂掉了。&lt;/p>
&lt;p>因为这两个分叉的提案都没有得到多数节点的认可，所以下一个Leader可能已经确认这两个提案，或者两个中的一个，也可能一个都没有确认。新的Leader在被选出后，需要面对的第一个问题是如何处理属于旧Term的提案。&lt;/p>
&lt;p>解决方案有两种：&lt;/p>
&lt;ol>
&lt;li>新Leader将日志中仍没有被多数节点认可的提案重新提出，直到被多数节点认可为止&lt;/li>
&lt;li>新Leader忽略属于旧的Term的提案，只提交属于本Term的提案；对于日志冲突，使用复制日志的方法解决，但不会显式认可旧Term的提案。&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/18-12-02/Snipaste_2018-12-02_22-02-45.png" alt="">&lt;/p>
&lt;p>对于方案1，有一个隐含的问题。如上图所示，在时间点1，S2为Leader，标红的两个提案并没有被确认。此时如果Leader在时间点2重新提出&amp;quot;Term1/Value3&amp;quot;，并且得到了S1和S2的认可，那么这条提案已经被多数节点认可。但是在时间点3，S3被选举为了Leader。S1和S2需要回滚日志以保持与S3日志的一致。此时就出现了一种情况，那就是已经被确认的日志被回滚掉了，强一致性就不能满足了。&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/18-12-02/Snipaste_2018-12-02_22-12-50.png" alt="">&lt;/p>
&lt;p>如果我们采用方案2，那么被选出的新Leader提出的第一个提案的Term一定为3。确认新提案的节点接下来会复制Leader的日志，回滚掉没有被认可的提案&amp;quot;Term2/Value1&amp;quot;。对于标绿的&amp;quot;Term1/Value3&amp;quot;，虽然被复制到了其它的节点上，但是这个值并不会被确认。这样一来，我们既保证了已经被确认的提案不会被回滚，又保证了日志的一致性。&lt;/p>
&lt;p>在具体实现中，“T3/V1”往往是一个空命令(no-op)。这样一来，即使没有写请求，Leader就可以更快的确认新的任期并同步Log了。&lt;/p>
&lt;h4 id="如何证明">如何证明 &lt;a href="#%e5%a6%82%e4%bd%95%e8%af%81%e6%98%8e" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>那么怎么保证上面的作法它是正确的呢？&lt;/p>
&lt;p>假设当前任期为T1，此时由于系统故障，我们选出了新的Leader - S2，并记Term为T2。因为我们严格遵守了Log Matching Property。&lt;/p>
&lt;p>那么，对于以下两种情况：&lt;/p>
&lt;ol>
&lt;li>T2的Leader和Voter的Log中最后一个提案的编号是一致的，那么可以知道他们日志中的提案都是完全一致的&lt;/li>
&lt;li>T2中Leader的Log比Voter更新，那么Leader一定包含比Voter更多的提案；否则Voter就不会给Leader投票&lt;/li>
&lt;/ol>
&lt;p>我们都可以证明，对于前一个Term已经被确认的提案，一定会被包含在后一个Term的日志中。也就是说，一个被确认的提案不会中途丢失。&lt;/p>
&lt;h2 id="成员变化">成员变化 &lt;a href="#%e6%88%90%e5%91%98%e5%8f%98%e5%8c%96" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>以上我们的讨论都基于系统的节点不会发生变更，但是在现实工程中，我们很难对此进行任何保证。所以一个实用的系统，一定能解决成员变化的问题。&lt;/p>
&lt;p>成员变化问题的本质是系统中不能同时出现两个Leader。&lt;/p>
&lt;p>在Raft中，我们将过渡期的配置称为“共同一致”（joint consensus），一但它被确认，说明系统已经过渡到了新的成员配置。&lt;/p>
&lt;p>具体的策略如下：&lt;/p>
&lt;ol>
&lt;li>系统中只有旧的配置C_old，新加入的成员不可能成为Leader&lt;/li>
&lt;li>当前系统的Leader接受到新的配置C_new。然后Leader向所有节点发起修改配置为C_old+new的提案。&lt;/li>
&lt;li>即使这个时间点Leader挂掉了，新的Leader也只会拥有旧配置C_old或者过渡期配置C_old+new。这取决于Leader选举的时机（和运气）。&lt;/li>
&lt;li>当过渡期配置C_old+new被多数节点确认后，Leader向所有节点发起修改配置为C_new的提案。&lt;/li>
&lt;li>如果这个时间点Leader挂掉了，新Leader会从拥有C_old+new和C_new的节点中选出。新的Leader仍然可以进行配置的变更，而不影响整个系统的安全性。&lt;/li>
&lt;li>直到C_new被确认，配置更换宣告完成。&lt;/li>
&lt;/ol>
&lt;p>“共同一致”允许节点无需考虑安全性的情况下，在任意时间进行配置的更换。配置的更换也不会影响客户端的请求。&lt;/p>
&lt;p>以上我们解决的是安全性问题，但是在实际工程中我们还需要兼顾效率问题。&lt;/p>
&lt;p>例如新加入的节点可以视作“non-voting members”，只同步数据，不能对提案进行投票。直到数据同步基本完成，才进行配置变更。这样避免了新的节点由于缺少Log不能及时的处理提案。&lt;/p>
&lt;p>又如新的成员列表中，不包含原先的Leader节点。在旧Leader认可了新的配置提案之后，就可以退位让贤，让其它节点选举出新的Leader。&lt;/p>
&lt;p>以及被清除出成员列表的节点，不会收到后续的心跳，它们会认为Leader已经失效，所以自己跳出来竞选Leader。这样一来，就会触发新的（无意义 的）Leader选举，影响系统的可用性。解决方案是，如果一个节点在一个时间段内收到了Leader的心跳，那么就会忽略Leader的竞选请求。这样既不会影响正常的选举，又可以屏蔽无效的选举请求。&lt;/p>
&lt;h2 id="快照">快照 &lt;a href="#%e5%bf%ab%e7%85%a7" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>我们的提案越来越多，日志也越来越长。随之而来的是漫长的恢复时间以及磁盘空间的浪费。快照技术可以帮我们清除旧的无用日志，只保留有用的状态信息。&lt;/p>
&lt;p>在Raft算法中，每个节点都能自主的生成快照。好处是避免Leader分发快照造成的效率降低，也简化了Leader的功能和职责。又由于日志的“TCP特性”，所以不同节点上，只要保证提案编号一致，那么其内容就可以保证一致。&lt;/p>
&lt;h2 id="客户端交互">客户端交互 &lt;a href="#%e5%ae%a2%e6%88%b7%e7%ab%af%e4%ba%a4%e4%ba%92" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>由于客户端不了解系统内部的选举情况，所以在开始通信时，会随机选一台节点发送请求。如果这个节点不是Leader，就会拒绝这个请求，会告知客户端哪个节点是当前任期的Leader。如果Leader失效，客户端请求超时，就会重新随机选择节点，获取新任Leader信息。&lt;/p>
&lt;p>对于客户端发送的写请求，Leader需要记录其唯一的请求ID，以避免客户端发送的重复请求。&lt;/p>
&lt;p>对于客户端发送的读请求，Leader需要再次确认它是仍是当前任期的Leader，避免向客户端发送过期数据。如果对数据正确性和时效性的敏感性不高，就可以向系统中的任意节点发送请求，&lt;/p>
&lt;h2 id="写在后面">写在后面 &lt;a href="#%e5%86%99%e5%9c%a8%e5%90%8e%e9%9d%a2" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>Paxos的亮点在于把复杂的东西说简单了。而Raft的亮点是把简单的东西具体化，使之成为能落地的工程项目。&lt;/p>
&lt;p>但是Paxos做为一种Quorum协议（俗称P2P），其工程复杂性是难以避免的。Raft在Paxos协议上面加入了很多限制以简化实现，但是想要完整的实现其功能仍不是一件容易的事。（有兴趣的同学可以&lt;a href="https://pdos.csail.mit.edu/6.824/labs/lab-raft.html">挑战一下自己&lt;/a>）&lt;/p>
&lt;p>下面一篇文章我们会换一种新思路，学习PecificA算法，看一看如何使用Paxos实现一个主从复制协议。&lt;/p>
&lt;h2 id="写在更后面">写在更后面 &lt;a href="#%e5%86%99%e5%9c%a8%e6%9b%b4%e5%90%8e%e9%9d%a2" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>个人觉得，Raft论文的一个更大亮点是充分的思考了Paxos/Multi-Paxos在工程实践上的缺陷。思考并提出有价值的质疑，而不是被他人的观点带着走，真的是最重要的能力之一了。&lt;/p>
&lt;p>大胆猜测是因为有足够的理解能力，有额外的带宽去发问？这是一个有意思的问题。&lt;/p></description></item><item><title>白话一致性协议 - Paxos、Raft和PacificA[0]</title><link>https://wizmann.top/posts/paxos-raft-pecifaca0/</link><pubDate>Sun, 25 Nov 2018 00:00:00 +0000</pubDate><guid>https://wizmann.top/posts/paxos-raft-pecifaca0/</guid><description>&lt;h2 id="一致性协议---paxos">一致性协议 - Paxos &lt;a href="#%e4%b8%80%e8%87%b4%e6%80%a7%e5%8d%8f%e8%ae%ae---paxos" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>在分布式系统当中，我们往往需要保持节点之间的一致性。在绝大多数情况下，我们需要让系统中的节点相互协调通力合作，有可能的让系统正确的工作。但是，由于分布式系统本身的特性，需要我们在不可靠的硬件上尽可能的构建可靠的系统。所以，看似简单的一致性问题成为了分布式系统领域的一个重要的课题。&lt;/p>
&lt;p>Paxos算法是Leslie Lamport于1990年提出的一种一致性算法。也是目前公认解决分布式一致性问题最有效的算法之一。&lt;/p>
&lt;p>Paxos算法的目标是在一个不可靠的分布式系统内，只通过网络通信，能够快速且正确地在集群内部对某个数据的值达成一致。并且保证不论发生任何异常，都不会破坏系统的一致性。&lt;/p>
&lt;h2 id="paxos算法">Paxos算法 &lt;a href="#paxos%e7%ae%97%e6%b3%95" class="anchor">🔗&lt;/a>&lt;/h2>&lt;h3 id="另一种简化了的形象的问题描述---买车位">另一种（简化了的）形象的问题描述 - 买车位 &lt;a href="#%e5%8f%a6%e4%b8%80%e7%a7%8d%e7%ae%80%e5%8c%96%e4%ba%86%e7%9a%84%e5%bd%a2%e8%b1%a1%e7%9a%84%e9%97%ae%e9%a2%98%e6%8f%8f%e8%bf%b0---%e4%b9%b0%e8%bd%a6%e4%bd%8d" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>某小区的一些居民在抢车位，而车位只有一个。居民们达成协议，只要一个报价获得半数以上居民认可，那么提出这个出价的居民则获得了车位的所有权。&lt;/p>
&lt;p>居民之间非常友善，如果知道了车位已经有了买家，就不会继续出价购买，并且会帮忙传播这个信息。&lt;/p>
&lt;p>居民们都是遵纪守法的好公民，在整个过程中都只会如实的与其它用户分享信息。信息的分享是在网上，通过一对一的私密聊天进行。但是小区的手机信号非常差，我们不能保证通信的质量，一些信息可能会丢失。但是居民不会掉线。也不会失去记忆，并且通信的内容是完整的，不会被篡改的。&lt;/p>
&lt;p>划重点：&lt;/p>
&lt;ol>
&lt;li>需要半数以上居民的认可，才能声称拥有车位。这个居民被称为车位的“公认拥有者”&lt;/li>
&lt;li>车位有且只有一个，所以一个车位不能同时有两个“公认拥有者”&lt;/li>
&lt;li>车位可以暂时没有拥有者，但是需要尽快选出一个&lt;/li>
&lt;li>通信是不可靠的(not-reliable)，但是正确性(integrity)和可持久性（durability）是可以保证的&lt;/li>
&lt;li>整个流程的目标是确定车位的拥有者。流程的参与者不会以自己拥有车位为最终目标&lt;/li>
&lt;/ol>
&lt;h3 id="如何选出最终的买家">如何选出最终的买家 &lt;a href="#%e5%a6%82%e4%bd%95%e9%80%89%e5%87%ba%e6%9c%80%e7%bb%88%e7%9a%84%e4%b9%b0%e5%ae%b6" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>由于通信是一对一的，对于所有参与者来说，他们对整个系统的了解都是片面的、过时的。但是参与者会通过与其它参与者进行通信，不断获得更及时的信息，从而最终达成一致。&lt;/p>
&lt;p>对于普通居民，会记录“已知最高报价”和“已确认报价”两个状态。会处理两种请求：&lt;/p>
&lt;ol>
&lt;li>询价：如果居民已确认了某个报价，则返回这个报价。否则会尝试更新已知最高报价，并报告买家当前的最高报价&lt;/li>
&lt;li>报价：居民会将请求中的报价与自己已知的最高报价进行比较，如果高于或等于本地已经报价，无论是否已经有已确认报价，都会确认这个报价&lt;/li>
&lt;/ol>
&lt;p>而对于买家，会发出两种不同的信息：&lt;/p>
&lt;ol>
&lt;li>询价：以某一个报价问询多数居民，如果有已确认的报价，则放弃自己的报价。如果这个报价低于居民已知的报价，则提高报价。&lt;/li>
&lt;li>报价：如果询价过程中收到了已确认的报价，则帮忙转发这个报价。否则发送自己的报价，如果报价获得了多数居民的认可，即可以认为所有权已经更新&lt;/li>
&lt;/ol>
&lt;h3 id="一致性的直观证明">一致性的直观证明 &lt;a href="#%e4%b8%80%e8%87%b4%e6%80%a7%e7%9a%84%e7%9b%b4%e8%a7%82%e8%af%81%e6%98%8e" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>报价阶段，除了保证正确性之外，对于居民和买家并没有任何约束。其本质参与者之间同步信息的过程。&lt;/p>
&lt;p>而一致性在报价阶段可以被很好的保证&lt;/p>
&lt;ul>
&lt;li>如果车位还没有主人，那么大家就拼一拼运气和手速，直到有一个买家获得了半数居民的认可。&lt;/li>
&lt;li>如果用户A已经声明以价格P买入车位（记为&lt;code>(P, A)&lt;/code>），此时必有多数居民已经认可&lt;code>(P, A)&lt;/code>。如果用户B想要声明以价格Q(Q &amp;gt;= P)买入车位，首先需要向多数居民询价，在询价的过程中，一定会收到用户A已经拥有车位的信息，此时B只会帮A扩散信息，而不会去争夺拥有权。系统最终会达到一个稳定的状态。&lt;/li>
&lt;/ul>
&lt;h3 id="失忆---放松可持久性要求">失忆 - 放松可持久性要求 &lt;a href="#%e5%a4%b1%e5%bf%86---%e6%94%be%e6%9d%be%e5%8f%af%e6%8c%81%e4%b9%85%e6%80%a7%e8%a6%81%e6%b1%82" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>如果我们放松可持久性的限制。即居民可以掉线，或者清空自己的记忆。那此时最差情况是多数已经确认报价的居民不再承认报价，那么系统就有可能退化到“仍没有人拥有车位”的状态。此时我们只需要再进行一轮选举，选出车位新的主人即可。这并不会破坏整个系统的一致性。&lt;/p>
&lt;h3 id="旁观者视角">旁观者视角 &lt;a href="#%e6%97%81%e8%a7%82%e8%80%85%e8%a7%86%e8%a7%92" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>假如我们做为旁观者，想要观察是谁最终拥有了车位。可以有以下两种方法：&lt;/p>
&lt;ol>
&lt;li>“推”模型&lt;/li>
&lt;/ol>
&lt;p>当新的买家被选举出来时，新买家会通知所有旁观者车位的拥有者发生了变化。这样观察者们可以实时的获得状态的变化，但是由于通信是不可靠的，旁观者可能会错过状态变化的信息。如果这种情况出现，观察者只能等待下次状态变化，才能更新自己的状态。&lt;/p>
&lt;ol start="2">
&lt;li>“拉”模型&lt;/li>
&lt;/ol>
&lt;p>旁观者可以假装自己是一个买家，向多数居民进行询价。如果最终买家已经确定，那么询价的响应中一定包含着最新的拥有者信息。&lt;/p>
&lt;h3 id="paxos如何解决活锁问题">Paxos如何解决活锁问题 &lt;a href="#paxos%e5%a6%82%e4%bd%95%e8%a7%a3%e5%86%b3%e6%b4%bb%e9%94%81%e9%97%ae%e9%a2%98" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>假设居民里有买家A、B，同时向其它居民提出询价/报价。如果他们的询价/报价顺序排列如下，会出现什么状况呢？&lt;/p>
&lt;pre tabindex="0">&lt;code>A: 询价，1块
众居民：好的，最高价为1块
B：加钱，2块
众居民：收到，最高价为2块
（此时A仍旧认为1块是最高报价）
A：最终报价，1块
众居民：不行不行，B已经加到两块钱了
A：（内心mmp）询价，3块
（B对A提高了报价也毫无准备）
B：最终报价，2块
众居民：滚粗，A已经报价3块了
B：我加钱
A：我再加
B：我加钱
A：我再加
众居民：。。。（你们玩个球啊）
&lt;/code>&lt;/pre>&lt;p>震惊，这帮无聊的人居然为了这几块钱可以玩一天！&lt;/p>
&lt;p>以这样的流程进行下去，系统会陷入活锁，几乎不能达成一致了。想要解决这个问题，可以将A和B的询价重试时间加入随机化因子，这样可以帮助更快的让居民们达成一致。&lt;/p>
&lt;h3 id="paxos如何解决投票分裂问题">Paxos如何解决投票分裂问题 &lt;a href="#paxos%e5%a6%82%e4%bd%95%e8%a7%a3%e5%86%b3%e6%8a%95%e7%a5%a8%e5%88%86%e8%a3%82%e9%97%ae%e9%a2%98" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>&lt;img src="https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/18-11-25/split-vots.png" alt="">&lt;/p>
&lt;p>如图所示，居民们的投票可能会发生分裂，即没有一个值达到了半数。这里的解决方案是让买家重新进行询价，同时加入随机化因子，使得投票达成半数以上的概率更大。&lt;/p>
&lt;h2 id="说正经的">说正经的 &lt;a href="#%e8%af%b4%e6%ad%a3%e7%bb%8f%e7%9a%84" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>对于Paxos算法的官方描述和正确性的详细证明可以参考&lt;a href="https://lamport.azurewebsites.net/pubs/paxos-simple.pdf">原论文&lt;/a>。这里就不搬运了，只是把我的例子和官方通用术语进行一下讲解，避免把大家带跑偏了。&lt;/p>
&lt;p>在上面的例子中，“车位”的通用术语被称为“共识”，整个系统中最多有一个共识。而“询价请求”被称为“prepare请求”，“报价请求”被称为“accept请求”。&lt;/p>
&lt;p>每个请求的价格被称为“编号”，编号大的请求可以覆盖编号小的请求，和“价高者得”是一个道理。请求中所带的信息被称为“value”，在我们的例子中，代表着购买者的身份信息。&lt;/p>
&lt;h2 id="basic-paxos和multi-paxos">Basic Paxos和Multi Paxos &lt;a href="#basic-paxos%e5%92%8cmulti-paxos" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>上面我们描述的Paxos算法又被称为Basic Paxos，因为每一轮流程执行完，所有的参与者都会（且只会）达成一个共识。而在实际的应用中，我们需要连续不断的对多个值达成共识。这时Basic Paxos算法就力不能及了，一来每一个值的共识都至少需要两次广播网络请求，性能太低，二来同时存在的多个提案会互相竞争，使得通信的效率下降。&lt;/p>
&lt;p>所以为了解决这个问题，Multi Paxos算法应运而生，即一种可以高效的、连续不断的对多个值达成共识的算法。&lt;/p>
&lt;p>下一篇文章中，我们会介绍一种被普遍认可，以及已经被工业界应用的Multi Paxos的实现 —— Raft算法。&lt;/p>
&lt;h2 id="参考链接">参考链接 &lt;a href="#%e5%8f%82%e8%80%83%e9%93%be%e6%8e%a5" class="anchor">🔗&lt;/a>&lt;/h2>&lt;ul>
&lt;li>&lt;a href="https://lamport.azurewebsites.net/pubs/paxos-simple.pdf">Paxos Made Simple&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://book.douban.com/subject/26292004/">从Paxos到Zookeeper&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://zhuanlan.zhihu.com/p/31780743">知乎：Paxos算法详解&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>