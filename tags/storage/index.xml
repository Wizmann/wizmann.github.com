<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Storage on Maerlyn's Rainbow</title><link>https://wizmann.top/tags/storage/</link><description>Recent content in Storage on Maerlyn's Rainbow</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Sun, 15 Aug 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://wizmann.top/tags/storage/index.xml" rel="self" type="application/rss+xml"/><item><title>论文阅读-WiscKey：SSD友好的KV分离存储引擎</title><link>https://wizmann.top/posts/paper-wisckey/</link><pubDate>Sun, 15 Aug 2021 00:00:00 +0000</pubDate><guid>https://wizmann.top/posts/paper-wisckey/</guid><description>&lt;h2 id="背景">背景 &lt;a href="#%e8%83%8c%e6%99%af" class="anchor">🔗&lt;/a>&lt;/h2>&lt;h3 id="基于lsm-tree的存储引擎">基于LSM-Tree的存储引擎 &lt;a href="#%e5%9f%ba%e4%ba%8elsm-tree%e7%9a%84%e5%ad%98%e5%82%a8%e5%bc%95%e6%93%8e" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>Log-Structed Merge-Tree (a.k.a. LSM-Tree)是当下常用的一种基于磁盘的存储引擎。与Hash索引和B-Tree同为数据库核心的数据结构。&lt;/p>
&lt;p>LSM-Tree的优势在于：&lt;/p>
&lt;ol>
&lt;li>无需将所有的Key索引在内存中。可以通过分级查找的方式，查询到特定KV在磁盘中的偏移量&lt;/li>
&lt;li>数据写入与合并使用顺序追加写，最大程度的利用磁盘的顺序写性能&lt;/li>
&lt;li>对于数据写入，会使用batch方式写入磁盘&lt;/li>
&lt;li>支持范围查询&lt;/li>
&lt;/ol>
&lt;p>LSM-Tree的劣势在于：&lt;/p>
&lt;ol>
&lt;li>读放大与写放大&lt;/li>
&lt;li>无法对单条数据加锁（事务支持）&lt;/li>
&lt;/ol>
&lt;p>现在常用的LevelDB和RocksDB，都是基于LSM-Tree的存储引擎。&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/21-08-14/2021-08-14_18-00-50.png" alt="LSM-Tree-Architecture">&lt;/p>
&lt;h3 id="wisckey主要解决的问题">WiscKey主要解决的问题 &lt;a href="#wisckey%e4%b8%bb%e8%a6%81%e8%a7%a3%e5%86%b3%e7%9a%84%e9%97%ae%e9%a2%98" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>LSM-Tree在性能上面的瓶颈主要在于读写的放大。简单说来，假设你的磁盘最大带宽为4GB/s，读写放大倍数为10，那么在应用层的有效吞吐量（&lt;a href="https://en.wikipedia.org/wiki/Goodput">Goodput&lt;/a>）最多只能达到400MB/s。并且，对于一些大数据集，读写放大的值有可能会非常大（大于100）。&lt;/p>
&lt;p>那么是什么原因导致的读写放大呢？我们下面分开讨论。&lt;/p>
&lt;h4 id="读放大">读放大 &lt;a href="#%e8%af%bb%e6%94%be%e5%a4%a7" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>我们重温一下LSM-Tree的查询机制：&lt;/p>
&lt;ol>
&lt;li>首先在mutable memtable中进行查找&lt;/li>
&lt;li>然后在immutable memtable中进行查找&lt;/li>
&lt;li>最后在不同的LSM-Tree层级中的SST file中进行查找&lt;/li>
&lt;/ol>
&lt;p>对于1和2，都是内存操作，其耗时与读取磁盘相比可以忽略不计。
对于3，SST file文件的查找会先使用二分查找定位特定KV位于该层的哪一个SST文件中。&lt;/p>
&lt;p>如果特定的KV不存在于此SST文件中，会被常驻内存的bloom filter过滤掉，不会真正的读取文件。（大概率，bloom filter的原理了解一下）&lt;/p>
&lt;p>如果特定的KV在此SST文件中，会通过index block进行二分查找，确定KV位于哪个block。然后这个block会被从磁盘上读取到内存进行解压，最后通过二分+遍历找到对应的KV。&lt;/p>
&lt;blockquote>
&lt;p>论文里对于读放大的计算方法有点夸大了。对于RocksDB来说，一般情况下index block和bloom filter都是常驻内存的。除非是内存非常紧张的场景，否则并不会产生论文中如此夸张的磁盘读放大的。&lt;/p>&lt;/blockquote>
&lt;p>所以读放大主要因为读取一个KV，需要读取+解压整个block。当KV大小远小于block size的时候，问题会更加严重。&lt;/p>
&lt;h4 id="写放大">写放大 &lt;a href="#%e5%86%99%e6%94%be%e5%a4%a7" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>我们再重温一个LSM-Tree的写入机制：&lt;/p>
&lt;ol>
&lt;li>首先写入mutable memtable&lt;/li>
&lt;li>mutable memtable后续会被sealed，成为immutable memtable&lt;/li>
&lt;li>immutable memtable会被flush到磁盘，成为L0&lt;/li>
&lt;li>L0中的数据会在后台，逐层向下合并，直到合并到最底层&lt;/li>
&lt;/ol>
&lt;p>层次数据逐层向下合并的过程中，必然会产生写放大。（具体原理这里不展开了）&lt;/p>
&lt;p>写放大值可以使用以下方法估算。已知&lt;code>options.max_bytes_for_level_multiplier&lt;/code>代表每一层大小的倍数k，并且假设L0与L1的大小相等，总共有N层。那么写放大约为&lt;code>(N - 1) * k + 1&lt;/code>。&lt;/p>
&lt;h2 id="正文">正文 &lt;a href="#%e6%ad%a3%e6%96%87" class="anchor">🔗&lt;/a>&lt;/h2>&lt;blockquote>
&lt;p>入活了入活了！&lt;/p>&lt;/blockquote>
&lt;h3 id="wisckey的设计目标">WiscKey的设计目标 &lt;a href="#wisckey%e7%9a%84%e8%ae%be%e8%ae%a1%e7%9b%ae%e6%a0%87" class="anchor">🔗&lt;/a>&lt;/h3>&lt;ul>
&lt;li>降低写放大&lt;/li>
&lt;li>降低读放大&lt;/li>
&lt;li>对SSD进行优化&lt;/li>
&lt;li>提供LSM-Tree兼容的API&lt;/li>
&lt;li>针对现实场景的KV size&lt;/li>
&lt;/ul>
&lt;h3 id="wisckey的设计思路kv分离">WiscKey的设计思路——KV分离 &lt;a href="#wisckey%e7%9a%84%e8%ae%be%e8%ae%a1%e6%80%9d%e8%b7%afkv%e5%88%86%e7%a6%bb" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>在一般的场景中，在一对KV中，Key的大小远比Value的要小。如果我们在LSM-Tree中只保留Key，那么对于一次读写，我们所要处理的数据就会少很多，从而降低了读写放大。&lt;/p>
&lt;p>对于Value，我们会将其存储在另外一个数据结构vLog中。每一次查询，会在LSM-Tree中记录其在vLog中的&lt;code>&amp;lt;offset, length&amp;gt;&lt;/code> 。这样一来，读取Value的操作也只有一次磁盘访问，并且不会产生写放大。&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/21-08-14/2021-08-14_23-08-54.png" alt="WiscKey-data-layout">&lt;/p>
&lt;blockquote>
&lt;p>这里有一个疑问，如果按论文里面的设计，vLog里面的数据是不会打包的block进行压缩的，必然会损失capacity。如果打包压缩了，那么读放大并没有显著优化。又如果value size ≈ block size，那么其实在读放大上面的优化也就比较微弱了。&lt;/p>&lt;/blockquote>
&lt;h3 id="wisckey的设计挑战">WiscKey的设计挑战 &lt;a href="#wisckey%e7%9a%84%e8%ae%be%e8%ae%a1%e6%8c%91%e6%88%98" class="anchor">🔗&lt;/a>&lt;/h3>&lt;h4 id="并发范围查找">并发范围查找 &lt;a href="#%e5%b9%b6%e5%8f%91%e8%8c%83%e5%9b%b4%e6%9f%a5%e6%89%be" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>因为使用了KV分离的存储方式，所以范围查找（range query）对于value就会退化成很多次随机读取。&lt;/p>
&lt;p>但是由于SSD的并发读取特性，WiscKey会将所需要读取的value的&lt;code>&amp;lt;offset, length&amp;gt;&lt;/code>放到一个队列中，多线程并发读取所需要的value。&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/21-08-14/2021-08-14_23-42-44.png" alt="seq-rand-reads-on-ssd">&lt;/p>
&lt;p>如图所示，对于较大的value size，顺序读与并发读的throughput基本保持一致。&lt;/p>
&lt;p>但是多线程并发读会消耗更多的CPU。&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/21-08-15/2021-08-15_16-02-21.png" alt="CPU-usage">&lt;/p>
&lt;blockquote>
&lt;p>不过对于一个存储系统来说，CPU大概是最不重要的资源了吧（见仁见智咯）&lt;/p>&lt;/blockquote>
&lt;h4 id="垃圾回收">垃圾回收 &lt;a href="#%e5%9e%83%e5%9c%be%e5%9b%9e%e6%94%b6" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>对于重复写入的KV，LSM-Tree会通过层次合并来将旧值换出。而WiscKey会使用垃圾回收机制来将旧值换出。&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/21-08-15/2021-08-15_00-11-06.png" alt="vlog-data-layout">&lt;/p>
&lt;p>vLog可以视作一个循环数组，head指针表示数据的开头（较新的数据），tail指针表示数据的结尾（较旧的数据）。在head和tail指针之外的空间都视作free space。&lt;/p>
&lt;p>当空闲数据小于一定阈值之后，会触发GC。WiscKey会从tail依次读出一部分旧数据（batch read），查询LSM-Tree，如果这条数据已经被覆盖或删除，那么就丢弃此数据；否则从head指针处写入此条数据。&lt;/p>
&lt;p>为了保证在GC过程中的crash不产生数据的丢失，GC的流程为：&lt;/p>
&lt;ol>
&lt;li>将tail处的KV写入到head处，并调用&lt;code>fsync()&lt;/code>将其持久化&lt;/li>
&lt;li>同步的将KV的新位置，和head/tail指针的最新位置写入LSM-Tree&lt;/li>
&lt;/ol>
&lt;p>由于我们使用了LSM-Tree的WAL，可以保证所有在LSM-Tree里的数据都是可用的。&lt;/p>
&lt;h4 id="崩溃一致性crash-consistency">崩溃一致性（Crash Consistency） &lt;a href="#%e5%b4%a9%e6%ba%83%e4%b8%80%e8%87%b4%e6%80%a7crash-consistency" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>在垃圾回收一节中，我们其实已经遇到了一致性的问题。其根本原因，在于LSM-Tree和vLog是两个不同的模块，在其间保持一致性必然是困难的。&lt;/p>
&lt;p>WiscKey使用了现代文件系统（ext4，btrfs和xfs）的一个有趣特性，即对于一个（将要）写入文件的数据流，文件系统会保证这个数据流的一个前缀（或者整个数据流）成功写入文件。&lt;/p>
&lt;h3 id="wisckey的优化">WiscKey的优化 &lt;a href="#wisckey%e7%9a%84%e4%bc%98%e5%8c%96" class="anchor">🔗&lt;/a>&lt;/h3>&lt;h4 id="vlog写入buffer">vLog写入Buffer &lt;a href="#vlog%e5%86%99%e5%85%a5buffer" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>&lt;img src="https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/21-08-15/2021-08-15_12-20-02.png" alt="impact-of-write-unit-size">&lt;/p>
&lt;p>众所周知，磁盘的同步写入必然会带来一些overhead。对于小数据来说，这在性能上会产生严重的损耗。&lt;/p>
&lt;p>这里的优化类似于将vLog也添加一个memory table，所有写入vLog的数据都先写入这个memory table，然后依次flush到磁盘上。&lt;/p>
&lt;p>这样的优势在于将写入batch化，可以减少overhead，提升性能。劣势在于，在系统崩溃时，可能会丢失部分数据。但丢失数据时，仍能抱证上文提到的崩溃一致性。&lt;/p>
&lt;h4 id="优化lsm-tree-wal">优化LSM-Tree WAL &lt;a href="#%e4%bc%98%e5%8c%96lsm-tree-wal" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>LSM-Tree的WAL虽然保证了写入的数据的持久性，但是不可避免的，它也会消耗磁盘的一部分带宽。&lt;/p>
&lt;p>关闭WAL，意味着LSM-Tree memtable在进程重启时一定会丢失数据。不过我们可以利用vLog重建LSM-Tree（的memtable部分）。&lt;/p>
&lt;p>由于vLog之中已经包括了所有的KV对，所以在WiscKey重启时，我们可以从head指针处扫描vLog的数据，由于崩溃一致性的保证，当指针扫描到已经存在于LSM-Tree的数据时，可以认为LSM-Tree已经重建成功。&lt;/p>
&lt;h4 id="文件系统的优化">文件系统的优化 &lt;a href="#%e6%96%87%e4%bb%b6%e7%b3%bb%e7%bb%9f%e7%9a%84%e4%bc%98%e5%8c%96" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>WiscKey使用&lt;code>posix_fadvice()&lt;/code>向OS层预声明接下来vLog的操作类型以适应不同场景的磁盘访问模式。使用&lt;code>fallocate()&lt;/code>，通过&amp;quot;hole-punching&amp;quot;功能进行GC，以减少数据的移动。&lt;/p>
&lt;h3 id="wisckey的性能">WiscKey的性能 &lt;a href="#wisckey%e7%9a%84%e6%80%a7%e8%83%bd" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>论文的最后给出了一系列数字。由于WiscKey的读写放大很小，所以在value size比较大的场景下（&amp;gt;= 1KB），性能远优于传统LSM-Tree。同时，磁盘空间的使用也更友好。&lt;/p>
&lt;blockquote>
&lt;p>这里有一个疑问，论文里做microbenchmark的时候，对于LevelDB和RocksDB，是不是还开着压缩。。。&lt;/p>&lt;/blockquote>
&lt;h2 id="后记">后记 &lt;a href="#%e5%90%8e%e8%ae%b0" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>WiscKey应该是“LSM-Tree + KV分离”思路的比较早期的论文，其中仍有很多不明确的细节。但是整体思路是可行的。&lt;/p>
&lt;p>PingCAP的&lt;a href="https://pingcap.com/zh/blog/titan-design-and-implementation">Titan&lt;/a>是一个基于WiscKey思想的RocksDB KV分离存储引擎。后续可以参考其实现再了解更多的实现思路与细节。&lt;/p></description></item><item><title>Introduction to Ceph</title><link>https://wizmann.top/posts/introduction-to-ceph/</link><pubDate>Sun, 29 Nov 2020 00:00:00 +0000</pubDate><guid>https://wizmann.top/posts/introduction-to-ceph/</guid><description>&lt;h2 id="什么是ceph">什么是Ceph &lt;a href="#%e4%bb%80%e4%b9%88%e6%98%afceph" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>Ceph是一个可扩展的，高性能的分布式存储系统。提供了三种不同类型的接口以适应不同的应用场景：&lt;/p>
&lt;ul>
&lt;li>block-based: 块存储，可以用做VM的虚拟磁盘&lt;/li>
&lt;li>object-based: 对象存储，与Amazon S3等常用对象存储兼容&lt;/li>
&lt;li>file system: POSIX兼容的分布式文件系统，可以被本地系统挂载，并且能被多个客户端共享&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/20-11-28/2020-11-28_22-04-49.png" alt="">&lt;/p>
&lt;h3 id="ceph的特性">Ceph的特性 &lt;a href="#ceph%e7%9a%84%e7%89%b9%e6%80%a7" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>由于采用了CRUSH算法，Ceph有着优异的可扩展性（宣称可以无限扩展）。并且借助可扩展性，进而实现高性能、高可靠性和高可用性。&lt;/p>
&lt;p>Ceph是一个去中心化的存储系统，无需中心节点进行资源的管理与调度，全部的管理功能由存储节点自治完成。使得整个系统可以自我管理与自我恢复，减少运维成本与管理成本。&lt;/p>
&lt;h2 id="rados---ceph的存储引擎">RADOS - Ceph的存储引擎 &lt;a href="#rados---ceph%e7%9a%84%e5%ad%98%e5%82%a8%e5%bc%95%e6%93%8e" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>RADOS=Reliable Autonomic Distributed Object Store。RADOS是Ceph底层的存储引擎，所有的接口都建立在RADOS的功能之上。&lt;/p>
&lt;h3 id="rados中的存储结构">RADOS中的存储结构 &lt;a href="#rados%e4%b8%ad%e7%9a%84%e5%ad%98%e5%82%a8%e7%bb%93%e6%9e%84" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>&lt;img src="https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/20-11-28/2020-11-28_22-24-56.png" alt="">&lt;/p>
&lt;ul>
&lt;li>存储池（pool）：逻辑层，每一个pool里都包含一些放置组&lt;/li>
&lt;li>放置组（placement-group, PG)：逻辑层，一份数据会在PG当中进行灾备复制。每一个PG都对应着一系列的存储节点&lt;/li>
&lt;li>存储节点（OSD）：用以存储数据的物理节点。与PG之间形成多对多的关系。&lt;/li>
&lt;/ul>
&lt;p>一份数据在写入RADOS时，会先选中一个pool。Pool中再使用一定的hash规则，伪随机的选中某一个PG。PG会将数据写入多个OSD中。读取数据时，也是类似的规则。&lt;/p>
&lt;p>Pool是用户可见的管理数据的基本单位，用户可以对Pool进行一系列的配置（权限控制、使用SSD or HDD、使用数据拷贝 or 纠删码，etc.）。而PG与OSD对于用户是不可见的。&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/20-11-28/2020-11-28_23-00-51.png" alt="">&lt;/p>
&lt;h4 id="pg的组织">PG的组织 &lt;a href="#pg%e7%9a%84%e7%bb%84%e7%bb%87" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>在一致性哈希中，我们使用节点来划分哈希值域。这种方法的问题是，如果产生了数据不平衡，我们需要重新进行划分值域来进行再平衡。这会造成大量的数据迁移。&lt;/p>
&lt;p>CRUSH采用了虚拟节点（也就是PG）将哈希值域划分成了固定的等长区域。这种方法在单条数据与物理节点之间加入了一个虚拟层。之后，再使用哈希取模的算法确定数据属于哪个PG。使得数据的迁移是以虚拟节点为单位，而不是对每一条数据都重新计算。Ceph官方的建议是，每1个OSD对应着100个PG。&lt;/p>
&lt;p>一般情况下，在规划的初期需要确定PG的数目，如果后期需要调整PG，有可能会导致大量的数据迁移，甚至需要服务暂时停止服务。&lt;/p>
&lt;h3 id="监控子集群mon与cluster-map">监控子集群（MON）与Cluster Map &lt;a href="#%e7%9b%91%e6%8e%a7%e5%ad%90%e9%9b%86%e7%be%a4mon%e4%b8%8ecluster-map" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>&lt;img src="https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/20-11-28/2020-11-28_22-48-10.png" alt="">&lt;/p>
&lt;p>RADOS集群中除了OSD存储节点之外，还有监控子集群（MON），用于存储系统的拓扑结构——Cluster Map。&lt;/p>
&lt;blockquote>
&lt;p>元数组管理（MDS）节点用于管理CephFS中的文件元信息，后文会有介绍。&lt;/p>&lt;/blockquote>
&lt;p>不同于传统的中心化管理节点，MON并不会对资源进行调配与调度，而仅仅是一个观测者，用以存储系统当前的拓扑与状态。&lt;/p>
&lt;p>MON与OSD、OSD与OSD之间会定时发送心跳包，检测OSD是否健康。如果某个节点失效，MON会更新内部存储的拓扑结构信息（ClusterMap），并且通过P2P协议广播出去，从而使得整个系统都有着（最终）一致的拓扑信息。&lt;/p>
&lt;h3 id="主从同步与节点自治">主从同步与节点自治 &lt;a href="#%e4%b8%bb%e4%bb%8e%e5%90%8c%e6%ad%a5%e4%b8%8e%e8%8a%82%e7%82%b9%e8%87%aa%e6%b2%bb" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>在一个PG中，会有一个主节点（Primary）和一个或多个从节点（Secondary）。主节点负责维护从节点的状态，包括数据复制（replication）、失效检测（failure detection）和失效恢复（failure recovery）。&lt;/p>
&lt;h2 id="crush---ceph皇冠上的明珠">CRUSH - Ceph皇冠上的明珠 &lt;a href="#crush---ceph%e7%9a%87%e5%86%a0%e4%b8%8a%e7%9a%84%e6%98%8e%e7%8f%a0" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>CRUSH是一个可扩展的，伪随机的数据放置算法。以去中心化方法，将PG按规则映射到相应的存储设备上。并且系统的拓扑结构发生变化时，尽可能的减少数据的迁移。&lt;/p>
&lt;h3 id="crush的优势">CRUSH的优势 &lt;a href="#crush%e7%9a%84%e4%bc%98%e5%8a%bf" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>CRUSH对于每一个数据元素使用一个伪随机算法，决定其放置的位置。所以，只要所有参与者都拥有相同的系统拓扑结构信息，那么数据的位置就是一定的。所以我们可以去掉中心节点，采用P2P的方法来进行数据的存储与检索。&lt;/p>
&lt;p>并且，由于伪随机算法只与单个PG相关，如果我们操作得当，节点数量的变化并不会引起大量的数据迁移，而是会接近理论最优值。&lt;/p>
&lt;h3 id="数据放置data-placement">数据放置（Data Placement） &lt;a href="#%e6%95%b0%e6%8d%ae%e6%94%be%e7%bd%aedata-placement" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>CRUSH的数据放置算法有很多不同的实现，这里只介绍最常用的straw算法。&lt;/p>
&lt;p>Ceph中，每一个OSD节点都有一个权值w，代表着某个节点能支持多少数据的存储与检索。一般来说，权值与节点的容量成正比。&lt;/p>
&lt;p>假设一个pool里面有n个PG，在一条新的数据写入时，我们分别会计算这n个PG的length值。&lt;/p>
&lt;p>$$ length_{i} = f(w_{i}) * hash(x) $$&lt;/p>
&lt;p>$f(w_{i})$是一个只于当前OSD节点权值有关的函数。$hash(x)$代表当前PG的哈希值。所以，PG会放置在哪个OSD上，仅与其权值相关。&lt;/p>
&lt;p>假设某个OSD节点发生变化时（新加、删除、权值变化），在此受影响节点的数据会迁移到其它的OSD节点。其它OSD节点的原有数据并不会受到影响。&lt;/p>
&lt;blockquote>
&lt;p>Ceph当中的straw算法有&lt;code>straw1&lt;/code>和&lt;code>straw2&lt;/code>。&lt;code>straw1&lt;/code>的实现采用了有缺陷f(w)函数，会导致意外的数据迁移。&lt;code>straw2&lt;/code>解决了这个问题。
详情请戳&lt;a href="https://www.spinics.net/lists/ceph-devel/msg21635.html">这里&lt;/a>&lt;/p>&lt;/blockquote>
&lt;h4 id="主从架构">主从架构 &lt;a href="#%e4%b8%bb%e4%bb%8e%e6%9e%b6%e6%9e%84" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>每个PG所包含的OSD都由CRUSH算法计算得出，并根据配置选出前r个OSD进行主从配对。列表中的第1个OSD做为主节点（Primary），其它的节点为从节点（Secondary）。&lt;/p>
&lt;p>主从节点的分配与管理由PG内部进行自治，不需要额外的外部系统进行管理。&lt;/p>
&lt;h2 id="cephfs">CephFS &lt;a href="#cephfs" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>CephFS是一个POSIX兼容的（共享）文件系统。CephFS利用文件元数据子系统（MDS）来维护目录树结构和文件和目录的元信息（owner, timestamps, inodes, etc.)等。&lt;/p>
&lt;p>MDS子系统会在内存里面维护一份Cache，对于需要持久化的信息，会使用WAL的方式写入RADOS里一个专用的Pool当中。&lt;/p>
&lt;h3 id="动态树划分dynamic-tree-partitioningdtp">动态树划分（Dynamic Tree Partitioning，DTP） &lt;a href="#%e5%8a%a8%e6%80%81%e6%a0%91%e5%88%92%e5%88%86dynamic-tree-partitioningdtp" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>CephFS的扩展性的关键之一，在于元信息子系统的扩展性。CephFS实现了动态树划分的算法，将目录树结构根据当前系统的负载，将其划分到不同的MDS节点上去。&lt;/p>
&lt;p>维护目录树结构的优势在于利用了文件系统的局部性（locality），可以方便的进行预取（prefetch）。动态的树划分，可以保证元信息可以线性增长，以保持高可扩展性。&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/Wizmann/assets/master/wizmann-pic/20-11-29/2020-11-29_18-53-09.png" alt="">&lt;/p>
&lt;h2 id="写在最后">写在最后 &lt;a href="#%e5%86%99%e5%9c%a8%e6%9c%80%e5%90%8e" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>本文基于Ceph的三篇论文综合而成（CephFS、RADOS、CRUSH）。其中加入了一些自己的看法，使其逻辑通顺，并不保证与论文的思路完全一致。&lt;/p>
&lt;p>这三篇论文并没有明确的依赖关系，换句话说，需要综合阅读，才能有比较明确的理解。&lt;/p>
&lt;p>建议在通读论文后，去学习一下&lt;a href="https://www.youtube.com/watch?v=PmLPbrf-x9g&amp;amp;ab_channel=Ceph">这个视频&lt;/a>，会对理解Ceph有很大的帮助。Youtube上面还有很多Ceph的tech talk，可以一并的了解一下。&lt;/p>
&lt;p>Ceph相关的书籍以实践居多，只推荐&lt;a href="https://www.oreilly.com/library/view/learning-ceph-/9781787127913/">Learning Ceph&lt;/a>。&lt;/p>
&lt;h2 id="参考链接">参考链接 &lt;a href="#%e5%8f%82%e8%80%83%e9%93%be%e6%8e%a5" class="anchor">🔗&lt;/a>&lt;/h2>&lt;ul>
&lt;li>&lt;a href="https://crossoverjie.top/2018/01/08/Consistent-Hash/">一致性 Hash 算法分析&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://zhuanlan.zhihu.com/p/60963885">从一致性 hash 到 ceph crush&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://ceph.io/publications/">Ceph Publications&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Bw-Tree：在新硬件平台上的新B-Tree</title><link>https://wizmann.top/posts/bw-tree/</link><pubDate>Mon, 18 Dec 2017 00:49:00 +0000</pubDate><guid>https://wizmann.top/posts/bw-tree/</guid><description>&lt;h2 id="ars-与-bw-tree">ARS 与 Bw-Tree &lt;a href="#ars-%e4%b8%8e-bw-tree" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>nosql数据库从本质上说，都属于ARS（Atomic Record Stores，“原子记录存储”）。&lt;/p>
&lt;p>最常见的“原子记录存储”一种实现就是朴素的Hash表：通过一个特定的key，来读写一条独立的数据记录。&lt;/p>
&lt;p>一些基于key-value（键-值）模型的nosql的内部实现正是使用了Hash表，例如Redis&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>、memcache、Riak等。&lt;/p>
&lt;p>而有一些nosql数据库为了支持高效的key-sequential（键-序列）查找，采用了tree-based&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>数据结构进行数据存储，所以B-Tree成为了一些数据库（both nosql and sql）的首选。&lt;/p>
&lt;p>本文介绍了一种基于新型硬件的高性能ARS实现，其核心技术有：&lt;/p>
&lt;ol>
&lt;li>页式内存管理&lt;/li>
&lt;li>Bw-Tree，一种基于B-Tree的树存储结构&lt;/li>
&lt;li>基于日志的存储管理&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-8/19953353.jpg" alt="">&lt;/p>
&lt;h2 id="在现代硬件上的bw-tree">在现代硬件上的Bw-Tree &lt;a href="#%e5%9c%a8%e7%8e%b0%e4%bb%a3%e7%a1%ac%e4%bb%b6%e4%b8%8a%e7%9a%84bw-tree" class="anchor">🔗&lt;/a>&lt;/h2>&lt;h3 id="现代多核cpu">现代多核CPU &lt;a href="#%e7%8e%b0%e4%bb%a3%e5%a4%9a%e6%a0%b8cpu" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>多核CPU带来了高并发，但是同时也引入了锁竞争和缓存失效问题。&lt;/p>
&lt;p>为了讲解锁竞争，这里我们举一个经典的例子：多线程计数器。&lt;/p>
&lt;p>我们都知道，如果不使用锁来保护计数器变量的话，那么最终的结果几乎不可能是正确的。多个线程中，只能有唯一一个线程可以持有锁，其它的线程都需要等待。这样就影响到了程序的并发度。&lt;/p>
&lt;blockquote>
&lt;p>这里一些高级玩家会提到CAS（compare-and-swap）命令。CAS翻译成中文就是“比较并交换”，用在多线程编程中实现数据交换操作。详情请见：&lt;a href="https://en.wikipedia.org/wiki/Compare-and-swap">比较并交换&lt;/a>&lt;/p>&lt;/blockquote>
&lt;p>缓存失效产生于两种场景。一是上文提到的锁争用，即每次锁争用都会让当前线程陷入内核态，产生上下文切换，导致CPU缓存的失效。二是缓存一致性协议导致的缓存行失效，即多个CPU在共享一个共同的主存资源时，为了保证缓存中的数据一致性，在主存资源被修改后，相应的缓存行也会失效。&lt;/p>
&lt;p>Bw-Tree为了解决以上的问题，使用了以下两个措施。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>无锁数据结构 &lt;br>
避免了系统阻塞在锁上，更避免了由于锁争用导致的缓存失效以及上下文切换。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>增量更新 &lt;br>
使用增量更新而不是原地更新，避免修改共享的主存资源，规避缓存一致性产生的缓存失效问题。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="现代存储设备">现代存储设备 &lt;a href="#%e7%8e%b0%e4%bb%a3%e5%ad%98%e5%82%a8%e8%ae%be%e5%a4%87" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>数据库系统的一个重要瓶颈就是IO性能。SSD（固态硬盘）提供了比传统机械 硬盘更好的读写性能，虽然表面上缓解了问题，但是IO仍然可能是整个系统的瓶颈。&lt;/p>
&lt;p>所以，Bw-Tree使用大内存做为读缓存，同时采用append-only结构保存写信息。因为不论是SSD还是机械硬盘，顺序读写的性能都非常出色。&lt;/p>
&lt;h2 id="bw-tree的页式存储结构">Bw-Tree的页式存储结构 &lt;a href="#bw-tree%e7%9a%84%e9%a1%b5%e5%bc%8f%e5%ad%98%e5%82%a8%e7%bb%93%e6%9e%84" class="anchor">🔗&lt;/a>&lt;/h2>&lt;h3 id="映射表mapping-table">映射表（Mapping Table） &lt;a href="#%e6%98%a0%e5%b0%84%e8%a1%a8mapping-table" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>上文中我们提到了缓存一致性（cache coherance），简单来说，就是当我们修改多个CPU共享的内存资源时，会导致缓存的失效，进而影响指令的执行效率。&lt;/p>
&lt;p>Bw-Tree使用页式（paginated）存储管理与映射表解决了这个问题。Bw-Tree以“页”做为存储的单元，每一页都有一个PID。页的物理位置可以存储在主存上，也可以存储在磁盘上。&lt;/p>
&lt;p>映射表将PID所代表的映射到页所在的具体物理位置。所以当页的物理位置发生改变时，仍然保持PID一致，不需要对Bw-Tree的结构进行任何修改。&lt;/p>
&lt;h3 id="增量修改delta-updating">增量修改（Delta Updating） &lt;a href="#%e5%a2%9e%e9%87%8f%e4%bf%ae%e6%94%b9delta-updating" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-10/18893113.jpg" alt="">&lt;/p>
&lt;p>假设我们现在有数据&lt;code>D&lt;/code>，当有修改请求&lt;code>delta&lt;/code>到来时，我们可以将数据&lt;code>D&lt;/code>&lt;strong>原地&lt;/strong>修改为&lt;code>D'&lt;/code>。也可以使用增量更新的方法将其保存为&lt;code>delta + D&lt;/code>的形式。&lt;/p>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-18/98762144.jpg" alt="">&lt;/p>
&lt;p>增量更新看起来比原地更新耗费了更多的内存，但其优势也非常明显。一是修改不需要对页加锁，可以使用CAS无锁方法安全的更新共享资源。二是不需要修改页内存，不会导致现有cache失效。&lt;/p>
&lt;p>当增量过多时，Bw-Tree会自动合并增量和页内存，这个过程同样是无锁的。&lt;/p>
&lt;h3 id="弹性虚拟页elastic-virtual-pages">弹性虚拟页（Elastic Virtual Pages） &lt;a href="#%e5%bc%b9%e6%80%a7%e8%99%9a%e6%8b%9f%e9%a1%b5elastic-virtual-pages" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-10/91449803.jpg" alt="image">&lt;/p>
&lt;blockquote>
&lt;p>B+-Tree，图片来源：&lt;a href="https://www.cs.usfca.edu/~galles/visualization/BPlusTree.html">B+ Trees Visualization&lt;/a>&lt;/p>&lt;/blockquote>
&lt;p>B+-Tree的特性有：&lt;/p>
&lt;ol>
&lt;li>所有记录都在叶子节点（节点也被称为页）上，并且是顺序存放的&lt;/li>
&lt;li>每个非叶子节点包含关键字（references）与孩子指针&lt;/li>
&lt;li>叶子节点有一个指向左右兄弟的指针，方便顺序遍历数据&lt;/li>
&lt;/ol>
&lt;p>Bw-Tree在B+-Tree的基础上添加了：&lt;/p>
&lt;ol>
&lt;li>low/high key，用来标明本节点以及子节点的数据范围&lt;/li>
&lt;li>每个节点（包括叶子节点和中间节点）都有一个side pointer指向其右面的兄弟节点&lt;/li>
&lt;/ol>
&lt;h3 id="范围查询">范围查询 &lt;a href="#%e8%8c%83%e5%9b%b4%e6%9f%a5%e8%af%a2" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>由于Bw-Tree的数据都分布在叶子节点，并且叶子节点之间可以通过兄弟指针进行遍历。&lt;/p>
&lt;p>我们使用游标（cursor）来标记当前检索的位置。并且使用缓存当前页上符合条件的记录。这样可以加速逐条遍历操作（&amp;ldquo;next-record&amp;rdquo; functionality）。&lt;/p>
&lt;p>逐条遍历操作是原子的，但是逐条遍历某一段数据则不是原子的。如果读写同时发生在同一页，则我们会根据事务的同步级别来决定是否需要重建检索缓存。&lt;/p>
&lt;h3 id="垃圾回收">垃圾回收 &lt;a href="#%e5%9e%83%e5%9c%be%e5%9b%9e%e6%94%b6" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>无锁数据结构导致的一个结果，是我们无法显式的让某个页失效。这意味着，一些线程会持有过时的数据，所以在删除这些过时的数据时要非常小心。&lt;/p>
&lt;p>所以这里引入了“代”（epoch）的概念，每一个线程执行每一个操作都在某一个代中，并且这一代中的所有数据以及过时数据都不会被删除。当某一代中所有线程操作都被执行完毕后，这一代就会结束，其中的过时数据就会被安全的回收。&lt;/p>
&lt;h2 id="bw-tree的结构改变">Bw-Tree的结构改变 &lt;a href="#bw-tree%e7%9a%84%e7%bb%93%e6%9e%84%e6%94%b9%e5%8f%98" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>Bw-Tree的结构会在某些操作下进行改变，以维护其性质。结构改变的操作仍然是无锁的。&lt;/p>
&lt;p>为了方便理解，我们假设结构改变的写操作不是并发的。&lt;/p>
&lt;h3 id="节点分裂">节点分裂 &lt;a href="#%e8%8a%82%e7%82%b9%e5%88%86%e8%a3%82" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>当某线程发现Bw-Tree中某个节点的大小超出了系统设定的阈值后，在执行完其预定的任务后，就会触发节点的分裂操作。&lt;/p>
&lt;p>节点分裂分为两步执行，每一步都是原子的：&lt;/p>
&lt;ol>
&lt;li>子节点分裂&lt;/li>
&lt;li>子节点更新后，向上更新父节点&lt;/li>
&lt;/ol>
&lt;h4 id="子节点分裂">子节点分裂 &lt;a href="#%e5%ad%90%e8%8a%82%e7%82%b9%e5%88%86%e8%a3%82" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-13/53138696.jpg" alt="">&lt;/p>
&lt;p>上图是Bw-Tree中的一部分，现在我们想分裂&lt;code>PQ&lt;/code>节点为&lt;code>P&lt;/code>和&lt;code>Q&lt;/code>两个子节点。即大于某个值&lt;code>KP&lt;/code>的数据全部迁移到新节点&lt;code>Q&lt;/code>。&lt;/p>
&lt;p>节点分裂的操作如下：&lt;/p>
&lt;ol>
&lt;li>先拷贝Q中的数据到一个新的数据页&lt;code>Q'&lt;/code>，这一步并不会影响到其它的线程。&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-13/52526691.jpg" alt="">&lt;/p>
&lt;ol start="2">
&lt;li>在P节点，使用CAS操作，添加分裂的信息Δ，标明该节点已经分裂，其中包含着&lt;code>KP&lt;/code>的信息，所以当查询的Key大于&lt;code>KP&lt;/code>时，会跳转到新节点&lt;code>Q'&lt;/code>进行查询。&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-13/7152787.jpg" alt="">&lt;/p>
&lt;h4 id="父节点更新">父节点更新 &lt;a href="#%e7%88%b6%e8%8a%82%e7%82%b9%e6%9b%b4%e6%96%b0" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>同样，父节点的更新仍然是采用增量修改的模式。&lt;/p>
&lt;p>我们在父节点使用CAS操作添加一条修改增量（index term delta record）。当查询请求落在&lt;code>Q&lt;/code>节点的范围内时，会直接跳转到&lt;code>Q'&lt;/code>节点。&lt;/p>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-13/29669418.jpg" alt="">&lt;/p>
&lt;h4 id="更新合并">更新合并 &lt;a href="#%e6%9b%b4%e6%96%b0%e5%90%88%e5%b9%b6" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>使用增量更新可以减少数据更新、节点分裂时的时间开销，从而避免失败的分裂操作（failed splits）。&lt;/p>
&lt;p>但是最终，我们仍会将产生的更新增量合并成一条完整的记录，以避免过多的空间浪费和指针跳转。&lt;/p>
&lt;h3 id="子节点合并">子节点合并 &lt;a href="#%e5%ad%90%e8%8a%82%e7%82%b9%e5%90%88%e5%b9%b6" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>在Bw-Tree数据减少时，我们为了避免过多的指针跳转，需要进行节点合并。节点合并比节点分裂复杂一点，所以需要一系列的原子操作来进行。&lt;/p>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-14/37353459.jpg" alt="">&lt;/p>
&lt;ol>
&lt;li>删除节点&lt;/li>
&lt;/ol>
&lt;p>删除节点时使用标记删除，仍然是CAS操作，添加一个删除增量（remove node delta）到&lt;code>Q&lt;/code>。之后所有的读写请求全部转发到其左兄弟&lt;code>P&lt;/code>。&lt;/p>
&lt;blockquote>
&lt;p>这里有个猜测，在Q中原有的数据仍然是要到Q节点进行访问的。论文里没有说，通过常识推断一下应该如此。&lt;/p>&lt;/blockquote>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-14/79837579.jpg" alt="">&lt;/p>
&lt;ol start="2">
&lt;li>合并子节点&lt;/li>
&lt;/ol>
&lt;p>在&lt;code>P&lt;/code>节点之前用CAS操作添加合并增量（node merge delta）。合并增量将&lt;code>P&lt;/code>节点和&lt;code>Q&lt;/code>节点从逻辑上合为一个，并且负责转发读写请求。&lt;/p>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-14/81902381.jpg" alt="">&lt;/p>
&lt;h4 id="父节点更新-1">父节点更新 &lt;a href="#%e7%88%b6%e8%8a%82%e7%82%b9%e6%9b%b4%e6%96%b0-1" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>这个就非常简单，就是CAS操作在父节点标记删除&lt;code>Q&lt;/code>节点的Key即可。&lt;/p>
&lt;h3 id="序列化结构修改">序列化结构修改 &lt;a href="#%e5%ba%8f%e5%88%97%e5%8c%96%e7%bb%93%e6%9e%84%e4%bf%ae%e6%94%b9" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>上面的Bw-Tree结构修改的实现都基于一个假设：数据库的同步机制会处理好数据更新冲突问题。&lt;/p>
&lt;p>同步机制包括数据库系统的锁管理器（lock manager）、事务管理组件（transactional component）；或者是采用弃疗方案，允许随机交叉写。&lt;/p>
&lt;p>但是在Bw-Tree内部，我们需要序列化树的结构修改（俗称排队）。当一个SMO操作遇到了未完成的SMO操作时，当前线程会等待之前的SMO操作完成后再继续当前操作。&lt;/p>
&lt;p>这样的方案可能会形成一个“SMO栈”，但是这种场景比较少见，实现起来也比较简单。&lt;/p>
&lt;h2 id="缓存管理">缓存管理 &lt;a href="#%e7%bc%93%e5%ad%98%e7%ae%a1%e7%90%86" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>缓存层负责内存与SSD磁盘之间的读、写回以及换页操作。缓存层为Bw-Tree层提供了逻辑页的抽象。&lt;/p>
&lt;p>Bw-Tree使用PID访问逻辑页，当逻辑页位于磁盘中时，缓存层先把页读取到内存中，再使用CAS操作将PID指针从磁盘偏移量指向内存偏移量。&lt;/p>
&lt;h3 id="预写日志协议与日志序列号lsn">预写日志协议与日志序列号（LSN） &lt;a href="#%e9%a2%84%e5%86%99%e6%97%a5%e5%bf%97%e5%8d%8f%e8%ae%ae%e4%b8%8e%e6%97%a5%e5%bf%97%e5%ba%8f%e5%88%97%e5%8f%b7lsn" class="anchor">🔗&lt;/a>&lt;/h3>&lt;ul>
&lt;li>日志序列号（LSN）&lt;/li>
&lt;/ul>
&lt;p>插入或修改Bw-Tree的增量都会被打上日志序列号。日志序列号由事务子系统产生。&lt;/p>
&lt;p>在页的刷写增量（flush delta）中，记录着最后被刷入（flush）磁盘的日志序列号。&lt;/p>
&lt;ul>
&lt;li>事务日志协调&lt;/li>
&lt;/ul>
&lt;p>事务子系统维护着一个“最大写入日志序列号”（End of Stable Log, ESL）。事务子系统通过ESL来限制数据管理子系统的持久化进度，也就是说，数据管理子系统只能持久化事务号小于ESL的事务。这保证了预写日志协议的因果性。&lt;/p>
&lt;p>数据管理子系统通过向事务子系统反馈最后刷写的事务序号以提供反馈。当事务子系统想要向前移动“事务重做指针”（Redo-Scan-Start-Point, RSSP）时，会向事务子系统发送请求。当事务序号大于RSSP，且小于ESL的所以事务全部刷写到磁盘之后，数据管理子系统会向事务子系统发送ACK信号。最后事务子系统向前移动RSSP，已经落盘的事务信息抛弃。&lt;/p>
&lt;ul>
&lt;li>Bw-Tree的结构修改&lt;/li>
&lt;/ul>
&lt;p>我们将Bw-Tree的结构修改（SMO）封装成为一个系统事务。由于Bw-Tree的结构修改是无锁的，所以会有多个线程操作同一页的情况。&lt;/p>
&lt;p>将SMO视做事务，允许我们并发的执行SMO操作，当且仅当某一个SMO操作“胜出”时，才将事务提交。&lt;/p>
&lt;h3 id="将页写回日志式存储lss">将页写回日志式存储（LSS） &lt;a href="#%e5%b0%86%e9%a1%b5%e5%86%99%e5%9b%9e%e6%97%a5%e5%bf%97%e5%bc%8f%e5%ad%98%e5%82%a8lss" class="anchor">🔗&lt;/a>&lt;/h3>&lt;ul>
&lt;li>页封装（Page Marshalling）&lt;/li>
&lt;/ul>
&lt;p>我们如果要将页写回LSS，首先要先逻辑页转化成线性顺序存储的结构，便于我们将其存储到磁盘。&lt;/p>
&lt;p>然后我们要将页的状态设定为“锁定”（captured），以防止后续的修改破坏预写日志协议。&lt;/p>
&lt;ul>
&lt;li>增量刷写（Incremental Flushing）&lt;/li>
&lt;/ul>
&lt;p>当刷写页面时，缓存管理器只会刷写ESL之前的数据记录。这样的好处有：&lt;/p>
&lt;ol>
&lt;li>减少磁盘的使用&lt;/li>
&lt;li>避免产生过多的磁盘垃圾&lt;/li>
&lt;li>避免SSD的写入放大开销&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>刷写操作&lt;/li>
&lt;/ul>
&lt;p>使用双Buffer进行磁盘刷写，可以避免线程等待IO操作。&lt;/p>
&lt;p>在刷写操作完成之后，我们会更新mapping table，将页指针改变成磁盘的偏移量。并将内存中的页换出。&lt;/p>
&lt;p>缓存管理器会监视系统内存的使用情况，如果内存不够用，就将一部分页从自动内存中换出。&lt;/p>
&lt;h2 id="性能">性能 &lt;a href="#%e6%80%a7%e8%83%bd" class="anchor">🔗&lt;/a>&lt;/h2>&lt;h3 id="我们的对手">我们的对手 &lt;a href="#%e6%88%91%e4%bb%ac%e7%9a%84%e5%af%b9%e6%89%8b" class="anchor">🔗&lt;/a>&lt;/h3>&lt;ul>
&lt;li>BerkeleyDB&lt;/li>
&lt;/ul>
&lt;p>性能良好的独立存储引擎。我们使用C语言实现版本，内置存储引擎为B-Tree。&lt;/p>
&lt;p>运行模式为性能较好的“无事务模式”，支持一写多读。&lt;/p>
&lt;p>在整个实验中，我们将BerkeleyDB的缓存开到足够大，全部使用主存进行存储与查询操作。&lt;/p>
&lt;ul>
&lt;li>SkipList&lt;/li>
&lt;/ul>
&lt;p>无锁的跳表实现。&lt;/p>
&lt;h3 id="测试数据集">测试数据集 &lt;a href="#%e6%b5%8b%e8%af%95%e6%95%b0%e6%8d%ae%e9%9b%86" class="anchor">🔗&lt;/a>&lt;/h3>&lt;ul>
&lt;li>XBox Live &lt;br>
网游数据。Key的平均大小是94Bytes，Value的平均大小是1.2K。读写比大概是7.5: 1&lt;/li>
&lt;li>商业数据库的备份流 &lt;br>
总共有27M数据块，去重后有12M。读写比为2.2：1，Key是20Bytes的SHA1哈希值，Value是44Bytes的元信息。&lt;/li>
&lt;li>综合KV数据 &lt;br>
8Bytes整数Key，8Bytes整数Value。读写比从5：1。&lt;/li>
&lt;/ul>
&lt;h3 id="比赛项目">比赛项目 &lt;a href="#%e6%af%94%e8%b5%9b%e9%a1%b9%e7%9b%ae" class="anchor">🔗&lt;/a>&lt;/h3>&lt;h4 id="增量链长度阈值对性能的影响">增量链长度阈值对性能的影响 &lt;a href="#%e5%a2%9e%e9%87%8f%e9%93%be%e9%95%bf%e5%ba%a6%e9%98%88%e5%80%bc%e5%af%b9%e6%80%a7%e8%83%bd%e7%9a%84%e5%bd%b1%e5%93%8d" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-17/40340311.jpg" alt="">&lt;/p>
&lt;p>增量链长度阈值影响触发页合并（consolidation）的频率。从图中我们做出如下推测：&lt;/p>
&lt;ol>
&lt;li>增量链长度较短时，读写性能好，但页合并开销大&lt;/li>
&lt;li>增量链长度转长时，读写开销大，页合并开销小&lt;/li>
&lt;/ol>
&lt;p>系统的整体性能就在这两个因素下面取一个平衡。&lt;/p>
&lt;h4 id="无锁环境下的更新失败">无锁环境下的更新失败 &lt;a href="#%e6%97%a0%e9%94%81%e7%8e%af%e5%a2%83%e4%b8%8b%e7%9a%84%e6%9b%b4%e6%96%b0%e5%a4%b1%e8%b4%a5" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-17/63409936.jpg" alt="">&lt;/p>
&lt;p>数据中我们可以看出，Bw-Tree写入失败（Failed Updates）是非常少的。但是“综合”数据集的“页拆分失败”和“页合并失败”明显高于其它数据集。&lt;/p>
&lt;p>这是因为“综合”数据集中的数据都比较小，更新起来比较快，所以会导致比较多的线程竞争。&lt;/p>
&lt;h4 id="与传统b-tree的比较">与传统B-Tree的比较 &lt;a href="#%e4%b8%8e%e4%bc%a0%e7%bb%9fb-tree%e7%9a%84%e6%af%94%e8%be%83" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-17/63409936.jpg" alt="">&lt;/p>
&lt;p>Bw-Tree明显胜出，其原因有：&lt;/p>
&lt;ol>
&lt;li>Bw-Tree是无锁的，所以并发程度更高&lt;/li>
&lt;li>Bw-Tree对CPU cache更加友好&lt;/li>
&lt;/ol>
&lt;h4 id="bw-tree与无锁跳表比较">Bw-Tree与无锁跳表比较 &lt;a href="#bw-tree%e4%b8%8e%e6%97%a0%e9%94%81%e8%b7%b3%e8%a1%a8%e6%af%94%e8%be%83" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-17/63409936.jpg" alt="">&lt;/p>
&lt;p>Bw-Tree仍然明显胜出，其原因归于Bw-Tree更友好使用了CPU cache。&lt;/p>
&lt;h4 id="缓存性能">缓存性能 &lt;a href="#%e7%bc%93%e5%ad%98%e6%80%a7%e8%83%bd" class="anchor">🔗&lt;/a>&lt;/h4>&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-pic/17-12-17/63354469.jpg" alt="">&lt;/p>
&lt;p>Bw-Tree的优势有：&lt;/p>
&lt;ol>
&lt;li>与跳表相比不需要过多的指针跳转&lt;/li>
&lt;li>内存使用更加高效&lt;/li>
&lt;/ol>
&lt;h2 id="后续201026">后续（201026） &lt;a href="#%e5%90%8e%e7%bb%ad201026" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>到了新组之后，发现了一个使用BW-Tree的存储引擎。不过已经即将废弃了（挥手&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>特指Redis的一些主要功能&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>tree-based数据结构的一个例子就是图书馆。图书馆用A-Z代表大类，然后使用数字代表小类，最后加上一个数字确定这本书的书号。 &lt;br>
图书馆藏书的顺序是按照书号的顺序进行排序的。所以我们在查找一本书的时候，可以使用特定的书号，利用其中的层次信息查找某一本书，也可以根据某一个书号范围进行查找一系列连续的书。 &lt;br>
如果还弄不明白的话，可以使用&lt;a href="http://www.ztflh.com/">中国图书馆分类法网站&lt;/a>加深一下理解。&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item></channel></rss>