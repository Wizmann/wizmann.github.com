<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>分类器 on Maerlyn's Rainbow</title><link>https://wizmann.top/tags/%E5%88%86%E7%B1%BB%E5%99%A8/</link><description>Recent content in 分类器 on Maerlyn's Rainbow</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Fri, 29 Nov 2013 00:00:00 +0000</lastBuildDate><atom:link href="https://wizmann.top/tags/%E5%88%86%E7%B1%BB%E5%99%A8/index.xml" rel="self" type="application/rss+xml"/><item><title>機器學習基石 - PLA算法初步</title><link>https://wizmann.top/posts/ml-foundations-pla/</link><pubDate>Fri, 29 Nov 2013 00:00:00 +0000</pubDate><guid>https://wizmann.top/posts/ml-foundations-pla/</guid><description>&lt;h2 id="什么是pla算法">什么是PLA算法 &lt;a href="#%e4%bb%80%e4%b9%88%e6%98%afpla%e7%ae%97%e6%b3%95" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>PLA = Perceptrons Learning Alogrithm&lt;/p>
&lt;p>WikiPedia上有一个大概的历史背景介绍。&lt;/p>
&lt;blockquote>
&lt;p>感知机（英语：Perceptron）是Frank Rosenblatt在1957年就职于Cornell航空实验室(Cornell Aeronautical Laboratory)时所发明的一种人工神经网络。它可以被视为一种最简单形式的前馈式人工神经网络，是一种二元线性分类器。&lt;/p>&lt;/blockquote>
&lt;h2 id="pla算法的原理">PLA算法的原理 &lt;a href="#pla%e7%ae%97%e6%b3%95%e7%9a%84%e5%8e%9f%e7%90%86" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-tk-pic/blog-perceptron-Ncell.png" alt="感知机示意图">&lt;/p>
&lt;blockquote>
&lt;p>对于每种输入值(1 - D)，我们计算一个权重。当前神经元的总激发值(a)就等于每种输入值(x)乘以权重(w)之和。&lt;/p>&lt;/blockquote>
&lt;p>由此我们就可以推导出公式如下。&lt;/p>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-tk-pic/blog-perceptron-formula-1.jpg" alt="neuron sum">&lt;/p>
&lt;p>我们可以为这个“神经元”的激发值设定一个阈值&lt;code>threshold&lt;/code>。&lt;/p>
&lt;p>如果 &lt;code>a &amp;gt; threshold&lt;/code>，则判定输入为正例。
如果 &lt;code>a &amp;lt; threshold&lt;/code>，则判定输入为负例。
对于 &lt;code>a == threshold&lt;/code>的情况，认为是特殊情况，不予考虑。&lt;/p>
&lt;p>所以，我们的感知器分类器就可以得到以下式子。&lt;/p>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-tk-pic/blog-perceptron-formula-2.png" alt="perceptron-formula-2">&lt;/p>
&lt;p>我们在数据向量中加入了阈值，并把式子统一成向量积的形式。&lt;/p>
&lt;h2 id="pla算法的错误修正">PLA算法的错误修正 &lt;a href="#pla%e7%ae%97%e6%b3%95%e7%9a%84%e9%94%99%e8%af%af%e4%bf%ae%e6%ad%a3" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>PLA算法是_错误驱动_的算法。&lt;/p>
&lt;blockquote>
&lt;p>当我们训练这个算法时，只要输出值是正确的，这个算法就不会进行任何数据的调整。反之，当输出值与实际值异号，这个算法就会自动调整参数的比重。&lt;/p>&lt;/blockquote>
&lt;p>&lt;img src="https://github.com/Wizmann/assets/raw/master/wizmann-tk-pic/blog-perceptron-update.png" alt="错误修正">&lt;/p>
&lt;p>我们先取一个随机向量&lt;code>W&lt;/code>，与现有的数据&lt;code>X[i]&lt;/code>做点乘，取得结果的符号。&lt;/p>
&lt;p>如果符号符合我们的预期的话，则&lt;code>continue&lt;/code>。
否则就要对&lt;code>W&lt;/code>进行修正。&lt;/p>
&lt;p>修正的方式是&lt;code>W += y * X[i]&lt;/code>，每一次修正都是减少现有向量&lt;code>W&lt;/code>与向量&lt;code>y * X[i]&lt;/code>的夹角，从而调整答案的正确性。&lt;/p>
&lt;h2 id="naive-pla-与-pocket-pla">Naive PLA 与 Pocket PLA &lt;a href="#naive-pla-%e4%b8%8e-pocket-pla" class="anchor">🔗&lt;/a>&lt;/h2>&lt;h3 id="naive-pla">Naive PLA &lt;a href="#naive-pla" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>Naive PLA算法的思想很简单。一直修正向量&lt;code>W&lt;/code>，直到向量&lt;code>W&lt;/code>满足所有数据为止。&lt;/p>
&lt;p>代码如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> numpy &lt;span style="color:#f92672">import&lt;/span> &lt;span style="color:#f92672">*&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">naive_pla&lt;/span>(datas):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> w &lt;span style="color:#f92672">=&lt;/span> datas[&lt;span style="color:#ae81ff">0&lt;/span>][&lt;span style="color:#ae81ff">0&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> iteration &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">while&lt;/span> &lt;span style="color:#66d9ef">True&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> iteration &lt;span style="color:#f92672">+=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> false_data &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> data &lt;span style="color:#f92672">in&lt;/span> datas:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> t &lt;span style="color:#f92672">=&lt;/span> dot(w, data[&lt;span style="color:#ae81ff">0&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> sign(data[&lt;span style="color:#ae81ff">1&lt;/span>]) &lt;span style="color:#f92672">!=&lt;/span> sign(t):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> error &lt;span style="color:#f92672">=&lt;/span> data[&lt;span style="color:#ae81ff">1&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> false_data &lt;span style="color:#f92672">+=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> w &lt;span style="color:#f92672">+=&lt;/span> error &lt;span style="color:#f92672">*&lt;/span> data[&lt;span style="color:#ae81ff">0&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> print &lt;span style="color:#e6db74">&amp;#39;iter&lt;/span>&lt;span style="color:#e6db74">%d&lt;/span>&lt;span style="color:#e6db74"> (&lt;/span>&lt;span style="color:#e6db74">%d&lt;/span>&lt;span style="color:#e6db74"> / &lt;/span>&lt;span style="color:#e6db74">%d&lt;/span>&lt;span style="color:#e6db74">)&amp;#39;&lt;/span> &lt;span style="color:#f92672">%&lt;/span> (iteration, false_data, len(datas))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#f92672">not&lt;/span> false_data:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">break&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> w
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="pocket-pla">Pocket PLA &lt;a href="#pocket-pla" class="anchor">🔗&lt;/a>&lt;/h3>&lt;p>Naive PLA的一大问题就是如果数据有杂音，不能完美的分类的话，算法就不会中止。&lt;/p>
&lt;p>所以，对于有杂音的数据，我们只能期望找到错误最少的结果。然后这是一个&lt;code>NP Hard&lt;/code>问题。&lt;/p>
&lt;p>Pocket PLA一个贪心的近似算法。和Naive PLA算法类似。&lt;/p>
&lt;p>变顺序迭代为随机迭代，如果找出错误，则修正结果。在修正过程中，记录犯错误最少的向量。&lt;/p>
&lt;p>代码如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> numpy &lt;span style="color:#66d9ef">as&lt;/span> np
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">pocket_pla&lt;/span>(datas, limit):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">###############&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">_calc_false&lt;/span>(vec):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> res &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> data &lt;span style="color:#f92672">in&lt;/span> datas:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> t &lt;span style="color:#f92672">=&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>dot(vec, data[&lt;span style="color:#ae81ff">0&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>sign(data[&lt;span style="color:#ae81ff">1&lt;/span>]) &lt;span style="color:#f92672">!=&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>sign(t):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> res &lt;span style="color:#f92672">+=&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> res
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e">###############&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> w &lt;span style="color:#f92672">=&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>random&lt;span style="color:#f92672">.&lt;/span>rand(&lt;span style="color:#ae81ff">5&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> least_false &lt;span style="color:#f92672">=&lt;/span> _calc_false(w)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> res &lt;span style="color:#f92672">=&lt;/span> w
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> i &lt;span style="color:#f92672">in&lt;/span> xrange(limit):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> data &lt;span style="color:#f92672">=&lt;/span> random&lt;span style="color:#f92672">.&lt;/span>choice(datas)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> t &lt;span style="color:#f92672">=&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>dot(w, data[&lt;span style="color:#ae81ff">0&lt;/span>])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>sign(data[&lt;span style="color:#ae81ff">1&lt;/span>]) &lt;span style="color:#f92672">!=&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>sign(t):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> t &lt;span style="color:#f92672">=&lt;/span> w &lt;span style="color:#f92672">+&lt;/span> data[&lt;span style="color:#ae81ff">1&lt;/span>] &lt;span style="color:#f92672">*&lt;/span> data[&lt;span style="color:#ae81ff">0&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> t_false &lt;span style="color:#f92672">=&lt;/span> _calc_false(t)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> w &lt;span style="color:#f92672">=&lt;/span> t
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> t_false &lt;span style="color:#f92672">&amp;lt;=&lt;/span> least_false:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> least_false &lt;span style="color:#f92672">=&lt;/span> t_false
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> res &lt;span style="color:#f92672">=&lt;/span> t
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> res, least_false
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="参考链接">参考链接 &lt;a href="#%e5%8f%82%e8%80%83%e9%93%be%e6%8e%a5" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>本文主要参考了&lt;a href="http://shaoxiongjiang.com/2013/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8-%E6%84%9F%E7%9F%A5%E5%99%A8-perceptron/">机器学习入门 - 感知器 (Perceptron)&lt;/a>和Wikipedia上面&lt;a href="http://zh.wikipedia.org/wiki/%E6%84%9F%E7%9F%A5%E5%99%A8">感知机&lt;/a>的词条。&lt;/p>
&lt;p>以及&lt;a href="https://class.coursera.org/ntumlone-001/class">機器學習基石 (Machine Learning Foundations)&lt;/a>公开课的幻灯片。&lt;/p>
&lt;h2 id="updated">Updated &lt;a href="#updated" class="anchor">🔗&lt;/a>&lt;/h2>&lt;p>2013-12-8&lt;/p>
&lt;p>修改了pocket-pla算法，提升了效率和准确性。&lt;/p>
&lt;p>参考了&lt;a href="https://class.coursera.org/ntumlone-001/forum/thread?thread_id=116#post-632">课程论坛&lt;/a>的讨论。并且感谢Li Tianyi同学指出我的问题。&lt;/p></description></item></channel></rss>